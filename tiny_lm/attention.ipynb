{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "DEV = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "CONTEXT_SIZE = 32\n",
    "MAX_LEN = 128\n",
    "EMBEDDING_DIMS = 128\n",
    "NO_HEADS = 4\n",
    "assert EMBEDDING_DIMS % NO_HEADS == 0\n",
    "HEAD_DIMS = EMBEDDING_DIMS // NO_HEADS\n",
    "NO_LAYERS = 5\n",
    "DROPOUT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.txt\") as f:\n",
    "    data = f.read()\n",
    "chars = list(set(data))\n",
    "VOCAB_SIZE = len(chars)\n",
    "s_to_i = {s: i for i, s in enumerate(chars)}\n",
    "i_to_s = {v: k for k, v in s_to_i.items()}\n",
    "\n",
    "def encode(text):\n",
    "    return [s_to_i[c] for c in text]\n",
    "\n",
    "def decode(ids):\n",
    "    return \"\".join([i_to_s[i] for i in ids])\n",
    "\n",
    "data = encode(data)\n",
    "train_data = data[:int(len(data) * 0.9)]\n",
    "val_data = data[int(len(data) * 0.9):]\n",
    "\n",
    "def batch_generator(split, indefinite=False):\n",
    "    idx = 0\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    per_batch_tokens = BATCH_SIZE * CONTEXT_SIZE + 1\n",
    "    while True:\n",
    "        if idx + per_batch_tokens > len(data):\n",
    "            # TODO: finish this\n",
    "            if not indefinite:\n",
    "                return\n",
    "            b_data = data[idx:]\n",
    "            b_data += data[0: per_batch_tokens - len(b_data)]\n",
    "        else:\n",
    "            b_data = data[idx: idx + per_batch_tokens]\n",
    "            idx += BATCH_SIZE * CONTEXT_SIZE\n",
    "        x = [b_data[i * CONTEXT_SIZE: (i+1) * CONTEXT_SIZE] for i in range(BATCH_SIZE)]\n",
    "        y = [b_data[i * CONTEXT_SIZE + 1: (i+1) * CONTEXT_SIZE + 1] for i in range(BATCH_SIZE)]\n",
    "        yield torch.tensor(x, device=DEV), torch.tensor(y, device=DEV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Wk = nn.Linear(EMBEDDING_DIMS, HEAD_DIMS)\n",
    "        self.Wv = nn.Linear(EMBEDDING_DIMS, HEAD_DIMS)\n",
    "        self.Wq = nn.Linear(EMBEDDING_DIMS, HEAD_DIMS)\n",
    "        self.ln = nn.LayerNorm(EMBEDDING_DIMS)\n",
    "        self.drop = nn.Dropout(DROPOUT)\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones((32, 32))))\n",
    "\n",
    "    def forward(self, k, q, v):\n",
    "        k, q, v = self.Wk(k), self.Wv(v), self.Wq(q)\n",
    "        attn = (q @ k.transpose(-1, -2)) * EMBEDDING_DIMS**-0.5\n",
    "        attn = attn.masked_fill(self.mask[:,:] == 0, float(\"-inf\"))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.drop(attn)\n",
    "        return attn @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(EMBEDDING_DIMS, EMBEDDING_DIMS * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(EMBEDDING_DIMS * 4, EMBEDDING_DIMS),\n",
    "            nn.Dropout(DROPOUT)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead() for _ in range(NO_HEADS)]\n",
    "        )\n",
    "        self.proj = nn.Linear(EMBEDDING_DIMS, EMBEDDING_DIMS)\n",
    "        self.drop = nn.Dropout(DROPOUT)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([h(k=x, q=x, v=x) for h in self.heads], dim=-1)\n",
    "        x = self.proj(x)\n",
    "        return self.drop(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention()\n",
    "        self.ln1 = nn.LayerNorm(EMBEDDING_DIMS)\n",
    "        self.ffwd = FeedForward()\n",
    "        self.ln2 = nn.LayerNorm(EMBEDDING_DIMS)\n",
    "        self.drop = nn.Dropout(DROPOUT)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ln2(self.ffwd(x) + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIMS)\n",
    "        self.pos_enc = torch.randn((MAX_LEN, EMBEDDING_DIMS))\n",
    "        positions = torch.arange(0, MAX_LEN).unsqueeze(1)\n",
    "        _2i = torch.arange(0, EMBEDDING_DIMS, 2)\n",
    "        self.pos_enc[:,::2] = torch.sin(positions / 10000 ** (_2i / EMBEDDING_DIMS))\n",
    "        self.pos_enc[:, 1::2] = torch.cos(positions / 10000 ** (_2i / EMBEDDING_DIMS))\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        return self.tok_emb(tokens) + self.pos_enc[tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embeddings = TransformerEmbedding()\n",
    "        self.layers = nn.Sequential(*[\n",
    "            Encoder() for _ in range(NO_LAYERS)\n",
    "        ])\n",
    "        self.to_vocab = nn.Linear(EMBEDDING_DIMS, VOCAB_SIZE)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        return self.to_vocab(self.layers(self.embeddings(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.3093\n",
      "3.7442\n",
      "10.5646\n",
      "14.0874\n",
      "14.5437\n",
      "11.7487\n",
      "9.6533\n",
      "7.0158\n",
      "5.7383\n",
      "5.1075\n",
      "6.3777\n",
      "6.1644\n",
      "5.3338\n",
      "4.7233\n",
      "4.4499\n",
      "4.6398\n",
      "4.0669\n",
      "4.0811\n",
      "4.2670\n",
      "3.9441\n",
      "3.8328\n",
      "3.8894\n",
      "3.8092\n",
      "3.5941\n",
      "3.6241\n",
      "3.6909\n",
      "3.4887\n",
      "3.5374\n",
      "3.5249\n",
      "3.4560\n",
      "3.4633\n",
      "3.4464\n",
      "3.4208\n",
      "3.4478\n",
      "3.4146\n",
      "3.4010\n",
      "3.4250\n",
      "3.3911\n",
      "3.3944\n",
      "3.3645\n",
      "3.3379\n",
      "3.3878\n",
      "3.3717\n",
      "3.4076\n",
      "3.3715\n",
      "3.3639\n",
      "3.3821\n",
      "3.4071\n",
      "3.3146\n",
      "3.3589\n",
      "3.3394\n",
      "3.3602\n",
      "3.3433\n",
      "3.3066\n",
      "3.3420\n",
      "3.2904\n",
      "3.3029\n",
      "3.3305\n",
      "3.3465\n",
      "3.3674\n",
      "3.3764\n",
      "3.3005\n",
      "3.3268\n",
      "3.3278\n",
      "3.3606\n",
      "3.3332\n",
      "3.3406\n",
      "3.3485\n",
      "3.3330\n",
      "3.3066\n",
      "3.3301\n",
      "3.3350\n",
      "3.3323\n",
      "3.3462\n",
      "3.3246\n",
      "3.3032\n",
      "3.3472\n",
      "3.3109\n",
      "3.3300\n",
      "3.3435\n",
      "3.3179\n",
      "3.3359\n",
      "3.3008\n",
      "3.3104\n",
      "3.2983\n",
      "3.3132\n",
      "3.3395\n",
      "3.3212\n",
      "3.3392\n",
      "3.3399\n",
      "3.3126\n",
      "3.2983\n",
      "3.3071\n",
      "3.2984\n",
      "3.3110\n",
      "3.3562\n",
      "3.3388\n",
      "3.3101\n",
      "3.3272\n",
      "3.3311\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "EPOCHS = 100\n",
    "LR = 0.1\n",
    "\n",
    "net = Model()\n",
    "optimizer = optim.AdamW(net.parameters(), lr=LR)\n",
    "generator = batch_generator(\"train\")\n",
    "\n",
    "for e in range(1, EPOCHS + 1):\n",
    "    x, y = get_batch(\"train\")\n",
    "    logits = net(x)\n",
    "    loss = F.cross_entropy(logits.view(-1, VOCAB_SIZE), y.view(-1))\n",
    "    print(\"%.4f\" %(loss.item()))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
