{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "CONTEXT_SIZE = 64\n",
    "EMBEDDING_DIMS = 1024\n",
    "HIDDEN_LAYERS = 5\n",
    "HIDDEN_DIMS = 1024\n",
    "EPOCHS = 100000\n",
    "LR = 0.01\n",
    "\n",
    "DEV = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tiny_shakespeare.txt\") as f:\n",
    "    data = f.read()\n",
    "chars = list(set(data))\n",
    "VOCAB_SIZE = len(chars)\n",
    "s_to_i = {s: i for i, s in enumerate(chars)}\n",
    "i_to_s = {v: k for k, v in s_to_i.items()}\n",
    "\n",
    "def encode(text):\n",
    "    return [s_to_i[c] for c in text]\n",
    "\n",
    "def decode(ids):\n",
    "    return \"\".join([i_to_s[i] for i in ids])\n",
    "\n",
    "data = torch.tensor(encode(data), device=DEV)\n",
    "train_data = data[:int(len(data) * 0.9)]\n",
    "val_data = data[int(len(data) * 0.9):]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    idxs = [random.randint(0, data.shape[-1] - CONTEXT_SIZE - 1) for _ in range(BATCH_SIZE)]\n",
    "    x = torch.stack([data[idx: idx + CONTEXT_SIZE] for idx in idxs])\n",
    "    y = torch.stack([data[idx + 1: idx + 1 + CONTEXT_SIZE] for idx in idxs])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCL(nn.Module):\n",
    "    def __init__(self, fan_in=HIDDEN_DIMS, fan_out=HIDDEN_DIMS):\n",
    "        super().__init__()\n",
    "        self.layers = [\n",
    "            nn.Linear(fan_in, fan_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Embedding(VOCAB_SIZE, EMBEDDING_DIMS),\n",
    "            FCL(EMBEDDING_DIMS, HIDDEN_DIMS),\n",
    "            *[FCL() for _ in range(HIDDEN_LAYERS)],\n",
    "            nn.Linear(HIDDEN_DIMS, VOCAB_SIZE)\n",
    "        )\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        return self.layers(tokens)\n",
    "    \n",
    "    def generate(self, context=\"\\n\", new_tokens=100):\n",
    "        context = torch.tensor([encode(context)])\n",
    "        out = []\n",
    "        while len(out) < new_tokens:\n",
    "            logits = net(context)\n",
    "            probs = F.softmax(logits, dim=-1)[:, -1:, :]\n",
    "            next = torch.multinomial(probs.view(-1, VOCAB_SIZE), num_samples=1)\n",
    "            out.append(next.item())\n",
    "            print(decode([next.item()]), end=\"\")\n",
    "            context = torch.cat((context, next), dim=-1)\n",
    "        return decode(out)\n",
    "    \n",
    "    \n",
    "net = LM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling without training\n",
    "net.generate(\"Fir\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_split_losses():\n",
    "    net.eval()\n",
    "    losses = {}\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        x, y = get_batch(split)\n",
    "        logits = net(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), y.view(-1))\n",
    "        losses[split] = loss.item()\n",
    "    return losses.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%d trainable parameters\" % (sum(p.nelement() for p in net.parameters())))\n",
    "print(\"- \" * 10)\n",
    "optimizer = optim.AdamW(net.parameters(), lr=LR)\n",
    "\n",
    "for e in range(1, EPOCHS + 1):\n",
    "    if e % (EPOCHS / 25) == 0:\n",
    "        LR /= 2\n",
    "        for group in optimizer.param_groups:\n",
    "            group[\"lr\"] = LR\n",
    "    x, y, = get_batch(\"train\")\n",
    "    logits = net(x)\n",
    "    loss = F.cross_entropy(logits.view(-1, logits.shape[-1] ), y.view(-1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    t_loss, v_loss = get_split_losses()\n",
    "    print(\"%d: lr: %.4f, train loss: %.4f, val loss: %.4f\" % (e, LR, t_loss, v_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
