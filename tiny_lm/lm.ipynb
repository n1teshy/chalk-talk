{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "CONTEXT_SIZE = 64\n",
    "EMBEDDING_DIMS = 1024\n",
    "HIDDEN_LAYERS = 5\n",
    "HIDDEN_DIMS = 1024\n",
    "EPOCHS = 100000\n",
    "LR = 0.01\n",
    "\n",
    "DEV = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.txt\") as f:\n",
    "    data = f.read()\n",
    "chars = list(set(data))\n",
    "VOCAB_SIZE = len(chars)\n",
    "s_to_i = {s: i for i, s in enumerate(chars)}\n",
    "i_to_s = {v: k for k, v in s_to_i.items()}\n",
    "\n",
    "def encode(text):\n",
    "    return [s_to_i[c] for c in text]\n",
    "\n",
    "def decode(ids):\n",
    "    return \"\".join([i_to_s[i] for i in ids])\n",
    "\n",
    "data = torch.tensor(encode(data), device=DEV)\n",
    "train_data = data[:int(len(data) * 0.9)]\n",
    "val_data = data[int(len(data) * 0.9):]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    idxs = [random.randint(0, data.shape[-1] - CONTEXT_SIZE - 1) for _ in range(BATCH_SIZE)]\n",
    "    x = torch.stack([data[idx: idx + CONTEXT_SIZE] for idx in idxs])\n",
    "    y = torch.stack([data[idx + 1: idx + 1 + CONTEXT_SIZE] for idx in idxs])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCL(nn.Module):\n",
    "    def __init__(self, fan_in=HIDDEN_DIMS, fan_out=HIDDEN_DIMS):\n",
    "        super().__init__()\n",
    "        self.layers = [\n",
    "            nn.Linear(fan_in, fan_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Embedding(VOCAB_SIZE, EMBEDDING_DIMS),\n",
    "            FCL(EMBEDDING_DIMS, HIDDEN_DIMS),\n",
    "            *[FCL() for _ in range(HIDDEN_LAYERS)],\n",
    "            nn.Linear(HIDDEN_DIMS, VOCAB_SIZE)\n",
    "        )\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        return self.layers(tokens)\n",
    "    \n",
    "    def generate(self, context=\"\\n\", new_tokens=100):\n",
    "        context = torch.tensor([encode(context)])\n",
    "        out = []\n",
    "        while len(out) < new_tokens:\n",
    "            logits = net(context)\n",
    "            probs = F.softmax(logits, dim=-1)[:, -1:, :]\n",
    "            next = torch.multinomial(probs.view(-1, VOCAB_SIZE), num_samples=1)\n",
    "            out.append(next.item())\n",
    "            print(decode([next.item()]), end=\"\")\n",
    "            context = torch.cat((context, next), dim=-1)\n",
    "        return decode(out)\n",
    "    \n",
    "    \n",
    "net = LM()\n",
    "net.load_state_dict(torch.load(\"emb_1024_loss_2.5.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_split_losses():\n",
    "    net.eval()\n",
    "    losses = {}\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        x, y = get_batch(split)\n",
    "        logits = net(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), y.view(-1))\n",
    "        losses[split] = loss.item()\n",
    "    return losses.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133185 trainable parameters\n",
      "1: lr: 0.0100, train loss: 4.0716, val loss: 4.0722\n",
      "2: lr: 0.0100, train loss: 3.9708, val loss: 3.9793\n",
      "3: lr: 0.0100, train loss: 3.8757, val loss: 3.8892\n",
      "4: lr: 0.0100, train loss: 3.8051, val loss: 3.8282\n",
      "5: lr: 0.0100, train loss: 3.7193, val loss: 3.7384\n",
      "6: lr: 0.0100, train loss: 3.6652, val loss: 3.6832\n",
      "7: lr: 0.0100, train loss: 3.6139, val loss: 3.6301\n",
      "8: lr: 0.0100, train loss: 3.5747, val loss: 3.6011\n",
      "9: lr: 0.0100, train loss: 3.4988, val loss: 3.5275\n",
      "10: lr: 0.0100, train loss: 3.5474, val loss: 3.5115\n",
      "11: lr: 0.0100, train loss: 3.4255, val loss: 3.5448\n",
      "12: lr: 0.0100, train loss: 3.4890, val loss: 3.5349\n",
      "13: lr: 0.0100, train loss: 3.3814, val loss: 3.4404\n",
      "14: lr: 0.0100, train loss: 3.4319, val loss: 3.3588\n",
      "15: lr: 0.0100, train loss: 3.3284, val loss: 3.3677\n",
      "16: lr: 0.0100, train loss: 3.3589, val loss: 3.3803\n",
      "17: lr: 0.0100, train loss: 3.2690, val loss: 3.3708\n",
      "18: lr: 0.0100, train loss: 3.2924, val loss: 3.3691\n",
      "19: lr: 0.0100, train loss: 3.2769, val loss: 3.3309\n",
      "20: lr: 0.0100, train loss: 3.3434, val loss: 3.2885\n",
      "21: lr: 0.0100, train loss: 3.2067, val loss: 3.2951\n",
      "22: lr: 0.0100, train loss: 3.2837, val loss: 3.3541\n",
      "23: lr: 0.0100, train loss: 3.2458, val loss: 3.3864\n",
      "24: lr: 0.0100, train loss: 3.2188, val loss: 3.2738\n",
      "25: lr: 0.0100, train loss: 3.3091, val loss: 3.2807\n",
      "26: lr: 0.0100, train loss: 3.2292, val loss: 3.3074\n",
      "27: lr: 0.0100, train loss: 3.2625, val loss: 3.3462\n",
      "28: lr: 0.0100, train loss: 3.2909, val loss: 3.3096\n",
      "29: lr: 0.0100, train loss: 3.2115, val loss: 3.3139\n",
      "30: lr: 0.0100, train loss: 3.2147, val loss: 3.2822\n",
      "31: lr: 0.0100, train loss: 3.2353, val loss: 3.3301\n",
      "32: lr: 0.0100, train loss: 3.1983, val loss: 3.2514\n",
      "33: lr: 0.0100, train loss: 3.2707, val loss: 3.2787\n",
      "34: lr: 0.0100, train loss: 3.1386, val loss: 3.2011\n",
      "35: lr: 0.0100, train loss: 3.2847, val loss: 3.2260\n",
      "36: lr: 0.0100, train loss: 3.1898, val loss: 3.2689\n",
      "37: lr: 0.0100, train loss: 3.1362, val loss: 3.1931\n",
      "38: lr: 0.0100, train loss: 3.1698, val loss: 3.1662\n",
      "39: lr: 0.0100, train loss: 3.1349, val loss: 3.2384\n",
      "40: lr: 0.0100, train loss: 3.2085, val loss: 3.2237\n",
      "41: lr: 0.0100, train loss: 3.1733, val loss: 3.2248\n",
      "42: lr: 0.0100, train loss: 3.1939, val loss: 3.1986\n",
      "43: lr: 0.0100, train loss: 3.1520, val loss: 3.1961\n",
      "44: lr: 0.0100, train loss: 3.2267, val loss: 3.2143\n",
      "45: lr: 0.0100, train loss: 3.1101, val loss: 3.1103\n",
      "46: lr: 0.0100, train loss: 3.1590, val loss: 3.2230\n",
      "47: lr: 0.0100, train loss: 3.1297, val loss: 3.1275\n",
      "48: lr: 0.0100, train loss: 3.0940, val loss: 3.1712\n",
      "49: lr: 0.0100, train loss: 3.0178, val loss: 3.0541\n",
      "50: lr: 0.0100, train loss: 3.0118, val loss: 3.1027\n",
      "51: lr: 0.0100, train loss: 3.1278, val loss: 3.1251\n",
      "52: lr: 0.0100, train loss: 3.0576, val loss: 3.1986\n",
      "53: lr: 0.0100, train loss: 2.9864, val loss: 3.2014\n",
      "54: lr: 0.0100, train loss: 3.1170, val loss: 3.1611\n",
      "55: lr: 0.0100, train loss: 3.0522, val loss: 3.0493\n",
      "56: lr: 0.0100, train loss: 3.0755, val loss: 3.1050\n",
      "57: lr: 0.0100, train loss: 3.0563, val loss: 3.0650\n",
      "58: lr: 0.0100, train loss: 2.9933, val loss: 3.0657\n",
      "59: lr: 0.0100, train loss: 3.0035, val loss: 3.1084\n",
      "60: lr: 0.0100, train loss: 3.0405, val loss: 3.0573\n",
      "61: lr: 0.0100, train loss: 2.9582, val loss: 3.0040\n",
      "62: lr: 0.0100, train loss: 2.8811, val loss: 3.0937\n",
      "63: lr: 0.0100, train loss: 2.9415, val loss: 2.9660\n",
      "64: lr: 0.0100, train loss: 2.9574, val loss: 2.9968\n",
      "65: lr: 0.0100, train loss: 2.9669, val loss: 2.9412\n",
      "66: lr: 0.0100, train loss: 2.9706, val loss: 3.0055\n",
      "67: lr: 0.0100, train loss: 2.8491, val loss: 3.0228\n",
      "68: lr: 0.0100, train loss: 2.9150, val loss: 2.9186\n",
      "69: lr: 0.0100, train loss: 2.8862, val loss: 2.9655\n",
      "70: lr: 0.0100, train loss: 2.9116, val loss: 3.0314\n",
      "71: lr: 0.0100, train loss: 2.8437, val loss: 2.9296\n",
      "72: lr: 0.0100, train loss: 2.8581, val loss: 2.8960\n",
      "73: lr: 0.0100, train loss: 2.8529, val loss: 2.9106\n",
      "74: lr: 0.0100, train loss: 2.8325, val loss: 2.8619\n",
      "75: lr: 0.0100, train loss: 2.8559, val loss: 2.8961\n",
      "76: lr: 0.0100, train loss: 2.8194, val loss: 2.8980\n",
      "77: lr: 0.0100, train loss: 2.8618, val loss: 2.8489\n",
      "78: lr: 0.0100, train loss: 2.9043, val loss: 2.8014\n",
      "79: lr: 0.0100, train loss: 2.8374, val loss: 2.8463\n",
      "80: lr: 0.0100, train loss: 2.8802, val loss: 2.8389\n",
      "81: lr: 0.0100, train loss: 2.8234, val loss: 2.8574\n",
      "82: lr: 0.0100, train loss: 2.8119, val loss: 2.7704\n",
      "83: lr: 0.0100, train loss: 2.8301, val loss: 2.7886\n",
      "84: lr: 0.0100, train loss: 2.7781, val loss: 2.8168\n",
      "85: lr: 0.0100, train loss: 2.7469, val loss: 2.8491\n",
      "86: lr: 0.0100, train loss: 2.7276, val loss: 2.7901\n",
      "87: lr: 0.0100, train loss: 2.7955, val loss: 2.7920\n",
      "88: lr: 0.0100, train loss: 2.7983, val loss: 2.7930\n",
      "89: lr: 0.0100, train loss: 2.7237, val loss: 2.7253\n",
      "90: lr: 0.0100, train loss: 2.8040, val loss: 2.7428\n",
      "91: lr: 0.0100, train loss: 2.7308, val loss: 2.7381\n",
      "92: lr: 0.0100, train loss: 2.6912, val loss: 2.7416\n",
      "93: lr: 0.0100, train loss: 2.7103, val loss: 2.7053\n",
      "94: lr: 0.0100, train loss: 2.7524, val loss: 2.7460\n",
      "95: lr: 0.0100, train loss: 2.7269, val loss: 2.7190\n",
      "96: lr: 0.0100, train loss: 2.7509, val loss: 2.7507\n",
      "97: lr: 0.0100, train loss: 2.7734, val loss: 2.7214\n",
      "98: lr: 0.0100, train loss: 2.6986, val loss: 2.6899\n",
      "99: lr: 0.0100, train loss: 2.6651, val loss: 2.6578\n",
      "100: lr: 0.0100, train loss: 2.6918, val loss: 2.6996\n",
      "101: lr: 0.0100, train loss: 2.6621, val loss: 2.7420\n",
      "102: lr: 0.0100, train loss: 2.6083, val loss: 2.6683\n",
      "103: lr: 0.0100, train loss: 2.6700, val loss: 2.6696\n",
      "104: lr: 0.0100, train loss: 2.6932, val loss: 2.6596\n",
      "105: lr: 0.0100, train loss: 2.6800, val loss: 2.6771\n",
      "106: lr: 0.0100, train loss: 2.6775, val loss: 2.6858\n",
      "107: lr: 0.0100, train loss: 2.6653, val loss: 2.6691\n",
      "108: lr: 0.0100, train loss: 2.7175, val loss: 2.7281\n",
      "109: lr: 0.0100, train loss: 2.6151, val loss: 2.6185\n",
      "110: lr: 0.0100, train loss: 2.6259, val loss: 2.6498\n",
      "111: lr: 0.0100, train loss: 2.5905, val loss: 2.6576\n",
      "112: lr: 0.0100, train loss: 2.6979, val loss: 2.6601\n",
      "113: lr: 0.0100, train loss: 2.6347, val loss: 2.6699\n",
      "114: lr: 0.0100, train loss: 2.6396, val loss: 2.6636\n",
      "115: lr: 0.0100, train loss: 2.6499, val loss: 2.6262\n",
      "116: lr: 0.0100, train loss: 2.6927, val loss: 2.6302\n",
      "117: lr: 0.0100, train loss: 2.6644, val loss: 2.5812\n",
      "118: lr: 0.0100, train loss: 2.6244, val loss: 2.6110\n",
      "119: lr: 0.0100, train loss: 2.6387, val loss: 2.5600\n",
      "120: lr: 0.0100, train loss: 2.6109, val loss: 2.5990\n",
      "121: lr: 0.0100, train loss: 2.6190, val loss: 2.6876\n",
      "122: lr: 0.0100, train loss: 2.6121, val loss: 2.6511\n",
      "123: lr: 0.0100, train loss: 2.6155, val loss: 2.6408\n",
      "124: lr: 0.0100, train loss: 2.5297, val loss: 2.5410\n",
      "125: lr: 0.0100, train loss: 2.5565, val loss: 2.6025\n",
      "126: lr: 0.0100, train loss: 2.6427, val loss: 2.6376\n",
      "127: lr: 0.0100, train loss: 2.5991, val loss: 2.5934\n",
      "128: lr: 0.0100, train loss: 2.5998, val loss: 2.5433\n",
      "129: lr: 0.0100, train loss: 2.6277, val loss: 2.6344\n",
      "130: lr: 0.0100, train loss: 2.6003, val loss: 2.5803\n",
      "131: lr: 0.0100, train loss: 2.6111, val loss: 2.6311\n",
      "132: lr: 0.0100, train loss: 2.6037, val loss: 2.5777\n",
      "133: lr: 0.0100, train loss: 2.5993, val loss: 2.6576\n",
      "134: lr: 0.0100, train loss: 2.6162, val loss: 2.6089\n",
      "135: lr: 0.0100, train loss: 2.5897, val loss: 2.5601\n",
      "136: lr: 0.0100, train loss: 2.6032, val loss: 2.5911\n",
      "137: lr: 0.0100, train loss: 2.5896, val loss: 2.6015\n",
      "138: lr: 0.0100, train loss: 2.5979, val loss: 2.6187\n",
      "139: lr: 0.0100, train loss: 2.5983, val loss: 2.5450\n",
      "140: lr: 0.0100, train loss: 2.5733, val loss: 2.5863\n",
      "141: lr: 0.0100, train loss: 2.6362, val loss: 2.5640\n",
      "142: lr: 0.0100, train loss: 2.5470, val loss: 2.5753\n",
      "143: lr: 0.0100, train loss: 2.6030, val loss: 2.6208\n",
      "144: lr: 0.0100, train loss: 2.6311, val loss: 2.5960\n",
      "145: lr: 0.0100, train loss: 2.6165, val loss: 2.5706\n",
      "146: lr: 0.0100, train loss: 2.5969, val loss: 2.5677\n",
      "147: lr: 0.0100, train loss: 2.5563, val loss: 2.6515\n",
      "148: lr: 0.0100, train loss: 2.6137, val loss: 2.6154\n",
      "149: lr: 0.0100, train loss: 2.6171, val loss: 2.5590\n",
      "150: lr: 0.0100, train loss: 2.5857, val loss: 2.5449\n",
      "151: lr: 0.0100, train loss: 2.5585, val loss: 2.5396\n",
      "152: lr: 0.0100, train loss: 2.5984, val loss: 2.5785\n",
      "153: lr: 0.0100, train loss: 2.6170, val loss: 2.5676\n",
      "154: lr: 0.0100, train loss: 2.5719, val loss: 2.5420\n",
      "155: lr: 0.0100, train loss: 2.5441, val loss: 2.5606\n",
      "156: lr: 0.0100, train loss: 2.5523, val loss: 2.5481\n",
      "157: lr: 0.0100, train loss: 2.5610, val loss: 2.5901\n",
      "158: lr: 0.0100, train loss: 2.5711, val loss: 2.5250\n",
      "159: lr: 0.0100, train loss: 2.5317, val loss: 2.5622\n",
      "160: lr: 0.0100, train loss: 2.5388, val loss: 2.5830\n",
      "161: lr: 0.0100, train loss: 2.5379, val loss: 2.5393\n",
      "162: lr: 0.0100, train loss: 2.5773, val loss: 2.5407\n",
      "163: lr: 0.0100, train loss: 2.5869, val loss: 2.5680\n",
      "164: lr: 0.0100, train loss: 2.5554, val loss: 2.5071\n",
      "165: lr: 0.0100, train loss: 2.5793, val loss: 2.5574\n",
      "166: lr: 0.0100, train loss: 2.6000, val loss: 2.5629\n",
      "167: lr: 0.0100, train loss: 2.5984, val loss: 2.5769\n",
      "168: lr: 0.0100, train loss: 2.5280, val loss: 2.6051\n",
      "169: lr: 0.0100, train loss: 2.5354, val loss: 2.5323\n",
      "170: lr: 0.0100, train loss: 2.5365, val loss: 2.6091\n",
      "171: lr: 0.0100, train loss: 2.5689, val loss: 2.5879\n",
      "172: lr: 0.0100, train loss: 2.5343, val loss: 2.5613\n",
      "173: lr: 0.0100, train loss: 2.5068, val loss: 2.5157\n",
      "174: lr: 0.0100, train loss: 2.5430, val loss: 2.5564\n",
      "175: lr: 0.0100, train loss: 2.5720, val loss: 2.5180\n",
      "176: lr: 0.0100, train loss: 2.5353, val loss: 2.5309\n",
      "177: lr: 0.0100, train loss: 2.5243, val loss: 2.5517\n",
      "178: lr: 0.0100, train loss: 2.5214, val loss: 2.4850\n",
      "179: lr: 0.0100, train loss: 2.5366, val loss: 2.5192\n",
      "180: lr: 0.0100, train loss: 2.4799, val loss: 2.5926\n",
      "181: lr: 0.0100, train loss: 2.5525, val loss: 2.5791\n",
      "182: lr: 0.0100, train loss: 2.5295, val loss: 2.5697\n",
      "183: lr: 0.0100, train loss: 2.5320, val loss: 2.5417\n",
      "184: lr: 0.0100, train loss: 2.5590, val loss: 2.5532\n",
      "185: lr: 0.0100, train loss: 2.5916, val loss: 2.5615\n",
      "186: lr: 0.0100, train loss: 2.5887, val loss: 2.5537\n",
      "187: lr: 0.0100, train loss: 2.5816, val loss: 2.5631\n",
      "188: lr: 0.0100, train loss: 2.5038, val loss: 2.5628\n",
      "189: lr: 0.0100, train loss: 2.5209, val loss: 2.5931\n",
      "190: lr: 0.0100, train loss: 2.5346, val loss: 2.5045\n",
      "191: lr: 0.0100, train loss: 2.4938, val loss: 2.5671\n",
      "192: lr: 0.0100, train loss: 2.5684, val loss: 2.5391\n",
      "193: lr: 0.0100, train loss: 2.5675, val loss: 2.4904\n",
      "194: lr: 0.0100, train loss: 2.5839, val loss: 2.5727\n",
      "195: lr: 0.0100, train loss: 2.5343, val loss: 2.5650\n",
      "196: lr: 0.0100, train loss: 2.5590, val loss: 2.5374\n",
      "197: lr: 0.0100, train loss: 2.5092, val loss: 2.5423\n",
      "198: lr: 0.0100, train loss: 2.5102, val loss: 2.4698\n",
      "199: lr: 0.0100, train loss: 2.5150, val loss: 2.5789\n",
      "200: lr: 0.0100, train loss: 2.5156, val loss: 2.5809\n",
      "201: lr: 0.0100, train loss: 2.5233, val loss: 2.5395\n",
      "202: lr: 0.0100, train loss: 2.4730, val loss: 2.5130\n",
      "203: lr: 0.0100, train loss: 2.5452, val loss: 2.5270\n",
      "204: lr: 0.0100, train loss: 2.5427, val loss: 2.5416\n",
      "205: lr: 0.0100, train loss: 2.5601, val loss: 2.5461\n",
      "206: lr: 0.0100, train loss: 2.5764, val loss: 2.5295\n",
      "207: lr: 0.0100, train loss: 2.5800, val loss: 2.5042\n",
      "208: lr: 0.0100, train loss: 2.5089, val loss: 2.5827\n",
      "209: lr: 0.0100, train loss: 2.5483, val loss: 2.5251\n",
      "210: lr: 0.0100, train loss: 2.5182, val loss: 2.5000\n",
      "211: lr: 0.0100, train loss: 2.5288, val loss: 2.5849\n",
      "212: lr: 0.0100, train loss: 2.5492, val loss: 2.5158\n",
      "213: lr: 0.0100, train loss: 2.5444, val loss: 2.5134\n",
      "214: lr: 0.0100, train loss: 2.5460, val loss: 2.5051\n",
      "215: lr: 0.0100, train loss: 2.5355, val loss: 2.5066\n",
      "216: lr: 0.0100, train loss: 2.5615, val loss: 2.5301\n",
      "217: lr: 0.0100, train loss: 2.5300, val loss: 2.5375\n",
      "218: lr: 0.0100, train loss: 2.5548, val loss: 2.5828\n",
      "219: lr: 0.0100, train loss: 2.5237, val loss: 2.5529\n",
      "220: lr: 0.0100, train loss: 2.4981, val loss: 2.5444\n",
      "221: lr: 0.0100, train loss: 2.5128, val loss: 2.5499\n",
      "222: lr: 0.0100, train loss: 2.4887, val loss: 2.5423\n",
      "223: lr: 0.0100, train loss: 2.5409, val loss: 2.5831\n",
      "224: lr: 0.0100, train loss: 2.5084, val loss: 2.5867\n",
      "225: lr: 0.0100, train loss: 2.5443, val loss: 2.4953\n",
      "226: lr: 0.0100, train loss: 2.5108, val loss: 2.5727\n",
      "227: lr: 0.0100, train loss: 2.5784, val loss: 2.4804\n",
      "228: lr: 0.0100, train loss: 2.5297, val loss: 2.5447\n",
      "229: lr: 0.0100, train loss: 2.5434, val loss: 2.4644\n",
      "230: lr: 0.0100, train loss: 2.4989, val loss: 2.5365\n",
      "231: lr: 0.0100, train loss: 2.5689, val loss: 2.4793\n",
      "232: lr: 0.0100, train loss: 2.4887, val loss: 2.5365\n",
      "233: lr: 0.0100, train loss: 2.5417, val loss: 2.5572\n",
      "234: lr: 0.0100, train loss: 2.5411, val loss: 2.5021\n",
      "235: lr: 0.0100, train loss: 2.5266, val loss: 2.5354\n",
      "236: lr: 0.0100, train loss: 2.5106, val loss: 2.5261\n",
      "237: lr: 0.0100, train loss: 2.5424, val loss: 2.5393\n",
      "238: lr: 0.0100, train loss: 2.4758, val loss: 2.5475\n",
      "239: lr: 0.0100, train loss: 2.4907, val loss: 2.5215\n",
      "240: lr: 0.0100, train loss: 2.5424, val loss: 2.4617\n",
      "241: lr: 0.0100, train loss: 2.4984, val loss: 2.5140\n",
      "242: lr: 0.0100, train loss: 2.5145, val loss: 2.5072\n",
      "243: lr: 0.0100, train loss: 2.5424, val loss: 2.5412\n",
      "244: lr: 0.0100, train loss: 2.5226, val loss: 2.4827\n",
      "245: lr: 0.0100, train loss: 2.5120, val loss: 2.5518\n",
      "246: lr: 0.0100, train loss: 2.5095, val loss: 2.4823\n",
      "247: lr: 0.0100, train loss: 2.5314, val loss: 2.5350\n",
      "248: lr: 0.0100, train loss: 2.5181, val loss: 2.5287\n",
      "249: lr: 0.0100, train loss: 2.5055, val loss: 2.4763\n",
      "250: lr: 0.0100, train loss: 2.5435, val loss: 2.5156\n",
      "251: lr: 0.0100, train loss: 2.5091, val loss: 2.5284\n",
      "252: lr: 0.0100, train loss: 2.5039, val loss: 2.5527\n",
      "253: lr: 0.0100, train loss: 2.4952, val loss: 2.5645\n",
      "254: lr: 0.0100, train loss: 2.5207, val loss: 2.4899\n",
      "255: lr: 0.0100, train loss: 2.5048, val loss: 2.4744\n",
      "256: lr: 0.0100, train loss: 2.5246, val loss: 2.5028\n",
      "257: lr: 0.0100, train loss: 2.4751, val loss: 2.5112\n",
      "258: lr: 0.0100, train loss: 2.4694, val loss: 2.5705\n",
      "259: lr: 0.0100, train loss: 2.5257, val loss: 2.5363\n",
      "260: lr: 0.0100, train loss: 2.4988, val loss: 2.5257\n",
      "261: lr: 0.0100, train loss: 2.4734, val loss: 2.5626\n",
      "262: lr: 0.0100, train loss: 2.4571, val loss: 2.4672\n",
      "263: lr: 0.0100, train loss: 2.4909, val loss: 2.4934\n",
      "264: lr: 0.0100, train loss: 2.5253, val loss: 2.5282\n",
      "265: lr: 0.0100, train loss: 2.5532, val loss: 2.5242\n",
      "266: lr: 0.0100, train loss: 2.5038, val loss: 2.4825\n",
      "267: lr: 0.0100, train loss: 2.4979, val loss: 2.4896\n",
      "268: lr: 0.0100, train loss: 2.4782, val loss: 2.5266\n",
      "269: lr: 0.0100, train loss: 2.5436, val loss: 2.5615\n",
      "270: lr: 0.0100, train loss: 2.5219, val loss: 2.5168\n",
      "271: lr: 0.0100, train loss: 2.5199, val loss: 2.5145\n",
      "272: lr: 0.0100, train loss: 2.5346, val loss: 2.4886\n",
      "273: lr: 0.0100, train loss: 2.4882, val loss: 2.4920\n",
      "274: lr: 0.0100, train loss: 2.5356, val loss: 2.5057\n",
      "275: lr: 0.0100, train loss: 2.5244, val loss: 2.5200\n",
      "276: lr: 0.0100, train loss: 2.5037, val loss: 2.4830\n",
      "277: lr: 0.0100, train loss: 2.5146, val loss: 2.4880\n",
      "278: lr: 0.0100, train loss: 2.5457, val loss: 2.5013\n",
      "279: lr: 0.0100, train loss: 2.5040, val loss: 2.4931\n",
      "280: lr: 0.0100, train loss: 2.4802, val loss: 2.4966\n",
      "281: lr: 0.0100, train loss: 2.5510, val loss: 2.5431\n",
      "282: lr: 0.0100, train loss: 2.5028, val loss: 2.5217\n",
      "283: lr: 0.0100, train loss: 2.5321, val loss: 2.5076\n",
      "284: lr: 0.0100, train loss: 2.5223, val loss: 2.5169\n",
      "285: lr: 0.0100, train loss: 2.5040, val loss: 2.5028\n",
      "286: lr: 0.0100, train loss: 2.5036, val loss: 2.4874\n",
      "287: lr: 0.0100, train loss: 2.5146, val loss: 2.5132\n",
      "288: lr: 0.0100, train loss: 2.5197, val loss: 2.4625\n",
      "289: lr: 0.0100, train loss: 2.5269, val loss: 2.5604\n",
      "290: lr: 0.0100, train loss: 2.5240, val loss: 2.5020\n",
      "291: lr: 0.0100, train loss: 2.5003, val loss: 2.4966\n",
      "292: lr: 0.0100, train loss: 2.4957, val loss: 2.5066\n",
      "293: lr: 0.0100, train loss: 2.5812, val loss: 2.5041\n",
      "294: lr: 0.0100, train loss: 2.4386, val loss: 2.4797\n",
      "295: lr: 0.0100, train loss: 2.4890, val loss: 2.5320\n",
      "296: lr: 0.0100, train loss: 2.5374, val loss: 2.5594\n",
      "297: lr: 0.0100, train loss: 2.4881, val loss: 2.5365\n",
      "298: lr: 0.0100, train loss: 2.4697, val loss: 2.5341\n",
      "299: lr: 0.0100, train loss: 2.5251, val loss: 2.4760\n",
      "300: lr: 0.0100, train loss: 2.5039, val loss: 2.5299\n",
      "301: lr: 0.0100, train loss: 2.4832, val loss: 2.5227\n",
      "302: lr: 0.0100, train loss: 2.5068, val loss: 2.5054\n",
      "303: lr: 0.0100, train loss: 2.5117, val loss: 2.5211\n",
      "304: lr: 0.0100, train loss: 2.4932, val loss: 2.5205\n",
      "305: lr: 0.0100, train loss: 2.4906, val loss: 2.5166\n",
      "306: lr: 0.0100, train loss: 2.4861, val loss: 2.4783\n",
      "307: lr: 0.0100, train loss: 2.4644, val loss: 2.4858\n",
      "308: lr: 0.0100, train loss: 2.5366, val loss: 2.5096\n",
      "309: lr: 0.0100, train loss: 2.4929, val loss: 2.5250\n",
      "310: lr: 0.0100, train loss: 2.5003, val loss: 2.4268\n",
      "311: lr: 0.0100, train loss: 2.4962, val loss: 2.5418\n",
      "312: lr: 0.0100, train loss: 2.5016, val loss: 2.5057\n",
      "313: lr: 0.0100, train loss: 2.4719, val loss: 2.5319\n",
      "314: lr: 0.0100, train loss: 2.5208, val loss: 2.4672\n",
      "315: lr: 0.0100, train loss: 2.4776, val loss: 2.4547\n",
      "316: lr: 0.0100, train loss: 2.4741, val loss: 2.4730\n",
      "317: lr: 0.0100, train loss: 2.4475, val loss: 2.5090\n",
      "318: lr: 0.0100, train loss: 2.4376, val loss: 2.5319\n",
      "319: lr: 0.0100, train loss: 2.5089, val loss: 2.5326\n",
      "320: lr: 0.0100, train loss: 2.4565, val loss: 2.4866\n",
      "321: lr: 0.0100, train loss: 2.4690, val loss: 2.5185\n",
      "322: lr: 0.0100, train loss: 2.4930, val loss: 2.4706\n",
      "323: lr: 0.0100, train loss: 2.4934, val loss: 2.4929\n",
      "324: lr: 0.0100, train loss: 2.4924, val loss: 2.5161\n",
      "325: lr: 0.0100, train loss: 2.4991, val loss: 2.4820\n",
      "326: lr: 0.0100, train loss: 2.4773, val loss: 2.4854\n",
      "327: lr: 0.0100, train loss: 2.4778, val loss: 2.4440\n",
      "328: lr: 0.0100, train loss: 2.4324, val loss: 2.4955\n",
      "329: lr: 0.0100, train loss: 2.4631, val loss: 2.4873\n",
      "330: lr: 0.0100, train loss: 2.4796, val loss: 2.4890\n",
      "331: lr: 0.0100, train loss: 2.5324, val loss: 2.5138\n",
      "332: lr: 0.0100, train loss: 2.4830, val loss: 2.5379\n",
      "333: lr: 0.0100, train loss: 2.4617, val loss: 2.5121\n",
      "334: lr: 0.0100, train loss: 2.5038, val loss: 2.4963\n",
      "335: lr: 0.0100, train loss: 2.4839, val loss: 2.4550\n",
      "336: lr: 0.0100, train loss: 2.4806, val loss: 2.5527\n",
      "337: lr: 0.0100, train loss: 2.4475, val loss: 2.4977\n",
      "338: lr: 0.0100, train loss: 2.4574, val loss: 2.4837\n",
      "339: lr: 0.0100, train loss: 2.4689, val loss: 2.5387\n",
      "340: lr: 0.0100, train loss: 2.5001, val loss: 2.4927\n",
      "341: lr: 0.0100, train loss: 2.4652, val loss: 2.4634\n",
      "342: lr: 0.0100, train loss: 2.4583, val loss: 2.4892\n",
      "343: lr: 0.0100, train loss: 2.4292, val loss: 2.4825\n",
      "344: lr: 0.0100, train loss: 2.4805, val loss: 2.4727\n",
      "345: lr: 0.0100, train loss: 2.5289, val loss: 2.5042\n",
      "346: lr: 0.0100, train loss: 2.4953, val loss: 2.4983\n",
      "347: lr: 0.0100, train loss: 2.4762, val loss: 2.4584\n",
      "348: lr: 0.0100, train loss: 2.5086, val loss: 2.4886\n",
      "349: lr: 0.0100, train loss: 2.5032, val loss: 2.5254\n",
      "350: lr: 0.0100, train loss: 2.4544, val loss: 2.5003\n",
      "351: lr: 0.0100, train loss: 2.4872, val loss: 2.5519\n",
      "352: lr: 0.0100, train loss: 2.5015, val loss: 2.5229\n",
      "353: lr: 0.0100, train loss: 2.5287, val loss: 2.4866\n",
      "354: lr: 0.0100, train loss: 2.5128, val loss: 2.4956\n",
      "355: lr: 0.0100, train loss: 2.4580, val loss: 2.5390\n",
      "356: lr: 0.0100, train loss: 2.4683, val loss: 2.5031\n",
      "357: lr: 0.0100, train loss: 2.4881, val loss: 2.4500\n",
      "358: lr: 0.0100, train loss: 2.5240, val loss: 2.5183\n",
      "359: lr: 0.0100, train loss: 2.5239, val loss: 2.5389\n",
      "360: lr: 0.0100, train loss: 2.5286, val loss: 2.4813\n",
      "361: lr: 0.0100, train loss: 2.4850, val loss: 2.5038\n",
      "362: lr: 0.0100, train loss: 2.4927, val loss: 2.5085\n",
      "363: lr: 0.0100, train loss: 2.4652, val loss: 2.4812\n",
      "364: lr: 0.0100, train loss: 2.4189, val loss: 2.5702\n",
      "365: lr: 0.0100, train loss: 2.4971, val loss: 2.4857\n",
      "366: lr: 0.0100, train loss: 2.4862, val loss: 2.5049\n",
      "367: lr: 0.0100, train loss: 2.4975, val loss: 2.4600\n",
      "368: lr: 0.0100, train loss: 2.4704, val loss: 2.4723\n",
      "369: lr: 0.0100, train loss: 2.4985, val loss: 2.5226\n",
      "370: lr: 0.0100, train loss: 2.4725, val loss: 2.4532\n",
      "371: lr: 0.0100, train loss: 2.4400, val loss: 2.4795\n",
      "372: lr: 0.0100, train loss: 2.4872, val loss: 2.4943\n",
      "373: lr: 0.0100, train loss: 2.4702, val loss: 2.4917\n",
      "374: lr: 0.0100, train loss: 2.4753, val loss: 2.4889\n",
      "375: lr: 0.0100, train loss: 2.4195, val loss: 2.4975\n",
      "376: lr: 0.0100, train loss: 2.4889, val loss: 2.5297\n",
      "377: lr: 0.0100, train loss: 2.5007, val loss: 2.5008\n",
      "378: lr: 0.0100, train loss: 2.4842, val loss: 2.5355\n",
      "379: lr: 0.0100, train loss: 2.4845, val loss: 2.5115\n",
      "380: lr: 0.0100, train loss: 2.4977, val loss: 2.4748\n",
      "381: lr: 0.0100, train loss: 2.4484, val loss: 2.4652\n",
      "382: lr: 0.0100, train loss: 2.5463, val loss: 2.5003\n",
      "383: lr: 0.0100, train loss: 2.4924, val loss: 2.4538\n",
      "384: lr: 0.0100, train loss: 2.4755, val loss: 2.4777\n",
      "385: lr: 0.0100, train loss: 2.4666, val loss: 2.5080\n",
      "386: lr: 0.0100, train loss: 2.4154, val loss: 2.5527\n",
      "387: lr: 0.0100, train loss: 2.5210, val loss: 2.4860\n",
      "388: lr: 0.0100, train loss: 2.5216, val loss: 2.4250\n",
      "389: lr: 0.0100, train loss: 2.4657, val loss: 2.5184\n",
      "390: lr: 0.0100, train loss: 2.4608, val loss: 2.5342\n",
      "391: lr: 0.0100, train loss: 2.5242, val loss: 2.5147\n",
      "392: lr: 0.0100, train loss: 2.4777, val loss: 2.4998\n",
      "393: lr: 0.0100, train loss: 2.5069, val loss: 2.4921\n",
      "394: lr: 0.0100, train loss: 2.4756, val loss: 2.5406\n",
      "395: lr: 0.0100, train loss: 2.5059, val loss: 2.5458\n",
      "396: lr: 0.0100, train loss: 2.4787, val loss: 2.4952\n",
      "397: lr: 0.0100, train loss: 2.5264, val loss: 2.4806\n",
      "398: lr: 0.0100, train loss: 2.4302, val loss: 2.4685\n",
      "399: lr: 0.0100, train loss: 2.5072, val loss: 2.4609\n",
      "400: lr: 0.0100, train loss: 2.5014, val loss: 2.5216\n",
      "401: lr: 0.0100, train loss: 2.4994, val loss: 2.4854\n",
      "402: lr: 0.0100, train loss: 2.4667, val loss: 2.4947\n",
      "403: lr: 0.0100, train loss: 2.5077, val loss: 2.5094\n",
      "404: lr: 0.0100, train loss: 2.4806, val loss: 2.5032\n",
      "405: lr: 0.0100, train loss: 2.4833, val loss: 2.4935\n",
      "406: lr: 0.0100, train loss: 2.4495, val loss: 2.4814\n",
      "407: lr: 0.0100, train loss: 2.4763, val loss: 2.4789\n",
      "408: lr: 0.0100, train loss: 2.4358, val loss: 2.4156\n",
      "409: lr: 0.0100, train loss: 2.5021, val loss: 2.5095\n",
      "410: lr: 0.0100, train loss: 2.4744, val loss: 2.4672\n",
      "411: lr: 0.0100, train loss: 2.4895, val loss: 2.5028\n",
      "412: lr: 0.0100, train loss: 2.4518, val loss: 2.4774\n",
      "413: lr: 0.0100, train loss: 2.4761, val loss: 2.5284\n",
      "414: lr: 0.0100, train loss: 2.4579, val loss: 2.4550\n",
      "415: lr: 0.0100, train loss: 2.5017, val loss: 2.4500\n",
      "416: lr: 0.0100, train loss: 2.4373, val loss: 2.4869\n",
      "417: lr: 0.0100, train loss: 2.4583, val loss: 2.4470\n",
      "418: lr: 0.0100, train loss: 2.4918, val loss: 2.4626\n",
      "419: lr: 0.0100, train loss: 2.4944, val loss: 2.4842\n",
      "420: lr: 0.0100, train loss: 2.5075, val loss: 2.4552\n",
      "421: lr: 0.0100, train loss: 2.5237, val loss: 2.4280\n",
      "422: lr: 0.0100, train loss: 2.4796, val loss: 2.4916\n",
      "423: lr: 0.0100, train loss: 2.5216, val loss: 2.4567\n",
      "424: lr: 0.0100, train loss: 2.4981, val loss: 2.5023\n",
      "425: lr: 0.0100, train loss: 2.4633, val loss: 2.5319\n",
      "426: lr: 0.0100, train loss: 2.4098, val loss: 2.4949\n",
      "427: lr: 0.0100, train loss: 2.4778, val loss: 2.4606\n",
      "428: lr: 0.0100, train loss: 2.4596, val loss: 2.5084\n",
      "429: lr: 0.0100, train loss: 2.4759, val loss: 2.4931\n",
      "430: lr: 0.0100, train loss: 2.5227, val loss: 2.4614\n",
      "431: lr: 0.0100, train loss: 2.4841, val loss: 2.4610\n",
      "432: lr: 0.0100, train loss: 2.5108, val loss: 2.5013\n",
      "433: lr: 0.0100, train loss: 2.4707, val loss: 2.5306\n",
      "434: lr: 0.0100, train loss: 2.4795, val loss: 2.5148\n",
      "435: lr: 0.0100, train loss: 2.5250, val loss: 2.4364\n",
      "436: lr: 0.0100, train loss: 2.4397, val loss: 2.4926\n",
      "437: lr: 0.0100, train loss: 2.4866, val loss: 2.4995\n",
      "438: lr: 0.0100, train loss: 2.5018, val loss: 2.4263\n",
      "439: lr: 0.0100, train loss: 2.4469, val loss: 2.4533\n",
      "440: lr: 0.0100, train loss: 2.4828, val loss: 2.5520\n",
      "441: lr: 0.0100, train loss: 2.4747, val loss: 2.5316\n",
      "442: lr: 0.0100, train loss: 2.4807, val loss: 2.4712\n",
      "443: lr: 0.0100, train loss: 2.5168, val loss: 2.4654\n",
      "444: lr: 0.0100, train loss: 2.5037, val loss: 2.4656\n",
      "445: lr: 0.0100, train loss: 2.4765, val loss: 2.4595\n",
      "446: lr: 0.0100, train loss: 2.5142, val loss: 2.4987\n",
      "447: lr: 0.0100, train loss: 2.5231, val loss: 2.5197\n",
      "448: lr: 0.0100, train loss: 2.4485, val loss: 2.4842\n",
      "449: lr: 0.0100, train loss: 2.4929, val loss: 2.4795\n",
      "450: lr: 0.0100, train loss: 2.4612, val loss: 2.5256\n",
      "451: lr: 0.0100, train loss: 2.4791, val loss: 2.5358\n",
      "452: lr: 0.0100, train loss: 2.5004, val loss: 2.5065\n",
      "453: lr: 0.0100, train loss: 2.4319, val loss: 2.4923\n",
      "454: lr: 0.0100, train loss: 2.4327, val loss: 2.4786\n",
      "455: lr: 0.0100, train loss: 2.5065, val loss: 2.4907\n",
      "456: lr: 0.0100, train loss: 2.5202, val loss: 2.4470\n",
      "457: lr: 0.0100, train loss: 2.4658, val loss: 2.4952\n",
      "458: lr: 0.0100, train loss: 2.5033, val loss: 2.4759\n",
      "459: lr: 0.0100, train loss: 2.4809, val loss: 2.4815\n",
      "460: lr: 0.0100, train loss: 2.4912, val loss: 2.4716\n",
      "461: lr: 0.0100, train loss: 2.4740, val loss: 2.4914\n",
      "462: lr: 0.0100, train loss: 2.4337, val loss: 2.4576\n",
      "463: lr: 0.0100, train loss: 2.4664, val loss: 2.5047\n",
      "464: lr: 0.0100, train loss: 2.5466, val loss: 2.4874\n",
      "465: lr: 0.0100, train loss: 2.5023, val loss: 2.4629\n",
      "466: lr: 0.0100, train loss: 2.4763, val loss: 2.4971\n",
      "467: lr: 0.0100, train loss: 2.4663, val loss: 2.4877\n",
      "468: lr: 0.0100, train loss: 2.4526, val loss: 2.4469\n",
      "469: lr: 0.0100, train loss: 2.4746, val loss: 2.4593\n",
      "470: lr: 0.0100, train loss: 2.4903, val loss: 2.4488\n",
      "471: lr: 0.0100, train loss: 2.4809, val loss: 2.4911\n",
      "472: lr: 0.0100, train loss: 2.4882, val loss: 2.5030\n",
      "473: lr: 0.0100, train loss: 2.4352, val loss: 2.5039\n",
      "474: lr: 0.0100, train loss: 2.4293, val loss: 2.5140\n",
      "475: lr: 0.0100, train loss: 2.5098, val loss: 2.5116\n",
      "476: lr: 0.0100, train loss: 2.4630, val loss: 2.4962\n",
      "477: lr: 0.0100, train loss: 2.4678, val loss: 2.5216\n",
      "478: lr: 0.0100, train loss: 2.5231, val loss: 2.4737\n",
      "479: lr: 0.0100, train loss: 2.4644, val loss: 2.5077\n",
      "480: lr: 0.0100, train loss: 2.4918, val loss: 2.4557\n",
      "481: lr: 0.0100, train loss: 2.4748, val loss: 2.4959\n",
      "482: lr: 0.0100, train loss: 2.4508, val loss: 2.4266\n",
      "483: lr: 0.0100, train loss: 2.4949, val loss: 2.4770\n",
      "484: lr: 0.0100, train loss: 2.4876, val loss: 2.4689\n",
      "485: lr: 0.0100, train loss: 2.4706, val loss: 2.4567\n",
      "486: lr: 0.0100, train loss: 2.4909, val loss: 2.4843\n",
      "487: lr: 0.0100, train loss: 2.4705, val loss: 2.4895\n",
      "488: lr: 0.0100, train loss: 2.4438, val loss: 2.4771\n",
      "489: lr: 0.0100, train loss: 2.4858, val loss: 2.5127\n",
      "490: lr: 0.0100, train loss: 2.4619, val loss: 2.4117\n",
      "491: lr: 0.0100, train loss: 2.4423, val loss: 2.4828\n",
      "492: lr: 0.0100, train loss: 2.4701, val loss: 2.4762\n",
      "493: lr: 0.0100, train loss: 2.4203, val loss: 2.5118\n",
      "494: lr: 0.0100, train loss: 2.4228, val loss: 2.4695\n",
      "495: lr: 0.0100, train loss: 2.4874, val loss: 2.4850\n",
      "496: lr: 0.0100, train loss: 2.4513, val loss: 2.4923\n",
      "497: lr: 0.0100, train loss: 2.4435, val loss: 2.4671\n",
      "498: lr: 0.0100, train loss: 2.4789, val loss: 2.5116\n",
      "499: lr: 0.0100, train loss: 2.4402, val loss: 2.4734\n",
      "500: lr: 0.0100, train loss: 2.4632, val loss: 2.4756\n",
      "501: lr: 0.0100, train loss: 2.4859, val loss: 2.5431\n",
      "502: lr: 0.0100, train loss: 2.4820, val loss: 2.5063\n",
      "503: lr: 0.0100, train loss: 2.4448, val loss: 2.4884\n",
      "504: lr: 0.0100, train loss: 2.3874, val loss: 2.4897\n",
      "505: lr: 0.0100, train loss: 2.5010, val loss: 2.4855\n",
      "506: lr: 0.0100, train loss: 2.4745, val loss: 2.4713\n",
      "507: lr: 0.0100, train loss: 2.4905, val loss: 2.4764\n",
      "508: lr: 0.0100, train loss: 2.4687, val loss: 2.4731\n",
      "509: lr: 0.0100, train loss: 2.4910, val loss: 2.4651\n",
      "510: lr: 0.0100, train loss: 2.4886, val loss: 2.4692\n",
      "511: lr: 0.0100, train loss: 2.4460, val loss: 2.4788\n",
      "512: lr: 0.0100, train loss: 2.5440, val loss: 2.5182\n",
      "513: lr: 0.0100, train loss: 2.4432, val loss: 2.4263\n",
      "514: lr: 0.0100, train loss: 2.4498, val loss: 2.5269\n",
      "515: lr: 0.0100, train loss: 2.4967, val loss: 2.4748\n",
      "516: lr: 0.0100, train loss: 2.4858, val loss: 2.4533\n",
      "517: lr: 0.0100, train loss: 2.5026, val loss: 2.4867\n",
      "518: lr: 0.0100, train loss: 2.5054, val loss: 2.4990\n",
      "519: lr: 0.0100, train loss: 2.4513, val loss: 2.5351\n",
      "520: lr: 0.0100, train loss: 2.4727, val loss: 2.4987\n",
      "521: lr: 0.0100, train loss: 2.4437, val loss: 2.4950\n",
      "522: lr: 0.0100, train loss: 2.4758, val loss: 2.4760\n",
      "523: lr: 0.0100, train loss: 2.4677, val loss: 2.5205\n",
      "524: lr: 0.0100, train loss: 2.5162, val loss: 2.4827\n",
      "525: lr: 0.0100, train loss: 2.4834, val loss: 2.5066\n",
      "526: lr: 0.0100, train loss: 2.4761, val loss: 2.4233\n",
      "527: lr: 0.0100, train loss: 2.4770, val loss: 2.5146\n",
      "528: lr: 0.0100, train loss: 2.5007, val loss: 2.4825\n",
      "529: lr: 0.0100, train loss: 2.4478, val loss: 2.4507\n",
      "530: lr: 0.0100, train loss: 2.4485, val loss: 2.4793\n",
      "531: lr: 0.0100, train loss: 2.4332, val loss: 2.5050\n",
      "532: lr: 0.0100, train loss: 2.4707, val loss: 2.5324\n",
      "533: lr: 0.0100, train loss: 2.4832, val loss: 2.4880\n",
      "534: lr: 0.0100, train loss: 2.4705, val loss: 2.4716\n",
      "535: lr: 0.0100, train loss: 2.4673, val loss: 2.4729\n",
      "536: lr: 0.0100, train loss: 2.4938, val loss: 2.5078\n",
      "537: lr: 0.0100, train loss: 2.4631, val loss: 2.5438\n",
      "538: lr: 0.0100, train loss: 2.4591, val loss: 2.4748\n",
      "539: lr: 0.0100, train loss: 2.4788, val loss: 2.5059\n",
      "540: lr: 0.0100, train loss: 2.5174, val loss: 2.5399\n",
      "541: lr: 0.0100, train loss: 2.4977, val loss: 2.4579\n",
      "542: lr: 0.0100, train loss: 2.4756, val loss: 2.4427\n",
      "543: lr: 0.0100, train loss: 2.4456, val loss: 2.5144\n",
      "544: lr: 0.0100, train loss: 2.4614, val loss: 2.4760\n",
      "545: lr: 0.0100, train loss: 2.4958, val loss: 2.5081\n",
      "546: lr: 0.0100, train loss: 2.5356, val loss: 2.4784\n",
      "547: lr: 0.0100, train loss: 2.4938, val loss: 2.5123\n",
      "548: lr: 0.0100, train loss: 2.5297, val loss: 2.4643\n",
      "549: lr: 0.0100, train loss: 2.4727, val loss: 2.5240\n",
      "550: lr: 0.0100, train loss: 2.4540, val loss: 2.5037\n",
      "551: lr: 0.0100, train loss: 2.4651, val loss: 2.4894\n",
      "552: lr: 0.0100, train loss: 2.5159, val loss: 2.5023\n",
      "553: lr: 0.0100, train loss: 2.4501, val loss: 2.4827\n",
      "554: lr: 0.0100, train loss: 2.4512, val loss: 2.5099\n",
      "555: lr: 0.0100, train loss: 2.4611, val loss: 2.5150\n",
      "556: lr: 0.0100, train loss: 2.5295, val loss: 2.4417\n",
      "557: lr: 0.0100, train loss: 2.4847, val loss: 2.4874\n",
      "558: lr: 0.0100, train loss: 2.4547, val loss: 2.4676\n",
      "559: lr: 0.0100, train loss: 2.5083, val loss: 2.4624\n",
      "560: lr: 0.0100, train loss: 2.4345, val loss: 2.4980\n",
      "561: lr: 0.0100, train loss: 2.4627, val loss: 2.5234\n",
      "562: lr: 0.0100, train loss: 2.4920, val loss: 2.4942\n",
      "563: lr: 0.0100, train loss: 2.4930, val loss: 2.4147\n",
      "564: lr: 0.0100, train loss: 2.4240, val loss: 2.4807\n",
      "565: lr: 0.0100, train loss: 2.4703, val loss: 2.5140\n",
      "566: lr: 0.0100, train loss: 2.4645, val loss: 2.4922\n",
      "567: lr: 0.0100, train loss: 2.4398, val loss: 2.4703\n",
      "568: lr: 0.0100, train loss: 2.4893, val loss: 2.5458\n",
      "569: lr: 0.0100, train loss: 2.4929, val loss: 2.4569\n",
      "570: lr: 0.0100, train loss: 2.4809, val loss: 2.4470\n",
      "571: lr: 0.0100, train loss: 2.5013, val loss: 2.5089\n",
      "572: lr: 0.0100, train loss: 2.4728, val loss: 2.4659\n",
      "573: lr: 0.0100, train loss: 2.4597, val loss: 2.4770\n",
      "574: lr: 0.0100, train loss: 2.4463, val loss: 2.5051\n",
      "575: lr: 0.0100, train loss: 2.4497, val loss: 2.4807\n",
      "576: lr: 0.0100, train loss: 2.4830, val loss: 2.4495\n",
      "577: lr: 0.0100, train loss: 2.4695, val loss: 2.4509\n",
      "578: lr: 0.0100, train loss: 2.4880, val loss: 2.4627\n",
      "579: lr: 0.0100, train loss: 2.4511, val loss: 2.4572\n",
      "580: lr: 0.0100, train loss: 2.4776, val loss: 2.5012\n",
      "581: lr: 0.0100, train loss: 2.4881, val loss: 2.4831\n",
      "582: lr: 0.0100, train loss: 2.4666, val loss: 2.5212\n",
      "583: lr: 0.0100, train loss: 2.4977, val loss: 2.5249\n",
      "584: lr: 0.0100, train loss: 2.4970, val loss: 2.5080\n",
      "585: lr: 0.0100, train loss: 2.4556, val loss: 2.4857\n",
      "586: lr: 0.0100, train loss: 2.4842, val loss: 2.4784\n",
      "587: lr: 0.0100, train loss: 2.4507, val loss: 2.4896\n",
      "588: lr: 0.0100, train loss: 2.4345, val loss: 2.4771\n",
      "589: lr: 0.0100, train loss: 2.4667, val loss: 2.5408\n",
      "590: lr: 0.0100, train loss: 2.4772, val loss: 2.4558\n",
      "591: lr: 0.0100, train loss: 2.4716, val loss: 2.4849\n",
      "592: lr: 0.0100, train loss: 2.4508, val loss: 2.4566\n",
      "593: lr: 0.0100, train loss: 2.4874, val loss: 2.4482\n",
      "594: lr: 0.0100, train loss: 2.4226, val loss: 2.4407\n",
      "595: lr: 0.0100, train loss: 2.4600, val loss: 2.5350\n",
      "596: lr: 0.0100, train loss: 2.4894, val loss: 2.5123\n",
      "597: lr: 0.0100, train loss: 2.4605, val loss: 2.4690\n",
      "598: lr: 0.0100, train loss: 2.4284, val loss: 2.5047\n",
      "599: lr: 0.0100, train loss: 2.4310, val loss: 2.4885\n",
      "600: lr: 0.0100, train loss: 2.5011, val loss: 2.4701\n",
      "601: lr: 0.0100, train loss: 2.4760, val loss: 2.4710\n",
      "602: lr: 0.0100, train loss: 2.4214, val loss: 2.5231\n",
      "603: lr: 0.0100, train loss: 2.4708, val loss: 2.4592\n",
      "604: lr: 0.0100, train loss: 2.4676, val loss: 2.4751\n",
      "605: lr: 0.0100, train loss: 2.4951, val loss: 2.4562\n",
      "606: lr: 0.0100, train loss: 2.4689, val loss: 2.4774\n",
      "607: lr: 0.0100, train loss: 2.5055, val loss: 2.4894\n",
      "608: lr: 0.0100, train loss: 2.4923, val loss: 2.4545\n",
      "609: lr: 0.0100, train loss: 2.4514, val loss: 2.4743\n",
      "610: lr: 0.0100, train loss: 2.4487, val loss: 2.4846\n",
      "611: lr: 0.0100, train loss: 2.4780, val loss: 2.4371\n",
      "612: lr: 0.0100, train loss: 2.4926, val loss: 2.4613\n",
      "613: lr: 0.0100, train loss: 2.4725, val loss: 2.5256\n",
      "614: lr: 0.0100, train loss: 2.5033, val loss: 2.4834\n",
      "615: lr: 0.0100, train loss: 2.4697, val loss: 2.5258\n",
      "616: lr: 0.0100, train loss: 2.4642, val loss: 2.4723\n",
      "617: lr: 0.0100, train loss: 2.4822, val loss: 2.5211\n",
      "618: lr: 0.0100, train loss: 2.4282, val loss: 2.4710\n",
      "619: lr: 0.0100, train loss: 2.4521, val loss: 2.5164\n",
      "620: lr: 0.0100, train loss: 2.4516, val loss: 2.5101\n",
      "621: lr: 0.0100, train loss: 2.4665, val loss: 2.4969\n",
      "622: lr: 0.0100, train loss: 2.4371, val loss: 2.5060\n",
      "623: lr: 0.0100, train loss: 2.4807, val loss: 2.4909\n",
      "624: lr: 0.0100, train loss: 2.4713, val loss: 2.4659\n",
      "625: lr: 0.0100, train loss: 2.4767, val loss: 2.4999\n",
      "626: lr: 0.0100, train loss: 2.4640, val loss: 2.4730\n",
      "627: lr: 0.0100, train loss: 2.4508, val loss: 2.5286\n",
      "628: lr: 0.0100, train loss: 2.4458, val loss: 2.4405\n",
      "629: lr: 0.0100, train loss: 2.4246, val loss: 2.4769\n",
      "630: lr: 0.0100, train loss: 2.4700, val loss: 2.4949\n",
      "631: lr: 0.0100, train loss: 2.4661, val loss: 2.4822\n",
      "632: lr: 0.0100, train loss: 2.4523, val loss: 2.4584\n",
      "633: lr: 0.0100, train loss: 2.4724, val loss: 2.4800\n",
      "634: lr: 0.0100, train loss: 2.4938, val loss: 2.4336\n",
      "635: lr: 0.0100, train loss: 2.5005, val loss: 2.4836\n",
      "636: lr: 0.0100, train loss: 2.4523, val loss: 2.5371\n",
      "637: lr: 0.0100, train loss: 2.4495, val loss: 2.4946\n",
      "638: lr: 0.0100, train loss: 2.4260, val loss: 2.4522\n",
      "639: lr: 0.0100, train loss: 2.4590, val loss: 2.5037\n",
      "640: lr: 0.0100, train loss: 2.4568, val loss: 2.4964\n",
      "641: lr: 0.0100, train loss: 2.4687, val loss: 2.4987\n",
      "642: lr: 0.0100, train loss: 2.4646, val loss: 2.4724\n",
      "643: lr: 0.0100, train loss: 2.4904, val loss: 2.4438\n",
      "644: lr: 0.0100, train loss: 2.4965, val loss: 2.4665\n",
      "645: lr: 0.0100, train loss: 2.5122, val loss: 2.4447\n",
      "646: lr: 0.0100, train loss: 2.4987, val loss: 2.5090\n",
      "647: lr: 0.0100, train loss: 2.5220, val loss: 2.4778\n",
      "648: lr: 0.0100, train loss: 2.4470, val loss: 2.5304\n",
      "649: lr: 0.0100, train loss: 2.4378, val loss: 2.4677\n",
      "650: lr: 0.0100, train loss: 2.4845, val loss: 2.4800\n",
      "651: lr: 0.0100, train loss: 2.4939, val loss: 2.4314\n",
      "652: lr: 0.0100, train loss: 2.4806, val loss: 2.5033\n",
      "653: lr: 0.0100, train loss: 2.4932, val loss: 2.4707\n",
      "654: lr: 0.0100, train loss: 2.4571, val loss: 2.4945\n",
      "655: lr: 0.0100, train loss: 2.4916, val loss: 2.4855\n",
      "656: lr: 0.0100, train loss: 2.4996, val loss: 2.5046\n",
      "657: lr: 0.0100, train loss: 2.5063, val loss: 2.5121\n",
      "658: lr: 0.0100, train loss: 2.4722, val loss: 2.4546\n",
      "659: lr: 0.0100, train loss: 2.4885, val loss: 2.4536\n",
      "660: lr: 0.0100, train loss: 2.4682, val loss: 2.5120\n",
      "661: lr: 0.0100, train loss: 2.4468, val loss: 2.5048\n",
      "662: lr: 0.0100, train loss: 2.4585, val loss: 2.5204\n",
      "663: lr: 0.0100, train loss: 2.4892, val loss: 2.5266\n",
      "664: lr: 0.0100, train loss: 2.4188, val loss: 2.5297\n",
      "665: lr: 0.0100, train loss: 2.5154, val loss: 2.5445\n",
      "666: lr: 0.0100, train loss: 2.4563, val loss: 2.4274\n",
      "667: lr: 0.0100, train loss: 2.4209, val loss: 2.4871\n",
      "668: lr: 0.0100, train loss: 2.4739, val loss: 2.4895\n",
      "669: lr: 0.0100, train loss: 2.5029, val loss: 2.5332\n",
      "670: lr: 0.0100, train loss: 2.4852, val loss: 2.4257\n",
      "671: lr: 0.0100, train loss: 2.4785, val loss: 2.5418\n",
      "672: lr: 0.0100, train loss: 2.4810, val loss: 2.4756\n",
      "673: lr: 0.0100, train loss: 2.4627, val loss: 2.4755\n",
      "674: lr: 0.0100, train loss: 2.4945, val loss: 2.4563\n",
      "675: lr: 0.0100, train loss: 2.5106, val loss: 2.4845\n",
      "676: lr: 0.0100, train loss: 2.4471, val loss: 2.5026\n",
      "677: lr: 0.0100, train loss: 2.4145, val loss: 2.4607\n",
      "678: lr: 0.0100, train loss: 2.4659, val loss: 2.4951\n",
      "679: lr: 0.0100, train loss: 2.4571, val loss: 2.5049\n",
      "680: lr: 0.0100, train loss: 2.4955, val loss: 2.5177\n",
      "681: lr: 0.0100, train loss: 2.5339, val loss: 2.4833\n",
      "682: lr: 0.0100, train loss: 2.4655, val loss: 2.5030\n",
      "683: lr: 0.0100, train loss: 2.4542, val loss: 2.4786\n",
      "684: lr: 0.0100, train loss: 2.4746, val loss: 2.4459\n",
      "685: lr: 0.0100, train loss: 2.4787, val loss: 2.4544\n",
      "686: lr: 0.0100, train loss: 2.4294, val loss: 2.4818\n",
      "687: lr: 0.0100, train loss: 2.5017, val loss: 2.4938\n",
      "688: lr: 0.0100, train loss: 2.4731, val loss: 2.5226\n",
      "689: lr: 0.0100, train loss: 2.4705, val loss: 2.5341\n",
      "690: lr: 0.0100, train loss: 2.4835, val loss: 2.4444\n",
      "691: lr: 0.0100, train loss: 2.4382, val loss: 2.5255\n",
      "692: lr: 0.0100, train loss: 2.4701, val loss: 2.4830\n",
      "693: lr: 0.0100, train loss: 2.4596, val loss: 2.4681\n",
      "694: lr: 0.0100, train loss: 2.4313, val loss: 2.4745\n",
      "695: lr: 0.0100, train loss: 2.4889, val loss: 2.4954\n",
      "696: lr: 0.0100, train loss: 2.4717, val loss: 2.5025\n",
      "697: lr: 0.0100, train loss: 2.4810, val loss: 2.4327\n",
      "698: lr: 0.0100, train loss: 2.4868, val loss: 2.4366\n",
      "699: lr: 0.0100, train loss: 2.5270, val loss: 2.4738\n",
      "700: lr: 0.0100, train loss: 2.4510, val loss: 2.5249\n",
      "701: lr: 0.0100, train loss: 2.4639, val loss: 2.4619\n",
      "702: lr: 0.0100, train loss: 2.4254, val loss: 2.5441\n",
      "703: lr: 0.0100, train loss: 2.5051, val loss: 2.4609\n",
      "704: lr: 0.0100, train loss: 2.4632, val loss: 2.4862\n",
      "705: lr: 0.0100, train loss: 2.4473, val loss: 2.4892\n",
      "706: lr: 0.0100, train loss: 2.4596, val loss: 2.5271\n",
      "707: lr: 0.0100, train loss: 2.4587, val loss: 2.4937\n",
      "708: lr: 0.0100, train loss: 2.4569, val loss: 2.4616\n",
      "709: lr: 0.0100, train loss: 2.4667, val loss: 2.4831\n",
      "710: lr: 0.0100, train loss: 2.4453, val loss: 2.5076\n",
      "711: lr: 0.0100, train loss: 2.4328, val loss: 2.5212\n",
      "712: lr: 0.0100, train loss: 2.4434, val loss: 2.5141\n",
      "713: lr: 0.0100, train loss: 2.5064, val loss: 2.4759\n",
      "714: lr: 0.0100, train loss: 2.4406, val loss: 2.4956\n",
      "715: lr: 0.0100, train loss: 2.4851, val loss: 2.5447\n",
      "716: lr: 0.0100, train loss: 2.4664, val loss: 2.4406\n",
      "717: lr: 0.0100, train loss: 2.4898, val loss: 2.5030\n",
      "718: lr: 0.0100, train loss: 2.4704, val loss: 2.5063\n",
      "719: lr: 0.0100, train loss: 2.4399, val loss: 2.5383\n",
      "720: lr: 0.0100, train loss: 2.4973, val loss: 2.4960\n",
      "721: lr: 0.0100, train loss: 2.4440, val loss: 2.4848\n",
      "722: lr: 0.0100, train loss: 2.4666, val loss: 2.4982\n",
      "723: lr: 0.0100, train loss: 2.4720, val loss: 2.4814\n",
      "724: lr: 0.0100, train loss: 2.5081, val loss: 2.4727\n",
      "725: lr: 0.0100, train loss: 2.4635, val loss: 2.4660\n",
      "726: lr: 0.0100, train loss: 2.4698, val loss: 2.5134\n",
      "727: lr: 0.0100, train loss: 2.4277, val loss: 2.4757\n",
      "728: lr: 0.0100, train loss: 2.4494, val loss: 2.4905\n",
      "729: lr: 0.0100, train loss: 2.4949, val loss: 2.5045\n",
      "730: lr: 0.0100, train loss: 2.4452, val loss: 2.4666\n",
      "731: lr: 0.0100, train loss: 2.4802, val loss: 2.4824\n",
      "732: lr: 0.0100, train loss: 2.4692, val loss: 2.5049\n",
      "733: lr: 0.0100, train loss: 2.4413, val loss: 2.4784\n",
      "734: lr: 0.0100, train loss: 2.3980, val loss: 2.5228\n",
      "735: lr: 0.0100, train loss: 2.5210, val loss: 2.5057\n",
      "736: lr: 0.0100, train loss: 2.5052, val loss: 2.5042\n",
      "737: lr: 0.0100, train loss: 2.4631, val loss: 2.4629\n",
      "738: lr: 0.0100, train loss: 2.4194, val loss: 2.5079\n",
      "739: lr: 0.0100, train loss: 2.4192, val loss: 2.4721\n",
      "740: lr: 0.0100, train loss: 2.4739, val loss: 2.5133\n",
      "741: lr: 0.0100, train loss: 2.4198, val loss: 2.4560\n",
      "742: lr: 0.0100, train loss: 2.5079, val loss: 2.4721\n",
      "743: lr: 0.0100, train loss: 2.4366, val loss: 2.4624\n",
      "744: lr: 0.0100, train loss: 2.4433, val loss: 2.4656\n",
      "745: lr: 0.0100, train loss: 2.5169, val loss: 2.5249\n",
      "746: lr: 0.0100, train loss: 2.4625, val loss: 2.4268\n",
      "747: lr: 0.0100, train loss: 2.4707, val loss: 2.4814\n",
      "748: lr: 0.0100, train loss: 2.4104, val loss: 2.4181\n",
      "749: lr: 0.0100, train loss: 2.4737, val loss: 2.4849\n",
      "750: lr: 0.0100, train loss: 2.4812, val loss: 2.5268\n",
      "751: lr: 0.0100, train loss: 2.4941, val loss: 2.5091\n",
      "752: lr: 0.0100, train loss: 2.4776, val loss: 2.4888\n",
      "753: lr: 0.0100, train loss: 2.4224, val loss: 2.5447\n",
      "754: lr: 0.0100, train loss: 2.4850, val loss: 2.4465\n",
      "755: lr: 0.0100, train loss: 2.4807, val loss: 2.5137\n",
      "756: lr: 0.0100, train loss: 2.4937, val loss: 2.4123\n",
      "757: lr: 0.0100, train loss: 2.4407, val loss: 2.4830\n",
      "758: lr: 0.0100, train loss: 2.4821, val loss: 2.4529\n",
      "759: lr: 0.0100, train loss: 2.4400, val loss: 2.4754\n",
      "760: lr: 0.0100, train loss: 2.4536, val loss: 2.5145\n",
      "761: lr: 0.0100, train loss: 2.4498, val loss: 2.4798\n",
      "762: lr: 0.0100, train loss: 2.4473, val loss: 2.4911\n",
      "763: lr: 0.0100, train loss: 2.4806, val loss: 2.4825\n",
      "764: lr: 0.0100, train loss: 2.4651, val loss: 2.4655\n",
      "765: lr: 0.0100, train loss: 2.4525, val loss: 2.5055\n",
      "766: lr: 0.0100, train loss: 2.4547, val loss: 2.4651\n",
      "767: lr: 0.0100, train loss: 2.4790, val loss: 2.4956\n",
      "768: lr: 0.0100, train loss: 2.5076, val loss: 2.5153\n",
      "769: lr: 0.0100, train loss: 2.4875, val loss: 2.5070\n",
      "770: lr: 0.0100, train loss: 2.4932, val loss: 2.5158\n",
      "771: lr: 0.0100, train loss: 2.4908, val loss: 2.5224\n",
      "772: lr: 0.0100, train loss: 2.4042, val loss: 2.4538\n",
      "773: lr: 0.0100, train loss: 2.4278, val loss: 2.5204\n",
      "774: lr: 0.0100, train loss: 2.4834, val loss: 2.5249\n",
      "775: lr: 0.0100, train loss: 2.4488, val loss: 2.4486\n",
      "776: lr: 0.0100, train loss: 2.4778, val loss: 2.5128\n",
      "777: lr: 0.0100, train loss: 2.4702, val loss: 2.4763\n",
      "778: lr: 0.0100, train loss: 2.4622, val loss: 2.4835\n",
      "779: lr: 0.0100, train loss: 2.5357, val loss: 2.5228\n",
      "780: lr: 0.0100, train loss: 2.4136, val loss: 2.4984\n",
      "781: lr: 0.0100, train loss: 2.4184, val loss: 2.4997\n",
      "782: lr: 0.0100, train loss: 2.4786, val loss: 2.4486\n",
      "783: lr: 0.0100, train loss: 2.4419, val loss: 2.4601\n",
      "784: lr: 0.0100, train loss: 2.4711, val loss: 2.4956\n",
      "785: lr: 0.0100, train loss: 2.4929, val loss: 2.4952\n",
      "786: lr: 0.0100, train loss: 2.4552, val loss: 2.4958\n",
      "787: lr: 0.0100, train loss: 2.4706, val loss: 2.4881\n",
      "788: lr: 0.0100, train loss: 2.4910, val loss: 2.4478\n",
      "789: lr: 0.0100, train loss: 2.4661, val loss: 2.4381\n",
      "790: lr: 0.0100, train loss: 2.4928, val loss: 2.5121\n",
      "791: lr: 0.0100, train loss: 2.4326, val loss: 2.4649\n",
      "792: lr: 0.0100, train loss: 2.4378, val loss: 2.5117\n",
      "793: lr: 0.0100, train loss: 2.4920, val loss: 2.4664\n",
      "794: lr: 0.0100, train loss: 2.4634, val loss: 2.4443\n",
      "795: lr: 0.0100, train loss: 2.4611, val loss: 2.4814\n",
      "796: lr: 0.0100, train loss: 2.4497, val loss: 2.4829\n",
      "797: lr: 0.0100, train loss: 2.4467, val loss: 2.4880\n",
      "798: lr: 0.0100, train loss: 2.4151, val loss: 2.4694\n",
      "799: lr: 0.0100, train loss: 2.4969, val loss: 2.4589\n",
      "800: lr: 0.0100, train loss: 2.4659, val loss: 2.4331\n",
      "801: lr: 0.0100, train loss: 2.4406, val loss: 2.4645\n",
      "802: lr: 0.0100, train loss: 2.4903, val loss: 2.4700\n",
      "803: lr: 0.0100, train loss: 2.4285, val loss: 2.4373\n",
      "804: lr: 0.0100, train loss: 2.4701, val loss: 2.4595\n",
      "805: lr: 0.0100, train loss: 2.4514, val loss: 2.4452\n",
      "806: lr: 0.0100, train loss: 2.4721, val loss: 2.4678\n",
      "807: lr: 0.0100, train loss: 2.4822, val loss: 2.4675\n",
      "808: lr: 0.0100, train loss: 2.4544, val loss: 2.5289\n",
      "809: lr: 0.0100, train loss: 2.4225, val loss: 2.4667\n",
      "810: lr: 0.0100, train loss: 2.4138, val loss: 2.4650\n",
      "811: lr: 0.0100, train loss: 2.4619, val loss: 2.4604\n",
      "812: lr: 0.0100, train loss: 2.4573, val loss: 2.5600\n",
      "813: lr: 0.0100, train loss: 2.4535, val loss: 2.4659\n",
      "814: lr: 0.0100, train loss: 2.4905, val loss: 2.4318\n",
      "815: lr: 0.0100, train loss: 2.4461, val loss: 2.4933\n",
      "816: lr: 0.0100, train loss: 2.4289, val loss: 2.4470\n",
      "817: lr: 0.0100, train loss: 2.4217, val loss: 2.5002\n",
      "818: lr: 0.0100, train loss: 2.4533, val loss: 2.4909\n",
      "819: lr: 0.0100, train loss: 2.5312, val loss: 2.5342\n",
      "820: lr: 0.0100, train loss: 2.4238, val loss: 2.4292\n",
      "821: lr: 0.0100, train loss: 2.4279, val loss: 2.5413\n",
      "822: lr: 0.0100, train loss: 2.4370, val loss: 2.5156\n",
      "823: lr: 0.0100, train loss: 2.4773, val loss: 2.4730\n",
      "824: lr: 0.0100, train loss: 2.4704, val loss: 2.5111\n",
      "825: lr: 0.0100, train loss: 2.5009, val loss: 2.5185\n",
      "826: lr: 0.0100, train loss: 2.4614, val loss: 2.4841\n",
      "827: lr: 0.0100, train loss: 2.4820, val loss: 2.5052\n",
      "828: lr: 0.0100, train loss: 2.4661, val loss: 2.4915\n",
      "829: lr: 0.0100, train loss: 2.4788, val loss: 2.4848\n",
      "830: lr: 0.0100, train loss: 2.4637, val loss: 2.5025\n",
      "831: lr: 0.0100, train loss: 2.4524, val loss: 2.4656\n",
      "832: lr: 0.0100, train loss: 2.4883, val loss: 2.4719\n",
      "833: lr: 0.0100, train loss: 2.4637, val loss: 2.4514\n",
      "834: lr: 0.0100, train loss: 2.4249, val loss: 2.4818\n",
      "835: lr: 0.0100, train loss: 2.5039, val loss: 2.4701\n",
      "836: lr: 0.0100, train loss: 2.4599, val loss: 2.5151\n",
      "837: lr: 0.0100, train loss: 2.5200, val loss: 2.4962\n",
      "838: lr: 0.0100, train loss: 2.4821, val loss: 2.5000\n",
      "839: lr: 0.0100, train loss: 2.4831, val loss: 2.4671\n",
      "840: lr: 0.0100, train loss: 2.4778, val loss: 2.4854\n",
      "841: lr: 0.0100, train loss: 2.4289, val loss: 2.5110\n",
      "842: lr: 0.0100, train loss: 2.4340, val loss: 2.5033\n",
      "843: lr: 0.0100, train loss: 2.4776, val loss: 2.4890\n",
      "844: lr: 0.0100, train loss: 2.4735, val loss: 2.5106\n",
      "845: lr: 0.0100, train loss: 2.4559, val loss: 2.4537\n",
      "846: lr: 0.0100, train loss: 2.4349, val loss: 2.4948\n",
      "847: lr: 0.0100, train loss: 2.5143, val loss: 2.5019\n",
      "848: lr: 0.0100, train loss: 2.4424, val loss: 2.5007\n",
      "849: lr: 0.0100, train loss: 2.4690, val loss: 2.4830\n",
      "850: lr: 0.0100, train loss: 2.4437, val loss: 2.4695\n",
      "851: lr: 0.0100, train loss: 2.4462, val loss: 2.5327\n",
      "852: lr: 0.0100, train loss: 2.4270, val loss: 2.4811\n",
      "853: lr: 0.0100, train loss: 2.4977, val loss: 2.5155\n",
      "854: lr: 0.0100, train loss: 2.4749, val loss: 2.4902\n",
      "855: lr: 0.0100, train loss: 2.4755, val loss: 2.4795\n",
      "856: lr: 0.0100, train loss: 2.4332, val loss: 2.4678\n",
      "857: lr: 0.0100, train loss: 2.5126, val loss: 2.4905\n",
      "858: lr: 0.0100, train loss: 2.4691, val loss: 2.4958\n",
      "859: lr: 0.0100, train loss: 2.4787, val loss: 2.4879\n",
      "860: lr: 0.0100, train loss: 2.4790, val loss: 2.4841\n",
      "861: lr: 0.0100, train loss: 2.4589, val loss: 2.4953\n",
      "862: lr: 0.0100, train loss: 2.4689, val loss: 2.4702\n",
      "863: lr: 0.0100, train loss: 2.4309, val loss: 2.4941\n",
      "864: lr: 0.0100, train loss: 2.4720, val loss: 2.4572\n",
      "865: lr: 0.0100, train loss: 2.4093, val loss: 2.5196\n",
      "866: lr: 0.0100, train loss: 2.4449, val loss: 2.4895\n",
      "867: lr: 0.0100, train loss: 2.4901, val loss: 2.4603\n",
      "868: lr: 0.0100, train loss: 2.4960, val loss: 2.5217\n",
      "869: lr: 0.0100, train loss: 2.4503, val loss: 2.4711\n",
      "870: lr: 0.0100, train loss: 2.4741, val loss: 2.4978\n",
      "871: lr: 0.0100, train loss: 2.4495, val loss: 2.4576\n",
      "872: lr: 0.0100, train loss: 2.4321, val loss: 2.5056\n",
      "873: lr: 0.0100, train loss: 2.4504, val loss: 2.5305\n",
      "874: lr: 0.0100, train loss: 2.4437, val loss: 2.4918\n",
      "875: lr: 0.0100, train loss: 2.5067, val loss: 2.5047\n",
      "876: lr: 0.0100, train loss: 2.4181, val loss: 2.4316\n",
      "877: lr: 0.0100, train loss: 2.4572, val loss: 2.5071\n",
      "878: lr: 0.0100, train loss: 2.4505, val loss: 2.4856\n",
      "879: lr: 0.0100, train loss: 2.4667, val loss: 2.4822\n",
      "880: lr: 0.0100, train loss: 2.4419, val loss: 2.4810\n",
      "881: lr: 0.0100, train loss: 2.4868, val loss: 2.4661\n",
      "882: lr: 0.0100, train loss: 2.4913, val loss: 2.4894\n",
      "883: lr: 0.0100, train loss: 2.4587, val loss: 2.5009\n",
      "884: lr: 0.0100, train loss: 2.4460, val loss: 2.4927\n",
      "885: lr: 0.0100, train loss: 2.4433, val loss: 2.5306\n",
      "886: lr: 0.0100, train loss: 2.5158, val loss: 2.5129\n",
      "887: lr: 0.0100, train loss: 2.4985, val loss: 2.5313\n",
      "888: lr: 0.0100, train loss: 2.4877, val loss: 2.5033\n",
      "889: lr: 0.0100, train loss: 2.4784, val loss: 2.4922\n",
      "890: lr: 0.0100, train loss: 2.4690, val loss: 2.5179\n",
      "891: lr: 0.0100, train loss: 2.4848, val loss: 2.4940\n",
      "892: lr: 0.0100, train loss: 2.4800, val loss: 2.5118\n",
      "893: lr: 0.0100, train loss: 2.4575, val loss: 2.5032\n",
      "894: lr: 0.0100, train loss: 2.4924, val loss: 2.4923\n",
      "895: lr: 0.0100, train loss: 2.4741, val loss: 2.5603\n",
      "896: lr: 0.0100, train loss: 2.4986, val loss: 2.4563\n",
      "897: lr: 0.0100, train loss: 2.5065, val loss: 2.4748\n",
      "898: lr: 0.0100, train loss: 2.4043, val loss: 2.4217\n",
      "899: lr: 0.0100, train loss: 2.4545, val loss: 2.5379\n",
      "900: lr: 0.0100, train loss: 2.4622, val loss: 2.4326\n",
      "901: lr: 0.0100, train loss: 2.4652, val loss: 2.4787\n",
      "902: lr: 0.0100, train loss: 2.5044, val loss: 2.5645\n",
      "903: lr: 0.0100, train loss: 2.4705, val loss: 2.4765\n",
      "904: lr: 0.0100, train loss: 2.4277, val loss: 2.4785\n",
      "905: lr: 0.0100, train loss: 2.4687, val loss: 2.4607\n",
      "906: lr: 0.0100, train loss: 2.4253, val loss: 2.5644\n",
      "907: lr: 0.0100, train loss: 2.4661, val loss: 2.5187\n",
      "908: lr: 0.0100, train loss: 2.4609, val loss: 2.4282\n",
      "909: lr: 0.0100, train loss: 2.5125, val loss: 2.4936\n",
      "910: lr: 0.0100, train loss: 2.4142, val loss: 2.4623\n",
      "911: lr: 0.0100, train loss: 2.4387, val loss: 2.5135\n",
      "912: lr: 0.0100, train loss: 2.4308, val loss: 2.5691\n",
      "913: lr: 0.0100, train loss: 2.4319, val loss: 2.5279\n",
      "914: lr: 0.0100, train loss: 2.4548, val loss: 2.5214\n",
      "915: lr: 0.0100, train loss: 2.4652, val loss: 2.4684\n",
      "916: lr: 0.0100, train loss: 2.4653, val loss: 2.4560\n",
      "917: lr: 0.0100, train loss: 2.4610, val loss: 2.5011\n",
      "918: lr: 0.0100, train loss: 2.4380, val loss: 2.4423\n",
      "919: lr: 0.0100, train loss: 2.4661, val loss: 2.4839\n",
      "920: lr: 0.0100, train loss: 2.4960, val loss: 2.5094\n",
      "921: lr: 0.0100, train loss: 2.4919, val loss: 2.4903\n",
      "922: lr: 0.0100, train loss: 2.4363, val loss: 2.4868\n",
      "923: lr: 0.0100, train loss: 2.4444, val loss: 2.4503\n",
      "924: lr: 0.0100, train loss: 2.4641, val loss: 2.5211\n",
      "925: lr: 0.0100, train loss: 2.4706, val loss: 2.5251\n",
      "926: lr: 0.0100, train loss: 2.4066, val loss: 2.4502\n",
      "927: lr: 0.0100, train loss: 2.4891, val loss: 2.5364\n",
      "928: lr: 0.0100, train loss: 2.4550, val loss: 2.5071\n",
      "929: lr: 0.0100, train loss: 2.4689, val loss: 2.4608\n",
      "930: lr: 0.0100, train loss: 2.4305, val loss: 2.4868\n",
      "931: lr: 0.0100, train loss: 2.4477, val loss: 2.4811\n",
      "932: lr: 0.0100, train loss: 2.4712, val loss: 2.4352\n",
      "933: lr: 0.0100, train loss: 2.4642, val loss: 2.4984\n",
      "934: lr: 0.0100, train loss: 2.4822, val loss: 2.4943\n",
      "935: lr: 0.0100, train loss: 2.4666, val loss: 2.4997\n",
      "936: lr: 0.0100, train loss: 2.4837, val loss: 2.4350\n",
      "937: lr: 0.0100, train loss: 2.4502, val loss: 2.5330\n",
      "938: lr: 0.0100, train loss: 2.4720, val loss: 2.5210\n",
      "939: lr: 0.0100, train loss: 2.4545, val loss: 2.5125\n",
      "940: lr: 0.0100, train loss: 2.5072, val loss: 2.4657\n",
      "941: lr: 0.0100, train loss: 2.4756, val loss: 2.5081\n",
      "942: lr: 0.0100, train loss: 2.4827, val loss: 2.5753\n",
      "943: lr: 0.0100, train loss: 2.4466, val loss: 2.4612\n",
      "944: lr: 0.0100, train loss: 2.4623, val loss: 2.5428\n",
      "945: lr: 0.0100, train loss: 2.4673, val loss: 2.4831\n",
      "946: lr: 0.0100, train loss: 2.4350, val loss: 2.4874\n",
      "947: lr: 0.0100, train loss: 2.4672, val loss: 2.4770\n",
      "948: lr: 0.0100, train loss: 2.4577, val loss: 2.4862\n",
      "949: lr: 0.0100, train loss: 2.4223, val loss: 2.5280\n",
      "950: lr: 0.0100, train loss: 2.4856, val loss: 2.4728\n",
      "951: lr: 0.0100, train loss: 2.4465, val loss: 2.4840\n",
      "952: lr: 0.0100, train loss: 2.4417, val loss: 2.5495\n",
      "953: lr: 0.0100, train loss: 2.4534, val loss: 2.4825\n",
      "954: lr: 0.0100, train loss: 2.4773, val loss: 2.4503\n",
      "955: lr: 0.0100, train loss: 2.4618, val loss: 2.4622\n",
      "956: lr: 0.0100, train loss: 2.4409, val loss: 2.4619\n",
      "957: lr: 0.0100, train loss: 2.4276, val loss: 2.4903\n",
      "958: lr: 0.0100, train loss: 2.4479, val loss: 2.5240\n",
      "959: lr: 0.0100, train loss: 2.4557, val loss: 2.4941\n",
      "960: lr: 0.0100, train loss: 2.4490, val loss: 2.5254\n",
      "961: lr: 0.0100, train loss: 2.4642, val loss: 2.4220\n",
      "962: lr: 0.0100, train loss: 2.5354, val loss: 2.5081\n",
      "963: lr: 0.0100, train loss: 2.4627, val loss: 2.4897\n",
      "964: lr: 0.0100, train loss: 2.4423, val loss: 2.5183\n",
      "965: lr: 0.0100, train loss: 2.4613, val loss: 2.4844\n",
      "966: lr: 0.0100, train loss: 2.4611, val loss: 2.5174\n",
      "967: lr: 0.0100, train loss: 2.4798, val loss: 2.4785\n",
      "968: lr: 0.0100, train loss: 2.4503, val loss: 2.4841\n",
      "969: lr: 0.0100, train loss: 2.4281, val loss: 2.5573\n",
      "970: lr: 0.0100, train loss: 2.4995, val loss: 2.4903\n",
      "971: lr: 0.0100, train loss: 2.4593, val loss: 2.4700\n",
      "972: lr: 0.0100, train loss: 2.4403, val loss: 2.4600\n",
      "973: lr: 0.0100, train loss: 2.5045, val loss: 2.5330\n",
      "974: lr: 0.0100, train loss: 2.4462, val loss: 2.4493\n",
      "975: lr: 0.0100, train loss: 2.4449, val loss: 2.4713\n",
      "976: lr: 0.0100, train loss: 2.4715, val loss: 2.5138\n",
      "977: lr: 0.0100, train loss: 2.4708, val loss: 2.4865\n",
      "978: lr: 0.0100, train loss: 2.4616, val loss: 2.4961\n",
      "979: lr: 0.0100, train loss: 2.4190, val loss: 2.4740\n",
      "980: lr: 0.0100, train loss: 2.4815, val loss: 2.4938\n",
      "981: lr: 0.0100, train loss: 2.5070, val loss: 2.4825\n",
      "982: lr: 0.0100, train loss: 2.4781, val loss: 2.4977\n",
      "983: lr: 0.0100, train loss: 2.4130, val loss: 2.4721\n",
      "984: lr: 0.0100, train loss: 2.4779, val loss: 2.5211\n",
      "985: lr: 0.0100, train loss: 2.4672, val loss: 2.4724\n",
      "986: lr: 0.0100, train loss: 2.4569, val loss: 2.4634\n",
      "987: lr: 0.0100, train loss: 2.4776, val loss: 2.4830\n",
      "988: lr: 0.0100, train loss: 2.5067, val loss: 2.5197\n",
      "989: lr: 0.0100, train loss: 2.4731, val loss: 2.5167\n",
      "990: lr: 0.0100, train loss: 2.4461, val loss: 2.4686\n",
      "991: lr: 0.0100, train loss: 2.4466, val loss: 2.4970\n",
      "992: lr: 0.0100, train loss: 2.4334, val loss: 2.4392\n",
      "993: lr: 0.0100, train loss: 2.5013, val loss: 2.5050\n",
      "994: lr: 0.0100, train loss: 2.4694, val loss: 2.4748\n",
      "995: lr: 0.0100, train loss: 2.4618, val loss: 2.4931\n",
      "996: lr: 0.0100, train loss: 2.4491, val loss: 2.4782\n",
      "997: lr: 0.0100, train loss: 2.4362, val loss: 2.4398\n",
      "998: lr: 0.0100, train loss: 2.4557, val loss: 2.4715\n",
      "999: lr: 0.0100, train loss: 2.4426, val loss: 2.4946\n",
      "1000: lr: 0.0100, train loss: 2.4454, val loss: 2.4551\n",
      "1001: lr: 0.0100, train loss: 2.4802, val loss: 2.4752\n",
      "1002: lr: 0.0100, train loss: 2.4757, val loss: 2.4540\n",
      "1003: lr: 0.0100, train loss: 2.4406, val loss: 2.5375\n",
      "1004: lr: 0.0100, train loss: 2.4571, val loss: 2.5343\n",
      "1005: lr: 0.0100, train loss: 2.4599, val loss: 2.5198\n",
      "1006: lr: 0.0100, train loss: 2.4890, val loss: 2.5304\n",
      "1007: lr: 0.0100, train loss: 2.4373, val loss: 2.5052\n",
      "1008: lr: 0.0100, train loss: 2.4635, val loss: 2.4885\n",
      "1009: lr: 0.0100, train loss: 2.4646, val loss: 2.4635\n",
      "1010: lr: 0.0100, train loss: 2.4061, val loss: 2.5061\n",
      "1011: lr: 0.0100, train loss: 2.4894, val loss: 2.4906\n",
      "1012: lr: 0.0100, train loss: 2.4662, val loss: 2.4984\n",
      "1013: lr: 0.0100, train loss: 2.4729, val loss: 2.4607\n",
      "1014: lr: 0.0100, train loss: 2.4397, val loss: 2.4815\n",
      "1015: lr: 0.0100, train loss: 2.4624, val loss: 2.4981\n",
      "1016: lr: 0.0100, train loss: 2.4850, val loss: 2.4604\n",
      "1017: lr: 0.0100, train loss: 2.4578, val loss: 2.4979\n",
      "1018: lr: 0.0100, train loss: 2.4628, val loss: 2.4791\n",
      "1019: lr: 0.0100, train loss: 2.4756, val loss: 2.4838\n",
      "1020: lr: 0.0100, train loss: 2.4510, val loss: 2.5071\n",
      "1021: lr: 0.0100, train loss: 2.4681, val loss: 2.4827\n",
      "1022: lr: 0.0100, train loss: 2.4615, val loss: 2.5060\n",
      "1023: lr: 0.0100, train loss: 2.4523, val loss: 2.4884\n",
      "1024: lr: 0.0100, train loss: 2.3993, val loss: 2.4920\n",
      "1025: lr: 0.0100, train loss: 2.4391, val loss: 2.4783\n",
      "1026: lr: 0.0100, train loss: 2.4392, val loss: 2.4621\n",
      "1027: lr: 0.0100, train loss: 2.4010, val loss: 2.4195\n",
      "1028: lr: 0.0100, train loss: 2.4887, val loss: 2.4628\n",
      "1029: lr: 0.0100, train loss: 2.4595, val loss: 2.4580\n",
      "1030: lr: 0.0100, train loss: 2.4325, val loss: 2.5305\n",
      "1031: lr: 0.0100, train loss: 2.4994, val loss: 2.5210\n",
      "1032: lr: 0.0100, train loss: 2.4541, val loss: 2.5279\n",
      "1033: lr: 0.0100, train loss: 2.4634, val loss: 2.5528\n",
      "1034: lr: 0.0100, train loss: 2.4095, val loss: 2.4943\n",
      "1035: lr: 0.0100, train loss: 2.5038, val loss: 2.4917\n",
      "1036: lr: 0.0100, train loss: 2.4373, val loss: 2.4356\n",
      "1037: lr: 0.0100, train loss: 2.4476, val loss: 2.4599\n",
      "1038: lr: 0.0100, train loss: 2.4608, val loss: 2.4968\n",
      "1039: lr: 0.0100, train loss: 2.4752, val loss: 2.4703\n",
      "1040: lr: 0.0100, train loss: 2.4453, val loss: 2.4335\n",
      "1041: lr: 0.0100, train loss: 2.4571, val loss: 2.4950\n",
      "1042: lr: 0.0100, train loss: 2.4450, val loss: 2.5058\n",
      "1043: lr: 0.0100, train loss: 2.4601, val loss: 2.4879\n",
      "1044: lr: 0.0100, train loss: 2.4783, val loss: 2.4509\n",
      "1045: lr: 0.0100, train loss: 2.4425, val loss: 2.5025\n",
      "1046: lr: 0.0100, train loss: 2.4268, val loss: 2.4978\n",
      "1047: lr: 0.0100, train loss: 2.4187, val loss: 2.5204\n",
      "1048: lr: 0.0100, train loss: 2.5139, val loss: 2.5142\n",
      "1049: lr: 0.0100, train loss: 2.4260, val loss: 2.5200\n",
      "1050: lr: 0.0100, train loss: 2.4822, val loss: 2.4935\n",
      "1051: lr: 0.0100, train loss: 2.4527, val loss: 2.5078\n",
      "1052: lr: 0.0100, train loss: 2.4561, val loss: 2.4851\n",
      "1053: lr: 0.0100, train loss: 2.4684, val loss: 2.4552\n",
      "1054: lr: 0.0100, train loss: 2.4793, val loss: 2.4932\n",
      "1055: lr: 0.0100, train loss: 2.5111, val loss: 2.4855\n",
      "1056: lr: 0.0100, train loss: 2.4475, val loss: 2.4706\n",
      "1057: lr: 0.0100, train loss: 2.4750, val loss: 2.5019\n",
      "1058: lr: 0.0100, train loss: 2.4840, val loss: 2.4670\n",
      "1059: lr: 0.0100, train loss: 2.4728, val loss: 2.4877\n",
      "1060: lr: 0.0100, train loss: 2.4771, val loss: 2.4776\n",
      "1061: lr: 0.0100, train loss: 2.4486, val loss: 2.5245\n",
      "1062: lr: 0.0100, train loss: 2.4270, val loss: 2.4548\n",
      "1063: lr: 0.0100, train loss: 2.4785, val loss: 2.4948\n",
      "1064: lr: 0.0100, train loss: 2.4788, val loss: 2.4832\n",
      "1065: lr: 0.0100, train loss: 2.4963, val loss: 2.4249\n",
      "1066: lr: 0.0100, train loss: 2.4840, val loss: 2.4628\n",
      "1067: lr: 0.0100, train loss: 2.5036, val loss: 2.4493\n",
      "1068: lr: 0.0100, train loss: 2.4951, val loss: 2.4491\n",
      "1069: lr: 0.0100, train loss: 2.4321, val loss: 2.5122\n",
      "1070: lr: 0.0100, train loss: 2.4284, val loss: 2.4880\n",
      "1071: lr: 0.0100, train loss: 2.4702, val loss: 2.5261\n",
      "1072: lr: 0.0100, train loss: 2.4845, val loss: 2.4628\n",
      "1073: lr: 0.0100, train loss: 2.4540, val loss: 2.4319\n",
      "1074: lr: 0.0100, train loss: 2.4710, val loss: 2.4760\n",
      "1075: lr: 0.0100, train loss: 2.4703, val loss: 2.4852\n",
      "1076: lr: 0.0100, train loss: 2.4373, val loss: 2.4988\n",
      "1077: lr: 0.0100, train loss: 2.4174, val loss: 2.4397\n",
      "1078: lr: 0.0100, train loss: 2.4728, val loss: 2.4729\n",
      "1079: lr: 0.0100, train loss: 2.4514, val loss: 2.5698\n",
      "1080: lr: 0.0100, train loss: 2.4678, val loss: 2.5079\n",
      "1081: lr: 0.0100, train loss: 2.4178, val loss: 2.4924\n",
      "1082: lr: 0.0100, train loss: 2.4969, val loss: 2.5208\n",
      "1083: lr: 0.0100, train loss: 2.4713, val loss: 2.4631\n",
      "1084: lr: 0.0100, train loss: 2.4503, val loss: 2.4889\n",
      "1085: lr: 0.0100, train loss: 2.4247, val loss: 2.4902\n",
      "1086: lr: 0.0100, train loss: 2.4378, val loss: 2.5090\n",
      "1087: lr: 0.0100, train loss: 2.4491, val loss: 2.5692\n",
      "1088: lr: 0.0100, train loss: 2.4612, val loss: 2.5019\n",
      "1089: lr: 0.0100, train loss: 2.4678, val loss: 2.4906\n",
      "1090: lr: 0.0100, train loss: 2.4991, val loss: 2.5214\n",
      "1091: lr: 0.0100, train loss: 2.4787, val loss: 2.4559\n",
      "1092: lr: 0.0100, train loss: 2.4267, val loss: 2.4865\n",
      "1093: lr: 0.0100, train loss: 2.4572, val loss: 2.4849\n",
      "1094: lr: 0.0100, train loss: 2.4828, val loss: 2.5108\n",
      "1095: lr: 0.0100, train loss: 2.4542, val loss: 2.4995\n",
      "1096: lr: 0.0100, train loss: 2.4248, val loss: 2.4964\n",
      "1097: lr: 0.0100, train loss: 2.4526, val loss: 2.4875\n",
      "1098: lr: 0.0100, train loss: 2.4644, val loss: 2.4947\n",
      "1099: lr: 0.0100, train loss: 2.4898, val loss: 2.5175\n",
      "1100: lr: 0.0100, train loss: 2.4758, val loss: 2.5525\n",
      "1101: lr: 0.0100, train loss: 2.3959, val loss: 2.4471\n",
      "1102: lr: 0.0100, train loss: 2.4555, val loss: 2.5281\n",
      "1103: lr: 0.0100, train loss: 2.4921, val loss: 2.5396\n",
      "1104: lr: 0.0100, train loss: 2.4497, val loss: 2.4563\n",
      "1105: lr: 0.0100, train loss: 2.4824, val loss: 2.4325\n",
      "1106: lr: 0.0100, train loss: 2.4766, val loss: 2.4382\n",
      "1107: lr: 0.0100, train loss: 2.4669, val loss: 2.5044\n",
      "1108: lr: 0.0100, train loss: 2.4922, val loss: 2.4813\n",
      "1109: lr: 0.0100, train loss: 2.4944, val loss: 2.4824\n",
      "1110: lr: 0.0100, train loss: 2.4603, val loss: 2.4915\n",
      "1111: lr: 0.0100, train loss: 2.4606, val loss: 2.4931\n",
      "1112: lr: 0.0100, train loss: 2.4902, val loss: 2.5259\n",
      "1113: lr: 0.0100, train loss: 2.5305, val loss: 2.4936\n",
      "1114: lr: 0.0100, train loss: 2.4914, val loss: 2.5255\n",
      "1115: lr: 0.0100, train loss: 2.4600, val loss: 2.4527\n",
      "1116: lr: 0.0100, train loss: 2.4227, val loss: 2.5262\n",
      "1117: lr: 0.0100, train loss: 2.4206, val loss: 2.4673\n",
      "1118: lr: 0.0100, train loss: 2.4229, val loss: 2.4890\n",
      "1119: lr: 0.0100, train loss: 2.4291, val loss: 2.4870\n",
      "1120: lr: 0.0100, train loss: 2.4063, val loss: 2.4446\n",
      "1121: lr: 0.0100, train loss: 2.4691, val loss: 2.5282\n",
      "1122: lr: 0.0100, train loss: 2.4792, val loss: 2.5466\n",
      "1123: lr: 0.0100, train loss: 2.4676, val loss: 2.4452\n",
      "1124: lr: 0.0100, train loss: 2.4831, val loss: 2.4492\n",
      "1125: lr: 0.0100, train loss: 2.5127, val loss: 2.4707\n",
      "1126: lr: 0.0100, train loss: 2.4364, val loss: 2.5104\n",
      "1127: lr: 0.0100, train loss: 2.4289, val loss: 2.4976\n",
      "1128: lr: 0.0100, train loss: 2.4853, val loss: 2.4927\n",
      "1129: lr: 0.0100, train loss: 2.5358, val loss: 2.4526\n",
      "1130: lr: 0.0100, train loss: 2.4392, val loss: 2.4539\n",
      "1131: lr: 0.0100, train loss: 2.5052, val loss: 2.4716\n",
      "1132: lr: 0.0100, train loss: 2.4975, val loss: 2.5024\n",
      "1133: lr: 0.0100, train loss: 2.4895, val loss: 2.4462\n",
      "1134: lr: 0.0100, train loss: 2.4499, val loss: 2.5153\n",
      "1135: lr: 0.0100, train loss: 2.4802, val loss: 2.4612\n",
      "1136: lr: 0.0100, train loss: 2.4886, val loss: 2.5244\n",
      "1137: lr: 0.0100, train loss: 2.4956, val loss: 2.4855\n",
      "1138: lr: 0.0100, train loss: 2.4456, val loss: 2.5625\n",
      "1139: lr: 0.0100, train loss: 2.4944, val loss: 2.5216\n",
      "1140: lr: 0.0100, train loss: 2.4597, val loss: 2.5172\n",
      "1141: lr: 0.0100, train loss: 2.4544, val loss: 2.4712\n",
      "1142: lr: 0.0100, train loss: 2.4699, val loss: 2.5282\n",
      "1143: lr: 0.0100, train loss: 2.4609, val loss: 2.4543\n",
      "1144: lr: 0.0100, train loss: 2.4593, val loss: 2.5271\n",
      "1145: lr: 0.0100, train loss: 2.4534, val loss: 2.4986\n",
      "1146: lr: 0.0100, train loss: 2.4386, val loss: 2.4841\n",
      "1147: lr: 0.0100, train loss: 2.4881, val loss: 2.4824\n",
      "1148: lr: 0.0100, train loss: 2.5000, val loss: 2.5036\n",
      "1149: lr: 0.0100, train loss: 2.4611, val loss: 2.4755\n",
      "1150: lr: 0.0100, train loss: 2.4589, val loss: 2.4680\n",
      "1151: lr: 0.0100, train loss: 2.4536, val loss: 2.5274\n",
      "1152: lr: 0.0100, train loss: 2.5032, val loss: 2.4542\n",
      "1153: lr: 0.0100, train loss: 2.4699, val loss: 2.4917\n",
      "1154: lr: 0.0100, train loss: 2.4812, val loss: 2.4521\n",
      "1155: lr: 0.0100, train loss: 2.4679, val loss: 2.4693\n",
      "1156: lr: 0.0100, train loss: 2.4531, val loss: 2.4734\n",
      "1157: lr: 0.0100, train loss: 2.4296, val loss: 2.4487\n",
      "1158: lr: 0.0100, train loss: 2.4578, val loss: 2.5161\n",
      "1159: lr: 0.0100, train loss: 2.4732, val loss: 2.4332\n",
      "1160: lr: 0.0100, train loss: 2.4582, val loss: 2.5448\n",
      "1161: lr: 0.0100, train loss: 2.4632, val loss: 2.5126\n",
      "1162: lr: 0.0100, train loss: 2.4602, val loss: 2.4751\n",
      "1163: lr: 0.0100, train loss: 2.4643, val loss: 2.4951\n",
      "1164: lr: 0.0100, train loss: 2.4822, val loss: 2.4461\n",
      "1165: lr: 0.0100, train loss: 2.4237, val loss: 2.4775\n",
      "1166: lr: 0.0100, train loss: 2.5048, val loss: 2.4566\n",
      "1167: lr: 0.0100, train loss: 2.4823, val loss: 2.4861\n",
      "1168: lr: 0.0100, train loss: 2.4728, val loss: 2.4536\n",
      "1169: lr: 0.0100, train loss: 2.4578, val loss: 2.5368\n",
      "1170: lr: 0.0100, train loss: 2.4417, val loss: 2.4067\n",
      "1171: lr: 0.0100, train loss: 2.4758, val loss: 2.5027\n",
      "1172: lr: 0.0100, train loss: 2.4506, val loss: 2.5197\n",
      "1173: lr: 0.0100, train loss: 2.4689, val loss: 2.4500\n",
      "1174: lr: 0.0100, train loss: 2.4266, val loss: 2.4943\n",
      "1175: lr: 0.0100, train loss: 2.4407, val loss: 2.4386\n",
      "1176: lr: 0.0100, train loss: 2.4601, val loss: 2.5237\n",
      "1177: lr: 0.0100, train loss: 2.4669, val loss: 2.5153\n",
      "1178: lr: 0.0100, train loss: 2.4267, val loss: 2.4903\n",
      "1179: lr: 0.0100, train loss: 2.5002, val loss: 2.5082\n",
      "1180: lr: 0.0100, train loss: 2.4687, val loss: 2.5225\n",
      "1181: lr: 0.0100, train loss: 2.4905, val loss: 2.5029\n",
      "1182: lr: 0.0100, train loss: 2.4700, val loss: 2.4898\n",
      "1183: lr: 0.0100, train loss: 2.4315, val loss: 2.4760\n",
      "1184: lr: 0.0100, train loss: 2.4386, val loss: 2.4674\n",
      "1185: lr: 0.0100, train loss: 2.4532, val loss: 2.4118\n",
      "1186: lr: 0.0100, train loss: 2.4884, val loss: 2.4547\n",
      "1187: lr: 0.0100, train loss: 2.4725, val loss: 2.4997\n",
      "1188: lr: 0.0100, train loss: 2.4750, val loss: 2.5087\n",
      "1189: lr: 0.0100, train loss: 2.4840, val loss: 2.4790\n",
      "1190: lr: 0.0100, train loss: 2.4521, val loss: 2.4892\n",
      "1191: lr: 0.0100, train loss: 2.5071, val loss: 2.4877\n",
      "1192: lr: 0.0100, train loss: 2.4778, val loss: 2.5332\n",
      "1193: lr: 0.0100, train loss: 2.4385, val loss: 2.4907\n",
      "1194: lr: 0.0100, train loss: 2.4858, val loss: 2.5004\n",
      "1195: lr: 0.0100, train loss: 2.4358, val loss: 2.4684\n",
      "1196: lr: 0.0100, train loss: 2.4501, val loss: 2.4533\n",
      "1197: lr: 0.0100, train loss: 2.4483, val loss: 2.4842\n",
      "1198: lr: 0.0100, train loss: 2.4659, val loss: 2.5061\n",
      "1199: lr: 0.0100, train loss: 2.4776, val loss: 2.4853\n",
      "1200: lr: 0.0100, train loss: 2.4927, val loss: 2.4787\n",
      "1201: lr: 0.0100, train loss: 2.4502, val loss: 2.5041\n",
      "1202: lr: 0.0100, train loss: 2.4679, val loss: 2.4869\n",
      "1203: lr: 0.0100, train loss: 2.4200, val loss: 2.5039\n",
      "1204: lr: 0.0100, train loss: 2.5123, val loss: 2.4584\n",
      "1205: lr: 0.0100, train loss: 2.4367, val loss: 2.4655\n",
      "1206: lr: 0.0100, train loss: 2.4954, val loss: 2.4975\n",
      "1207: lr: 0.0100, train loss: 2.4575, val loss: 2.4501\n",
      "1208: lr: 0.0100, train loss: 2.4564, val loss: 2.4632\n",
      "1209: lr: 0.0100, train loss: 2.4584, val loss: 2.4855\n",
      "1210: lr: 0.0100, train loss: 2.4745, val loss: 2.4240\n",
      "1211: lr: 0.0100, train loss: 2.4800, val loss: 2.4781\n",
      "1212: lr: 0.0100, train loss: 2.4609, val loss: 2.5233\n",
      "1213: lr: 0.0100, train loss: 2.5126, val loss: 2.4480\n",
      "1214: lr: 0.0100, train loss: 2.4209, val loss: 2.4754\n",
      "1215: lr: 0.0100, train loss: 2.4381, val loss: 2.4702\n",
      "1216: lr: 0.0100, train loss: 2.4631, val loss: 2.4954\n",
      "1217: lr: 0.0100, train loss: 2.4686, val loss: 2.5225\n",
      "1218: lr: 0.0100, train loss: 2.4401, val loss: 2.4971\n",
      "1219: lr: 0.0100, train loss: 2.5056, val loss: 2.4870\n",
      "1220: lr: 0.0100, train loss: 2.4824, val loss: 2.5180\n",
      "1221: lr: 0.0100, train loss: 2.4762, val loss: 2.4455\n",
      "1222: lr: 0.0100, train loss: 2.4406, val loss: 2.5016\n",
      "1223: lr: 0.0100, train loss: 2.4273, val loss: 2.5301\n",
      "1224: lr: 0.0100, train loss: 2.4425, val loss: 2.4953\n",
      "1225: lr: 0.0100, train loss: 2.4547, val loss: 2.5078\n",
      "1226: lr: 0.0100, train loss: 2.4583, val loss: 2.5057\n",
      "1227: lr: 0.0100, train loss: 2.4712, val loss: 2.4576\n",
      "1228: lr: 0.0100, train loss: 2.4600, val loss: 2.4936\n",
      "1229: lr: 0.0100, train loss: 2.4565, val loss: 2.5165\n",
      "1230: lr: 0.0100, train loss: 2.4483, val loss: 2.4667\n",
      "1231: lr: 0.0100, train loss: 2.4765, val loss: 2.5171\n",
      "1232: lr: 0.0100, train loss: 2.4879, val loss: 2.4881\n",
      "1233: lr: 0.0100, train loss: 2.4818, val loss: 2.5042\n",
      "1234: lr: 0.0100, train loss: 2.4403, val loss: 2.5455\n",
      "1235: lr: 0.0100, train loss: 2.4793, val loss: 2.4795\n",
      "1236: lr: 0.0100, train loss: 2.4749, val loss: 2.4735\n",
      "1237: lr: 0.0100, train loss: 2.4798, val loss: 2.4557\n",
      "1238: lr: 0.0100, train loss: 2.4236, val loss: 2.4638\n",
      "1239: lr: 0.0100, train loss: 2.4934, val loss: 2.4842\n",
      "1240: lr: 0.0100, train loss: 2.4481, val loss: 2.4471\n",
      "1241: lr: 0.0100, train loss: 2.4582, val loss: 2.4861\n",
      "1242: lr: 0.0100, train loss: 2.4683, val loss: 2.4948\n",
      "1243: lr: 0.0100, train loss: 2.4681, val loss: 2.5014\n",
      "1244: lr: 0.0100, train loss: 2.4785, val loss: 2.4537\n",
      "1245: lr: 0.0100, train loss: 2.4598, val loss: 2.4971\n",
      "1246: lr: 0.0100, train loss: 2.4354, val loss: 2.4705\n",
      "1247: lr: 0.0100, train loss: 2.4612, val loss: 2.5088\n",
      "1248: lr: 0.0100, train loss: 2.4001, val loss: 2.4690\n",
      "1249: lr: 0.0100, train loss: 2.4574, val loss: 2.5351\n",
      "1250: lr: 0.0100, train loss: 2.4224, val loss: 2.5459\n",
      "1251: lr: 0.0100, train loss: 2.4738, val loss: 2.4547\n",
      "1252: lr: 0.0100, train loss: 2.4934, val loss: 2.4465\n",
      "1253: lr: 0.0100, train loss: 2.4724, val loss: 2.5148\n",
      "1254: lr: 0.0100, train loss: 2.4427, val loss: 2.5153\n",
      "1255: lr: 0.0100, train loss: 2.4364, val loss: 2.4807\n",
      "1256: lr: 0.0100, train loss: 2.4819, val loss: 2.5027\n",
      "1257: lr: 0.0100, train loss: 2.4789, val loss: 2.4677\n",
      "1258: lr: 0.0100, train loss: 2.4756, val loss: 2.5566\n",
      "1259: lr: 0.0100, train loss: 2.4741, val loss: 2.4362\n",
      "1260: lr: 0.0100, train loss: 2.4492, val loss: 2.4719\n",
      "1261: lr: 0.0100, train loss: 2.4984, val loss: 2.4645\n",
      "1262: lr: 0.0100, train loss: 2.4020, val loss: 2.5095\n",
      "1263: lr: 0.0100, train loss: 2.4811, val loss: 2.4824\n",
      "1264: lr: 0.0100, train loss: 2.4690, val loss: 2.4782\n",
      "1265: lr: 0.0100, train loss: 2.4365, val loss: 2.5344\n",
      "1266: lr: 0.0100, train loss: 2.5300, val loss: 2.5354\n",
      "1267: lr: 0.0100, train loss: 2.4989, val loss: 2.4442\n",
      "1268: lr: 0.0100, train loss: 2.4477, val loss: 2.4940\n",
      "1269: lr: 0.0100, train loss: 2.4959, val loss: 2.5202\n",
      "1270: lr: 0.0100, train loss: 2.4624, val loss: 2.4849\n",
      "1271: lr: 0.0100, train loss: 2.4403, val loss: 2.4580\n",
      "1272: lr: 0.0100, train loss: 2.4556, val loss: 2.4992\n",
      "1273: lr: 0.0100, train loss: 2.4430, val loss: 2.4995\n",
      "1274: lr: 0.0100, train loss: 2.4754, val loss: 2.4947\n",
      "1275: lr: 0.0100, train loss: 2.4117, val loss: 2.4687\n",
      "1276: lr: 0.0100, train loss: 2.4423, val loss: 2.5113\n",
      "1277: lr: 0.0100, train loss: 2.4885, val loss: 2.4636\n",
      "1278: lr: 0.0100, train loss: 2.4752, val loss: 2.5042\n",
      "1279: lr: 0.0100, train loss: 2.4495, val loss: 2.4829\n",
      "1280: lr: 0.0100, train loss: 2.4148, val loss: 2.4747\n",
      "1281: lr: 0.0100, train loss: 2.4671, val loss: 2.4908\n",
      "1282: lr: 0.0100, train loss: 2.4594, val loss: 2.4714\n",
      "1283: lr: 0.0100, train loss: 2.4337, val loss: 2.4625\n",
      "1284: lr: 0.0100, train loss: 2.4773, val loss: 2.4765\n",
      "1285: lr: 0.0100, train loss: 2.4302, val loss: 2.4932\n",
      "1286: lr: 0.0100, train loss: 2.4670, val loss: 2.4859\n",
      "1287: lr: 0.0100, train loss: 2.5016, val loss: 2.4951\n",
      "1288: lr: 0.0100, train loss: 2.4356, val loss: 2.4325\n",
      "1289: lr: 0.0100, train loss: 2.4799, val loss: 2.5323\n",
      "1290: lr: 0.0100, train loss: 2.4655, val loss: 2.4827\n",
      "1291: lr: 0.0100, train loss: 2.4493, val loss: 2.4875\n",
      "1292: lr: 0.0100, train loss: 2.4713, val loss: 2.5282\n",
      "1293: lr: 0.0100, train loss: 2.4589, val loss: 2.4878\n",
      "1294: lr: 0.0100, train loss: 2.4692, val loss: 2.5121\n",
      "1295: lr: 0.0100, train loss: 2.3886, val loss: 2.4913\n",
      "1296: lr: 0.0100, train loss: 2.4423, val loss: 2.5219\n",
      "1297: lr: 0.0100, train loss: 2.4441, val loss: 2.4705\n",
      "1298: lr: 0.0100, train loss: 2.5031, val loss: 2.4764\n",
      "1299: lr: 0.0100, train loss: 2.4767, val loss: 2.5050\n",
      "1300: lr: 0.0100, train loss: 2.5064, val loss: 2.5197\n",
      "1301: lr: 0.0100, train loss: 2.4493, val loss: 2.4718\n",
      "1302: lr: 0.0100, train loss: 2.4520, val loss: 2.4884\n",
      "1303: lr: 0.0100, train loss: 2.4588, val loss: 2.4368\n",
      "1304: lr: 0.0100, train loss: 2.4389, val loss: 2.4773\n",
      "1305: lr: 0.0100, train loss: 2.4157, val loss: 2.5104\n",
      "1306: lr: 0.0100, train loss: 2.4769, val loss: 2.5033\n",
      "1307: lr: 0.0100, train loss: 2.4958, val loss: 2.5075\n",
      "1308: lr: 0.0100, train loss: 2.4192, val loss: 2.5052\n",
      "1309: lr: 0.0100, train loss: 2.4235, val loss: 2.4969\n",
      "1310: lr: 0.0100, train loss: 2.4376, val loss: 2.4627\n",
      "1311: lr: 0.0100, train loss: 2.4598, val loss: 2.4873\n",
      "1312: lr: 0.0100, train loss: 2.4402, val loss: 2.5120\n",
      "1313: lr: 0.0100, train loss: 2.4169, val loss: 2.4717\n",
      "1314: lr: 0.0100, train loss: 2.4867, val loss: 2.4883\n",
      "1315: lr: 0.0100, train loss: 2.4694, val loss: 2.4891\n",
      "1316: lr: 0.0100, train loss: 2.4333, val loss: 2.4662\n",
      "1317: lr: 0.0100, train loss: 2.4339, val loss: 2.4342\n",
      "1318: lr: 0.0100, train loss: 2.4339, val loss: 2.4937\n",
      "1319: lr: 0.0100, train loss: 2.4428, val loss: 2.5003\n",
      "1320: lr: 0.0100, train loss: 2.4836, val loss: 2.4861\n",
      "1321: lr: 0.0100, train loss: 2.4385, val loss: 2.5188\n",
      "1322: lr: 0.0100, train loss: 2.4942, val loss: 2.4901\n",
      "1323: lr: 0.0100, train loss: 2.4343, val loss: 2.4569\n",
      "1324: lr: 0.0100, train loss: 2.4930, val loss: 2.5076\n",
      "1325: lr: 0.0100, train loss: 2.4468, val loss: 2.4823\n",
      "1326: lr: 0.0100, train loss: 2.4488, val loss: 2.4531\n",
      "1327: lr: 0.0100, train loss: 2.4657, val loss: 2.4739\n",
      "1328: lr: 0.0100, train loss: 2.4209, val loss: 2.5072\n",
      "1329: lr: 0.0100, train loss: 2.4593, val loss: 2.4839\n",
      "1330: lr: 0.0100, train loss: 2.4434, val loss: 2.4840\n",
      "1331: lr: 0.0100, train loss: 2.4242, val loss: 2.4633\n",
      "1332: lr: 0.0100, train loss: 2.4575, val loss: 2.4865\n",
      "1333: lr: 0.0100, train loss: 2.4500, val loss: 2.4971\n",
      "1334: lr: 0.0100, train loss: 2.4471, val loss: 2.5147\n",
      "1335: lr: 0.0100, train loss: 2.4675, val loss: 2.4881\n",
      "1336: lr: 0.0100, train loss: 2.4518, val loss: 2.4933\n",
      "1337: lr: 0.0100, train loss: 2.4382, val loss: 2.4586\n",
      "1338: lr: 0.0100, train loss: 2.5164, val loss: 2.4578\n",
      "1339: lr: 0.0100, train loss: 2.4564, val loss: 2.5008\n",
      "1340: lr: 0.0100, train loss: 2.4949, val loss: 2.4825\n",
      "1341: lr: 0.0100, train loss: 2.4314, val loss: 2.4538\n",
      "1342: lr: 0.0100, train loss: 2.4740, val loss: 2.5182\n",
      "1343: lr: 0.0100, train loss: 2.4859, val loss: 2.5032\n",
      "1344: lr: 0.0100, train loss: 2.4521, val loss: 2.4558\n",
      "1345: lr: 0.0100, train loss: 2.4678, val loss: 2.4478\n",
      "1346: lr: 0.0100, train loss: 2.4889, val loss: 2.5379\n",
      "1347: lr: 0.0100, train loss: 2.4591, val loss: 2.4848\n",
      "1348: lr: 0.0100, train loss: 2.4565, val loss: 2.5297\n",
      "1349: lr: 0.0100, train loss: 2.4729, val loss: 2.4918\n",
      "1350: lr: 0.0100, train loss: 2.4741, val loss: 2.4803\n",
      "1351: lr: 0.0100, train loss: 2.4686, val loss: 2.5432\n",
      "1352: lr: 0.0100, train loss: 2.4843, val loss: 2.5292\n",
      "1353: lr: 0.0100, train loss: 2.4949, val loss: 2.4680\n",
      "1354: lr: 0.0100, train loss: 2.4646, val loss: 2.4892\n",
      "1355: lr: 0.0100, train loss: 2.4605, val loss: 2.4244\n",
      "1356: lr: 0.0100, train loss: 2.4610, val loss: 2.4869\n",
      "1357: lr: 0.0100, train loss: 2.4483, val loss: 2.5336\n",
      "1358: lr: 0.0100, train loss: 2.4661, val loss: 2.4453\n",
      "1359: lr: 0.0100, train loss: 2.4387, val loss: 2.4551\n",
      "1360: lr: 0.0100, train loss: 2.4693, val loss: 2.5000\n",
      "1361: lr: 0.0100, train loss: 2.4682, val loss: 2.5279\n",
      "1362: lr: 0.0100, train loss: 2.4385, val loss: 2.5116\n",
      "1363: lr: 0.0100, train loss: 2.4790, val loss: 2.4786\n",
      "1364: lr: 0.0100, train loss: 2.4243, val loss: 2.5134\n",
      "1365: lr: 0.0100, train loss: 2.4963, val loss: 2.5125\n",
      "1366: lr: 0.0100, train loss: 2.4110, val loss: 2.5002\n",
      "1367: lr: 0.0100, train loss: 2.4576, val loss: 2.5198\n",
      "1368: lr: 0.0100, train loss: 2.4465, val loss: 2.4926\n",
      "1369: lr: 0.0100, train loss: 2.4931, val loss: 2.4474\n",
      "1370: lr: 0.0100, train loss: 2.5140, val loss: 2.4986\n",
      "1371: lr: 0.0100, train loss: 2.5337, val loss: 2.5326\n",
      "1372: lr: 0.0100, train loss: 2.4823, val loss: 2.5034\n",
      "1373: lr: 0.0100, train loss: 2.4796, val loss: 2.4868\n",
      "1374: lr: 0.0100, train loss: 2.4590, val loss: 2.4838\n",
      "1375: lr: 0.0100, train loss: 2.4775, val loss: 2.4483\n",
      "1376: lr: 0.0100, train loss: 2.4753, val loss: 2.5046\n",
      "1377: lr: 0.0100, train loss: 2.4644, val loss: 2.4509\n",
      "1378: lr: 0.0100, train loss: 2.4994, val loss: 2.4871\n",
      "1379: lr: 0.0100, train loss: 2.4605, val loss: 2.4686\n",
      "1380: lr: 0.0100, train loss: 2.4298, val loss: 2.5087\n",
      "1381: lr: 0.0100, train loss: 2.4665, val loss: 2.5289\n",
      "1382: lr: 0.0100, train loss: 2.4608, val loss: 2.5071\n",
      "1383: lr: 0.0100, train loss: 2.4501, val loss: 2.4901\n",
      "1384: lr: 0.0100, train loss: 2.5019, val loss: 2.5316\n",
      "1385: lr: 0.0100, train loss: 2.4876, val loss: 2.4760\n",
      "1386: lr: 0.0100, train loss: 2.4269, val loss: 2.4972\n",
      "1387: lr: 0.0100, train loss: 2.4375, val loss: 2.4992\n",
      "1388: lr: 0.0100, train loss: 2.4526, val loss: 2.5275\n",
      "1389: lr: 0.0100, train loss: 2.4760, val loss: 2.4887\n",
      "1390: lr: 0.0100, train loss: 2.4657, val loss: 2.5193\n",
      "1391: lr: 0.0100, train loss: 2.4685, val loss: 2.5097\n",
      "1392: lr: 0.0100, train loss: 2.4974, val loss: 2.5037\n",
      "1393: lr: 0.0100, train loss: 2.4234, val loss: 2.4695\n",
      "1394: lr: 0.0100, train loss: 2.4378, val loss: 2.4835\n",
      "1395: lr: 0.0100, train loss: 2.4835, val loss: 2.4694\n",
      "1396: lr: 0.0100, train loss: 2.4296, val loss: 2.5622\n",
      "1397: lr: 0.0100, train loss: 2.4766, val loss: 2.4397\n",
      "1398: lr: 0.0100, train loss: 2.4551, val loss: 2.4762\n",
      "1399: lr: 0.0100, train loss: 2.5052, val loss: 2.4929\n",
      "1400: lr: 0.0100, train loss: 2.4947, val loss: 2.4763\n",
      "1401: lr: 0.0100, train loss: 2.4677, val loss: 2.4546\n",
      "1402: lr: 0.0100, train loss: 2.4330, val loss: 2.5379\n",
      "1403: lr: 0.0100, train loss: 2.4412, val loss: 2.4908\n",
      "1404: lr: 0.0100, train loss: 2.4530, val loss: 2.4670\n",
      "1405: lr: 0.0100, train loss: 2.4827, val loss: 2.4846\n",
      "1406: lr: 0.0100, train loss: 2.4691, val loss: 2.4766\n",
      "1407: lr: 0.0100, train loss: 2.4756, val loss: 2.4730\n",
      "1408: lr: 0.0100, train loss: 2.4633, val loss: 2.5362\n",
      "1409: lr: 0.0100, train loss: 2.4256, val loss: 2.4503\n",
      "1410: lr: 0.0100, train loss: 2.4447, val loss: 2.5366\n",
      "1411: lr: 0.0100, train loss: 2.4752, val loss: 2.4757\n",
      "1412: lr: 0.0100, train loss: 2.4484, val loss: 2.4682\n",
      "1413: lr: 0.0100, train loss: 2.4962, val loss: 2.5097\n",
      "1414: lr: 0.0100, train loss: 2.4714, val loss: 2.5000\n",
      "1415: lr: 0.0100, train loss: 2.4080, val loss: 2.4909\n",
      "1416: lr: 0.0100, train loss: 2.4506, val loss: 2.4376\n",
      "1417: lr: 0.0100, train loss: 2.4085, val loss: 2.4337\n",
      "1418: lr: 0.0100, train loss: 2.5033, val loss: 2.4722\n",
      "1419: lr: 0.0100, train loss: 2.4591, val loss: 2.4983\n",
      "1420: lr: 0.0100, train loss: 2.4502, val loss: 2.5191\n",
      "1421: lr: 0.0100, train loss: 2.4564, val loss: 2.5244\n",
      "1422: lr: 0.0100, train loss: 2.4713, val loss: 2.4964\n",
      "1423: lr: 0.0100, train loss: 2.4677, val loss: 2.5099\n",
      "1424: lr: 0.0100, train loss: 2.4556, val loss: 2.4824\n",
      "1425: lr: 0.0100, train loss: 2.4865, val loss: 2.5290\n",
      "1426: lr: 0.0100, train loss: 2.4288, val loss: 2.4806\n",
      "1427: lr: 0.0100, train loss: 2.4505, val loss: 2.5462\n",
      "1428: lr: 0.0100, train loss: 2.4820, val loss: 2.4400\n",
      "1429: lr: 0.0100, train loss: 2.4960, val loss: 2.5183\n",
      "1430: lr: 0.0100, train loss: 2.4764, val loss: 2.4676\n",
      "1431: lr: 0.0100, train loss: 2.4833, val loss: 2.4611\n",
      "1432: lr: 0.0100, train loss: 2.3999, val loss: 2.5457\n",
      "1433: lr: 0.0100, train loss: 2.4881, val loss: 2.5349\n",
      "1434: lr: 0.0100, train loss: 2.4471, val loss: 2.4665\n",
      "1435: lr: 0.0100, train loss: 2.4451, val loss: 2.4665\n",
      "1436: lr: 0.0100, train loss: 2.4207, val loss: 2.4808\n",
      "1437: lr: 0.0100, train loss: 2.4804, val loss: 2.5301\n",
      "1438: lr: 0.0100, train loss: 2.4812, val loss: 2.4700\n",
      "1439: lr: 0.0100, train loss: 2.4497, val loss: 2.4515\n",
      "1440: lr: 0.0100, train loss: 2.4136, val loss: 2.4868\n",
      "1441: lr: 0.0100, train loss: 2.4596, val loss: 2.4944\n",
      "1442: lr: 0.0100, train loss: 2.4920, val loss: 2.4836\n",
      "1443: lr: 0.0100, train loss: 2.4708, val loss: 2.4468\n",
      "1444: lr: 0.0100, train loss: 2.4471, val loss: 2.5105\n",
      "1445: lr: 0.0100, train loss: 2.4684, val loss: 2.4661\n",
      "1446: lr: 0.0100, train loss: 2.4801, val loss: 2.4850\n",
      "1447: lr: 0.0100, train loss: 2.4897, val loss: 2.4688\n",
      "1448: lr: 0.0100, train loss: 2.4466, val loss: 2.5524\n",
      "1449: lr: 0.0100, train loss: 2.4685, val loss: 2.5098\n",
      "1450: lr: 0.0100, train loss: 2.4810, val loss: 2.5359\n",
      "1451: lr: 0.0100, train loss: 2.5024, val loss: 2.4470\n",
      "1452: lr: 0.0100, train loss: 2.4738, val loss: 2.4756\n",
      "1453: lr: 0.0100, train loss: 2.4515, val loss: 2.5181\n",
      "1454: lr: 0.0100, train loss: 2.4757, val loss: 2.4366\n",
      "1455: lr: 0.0100, train loss: 2.4862, val loss: 2.5404\n",
      "1456: lr: 0.0100, train loss: 2.4945, val loss: 2.4747\n",
      "1457: lr: 0.0100, train loss: 2.4234, val loss: 2.4697\n",
      "1458: lr: 0.0100, train loss: 2.5084, val loss: 2.4764\n",
      "1459: lr: 0.0100, train loss: 2.4874, val loss: 2.4094\n",
      "1460: lr: 0.0100, train loss: 2.4843, val loss: 2.4741\n",
      "1461: lr: 0.0100, train loss: 2.4676, val loss: 2.5227\n",
      "1462: lr: 0.0100, train loss: 2.4929, val loss: 2.4992\n",
      "1463: lr: 0.0100, train loss: 2.4814, val loss: 2.4584\n",
      "1464: lr: 0.0100, train loss: 2.4782, val loss: 2.5195\n",
      "1465: lr: 0.0100, train loss: 2.4800, val loss: 2.5033\n",
      "1466: lr: 0.0100, train loss: 2.4994, val loss: 2.4771\n",
      "1467: lr: 0.0100, train loss: 2.4588, val loss: 2.5226\n",
      "1468: lr: 0.0100, train loss: 2.4679, val loss: 2.5081\n",
      "1469: lr: 0.0100, train loss: 2.4612, val loss: 2.4696\n",
      "1470: lr: 0.0100, train loss: 2.4854, val loss: 2.4928\n",
      "1471: lr: 0.0100, train loss: 2.5127, val loss: 2.4473\n",
      "1472: lr: 0.0100, train loss: 2.4703, val loss: 2.5164\n",
      "1473: lr: 0.0100, train loss: 2.4676, val loss: 2.4590\n",
      "1474: lr: 0.0100, train loss: 2.4718, val loss: 2.5044\n",
      "1475: lr: 0.0100, train loss: 2.4559, val loss: 2.4652\n",
      "1476: lr: 0.0100, train loss: 2.4556, val loss: 2.4876\n",
      "1477: lr: 0.0100, train loss: 2.4829, val loss: 2.5383\n",
      "1478: lr: 0.0100, train loss: 2.4155, val loss: 2.4772\n",
      "1479: lr: 0.0100, train loss: 2.5128, val loss: 2.4843\n",
      "1480: lr: 0.0100, train loss: 2.3907, val loss: 2.4857\n",
      "1481: lr: 0.0100, train loss: 2.4575, val loss: 2.4301\n",
      "1482: lr: 0.0100, train loss: 2.4906, val loss: 2.4474\n",
      "1483: lr: 0.0100, train loss: 2.4594, val loss: 2.5062\n",
      "1484: lr: 0.0100, train loss: 2.4672, val loss: 2.4410\n",
      "1485: lr: 0.0100, train loss: 2.4514, val loss: 2.5073\n",
      "1486: lr: 0.0100, train loss: 2.4493, val loss: 2.4585\n",
      "1487: lr: 0.0100, train loss: 2.4577, val loss: 2.4560\n",
      "1488: lr: 0.0100, train loss: 2.4392, val loss: 2.5008\n",
      "1489: lr: 0.0100, train loss: 2.4693, val loss: 2.4793\n",
      "1490: lr: 0.0100, train loss: 2.4900, val loss: 2.4823\n",
      "1491: lr: 0.0100, train loss: 2.4728, val loss: 2.4454\n",
      "1492: lr: 0.0100, train loss: 2.4823, val loss: 2.4689\n",
      "1493: lr: 0.0100, train loss: 2.4671, val loss: 2.5113\n",
      "1494: lr: 0.0100, train loss: 2.4366, val loss: 2.4977\n",
      "1495: lr: 0.0100, train loss: 2.4453, val loss: 2.4885\n",
      "1496: lr: 0.0100, train loss: 2.4963, val loss: 2.5171\n",
      "1497: lr: 0.0100, train loss: 2.4707, val loss: 2.4258\n",
      "1498: lr: 0.0100, train loss: 2.4880, val loss: 2.4870\n",
      "1499: lr: 0.0100, train loss: 2.4791, val loss: 2.4743\n",
      "1500: lr: 0.0100, train loss: 2.4438, val loss: 2.4973\n",
      "1501: lr: 0.0100, train loss: 2.4808, val loss: 2.4962\n",
      "1502: lr: 0.0100, train loss: 2.4750, val loss: 2.5306\n",
      "1503: lr: 0.0100, train loss: 2.4481, val loss: 2.5242\n",
      "1504: lr: 0.0100, train loss: 2.4506, val loss: 2.4913\n",
      "1505: lr: 0.0100, train loss: 2.4597, val loss: 2.4710\n",
      "1506: lr: 0.0100, train loss: 2.4881, val loss: 2.5304\n",
      "1507: lr: 0.0100, train loss: 2.4645, val loss: 2.5026\n",
      "1508: lr: 0.0100, train loss: 2.4720, val loss: 2.4209\n",
      "1509: lr: 0.0100, train loss: 2.4602, val loss: 2.4512\n",
      "1510: lr: 0.0100, train loss: 2.4496, val loss: 2.4973\n",
      "1511: lr: 0.0100, train loss: 2.4269, val loss: 2.5023\n",
      "1512: lr: 0.0100, train loss: 2.4708, val loss: 2.5055\n",
      "1513: lr: 0.0100, train loss: 2.4268, val loss: 2.4863\n",
      "1514: lr: 0.0100, train loss: 2.5000, val loss: 2.4564\n",
      "1515: lr: 0.0100, train loss: 2.4419, val loss: 2.5145\n",
      "1516: lr: 0.0100, train loss: 2.4537, val loss: 2.4942\n",
      "1517: lr: 0.0100, train loss: 2.4609, val loss: 2.5253\n",
      "1518: lr: 0.0100, train loss: 2.4428, val loss: 2.4918\n",
      "1519: lr: 0.0100, train loss: 2.4782, val loss: 2.4730\n",
      "1520: lr: 0.0100, train loss: 2.4893, val loss: 2.5139\n",
      "1521: lr: 0.0100, train loss: 2.4599, val loss: 2.4731\n",
      "1522: lr: 0.0100, train loss: 2.4613, val loss: 2.4948\n",
      "1523: lr: 0.0100, train loss: 2.4656, val loss: 2.4454\n",
      "1524: lr: 0.0100, train loss: 2.4302, val loss: 2.4758\n",
      "1525: lr: 0.0100, train loss: 2.4798, val loss: 2.5083\n",
      "1526: lr: 0.0100, train loss: 2.4559, val loss: 2.5024\n",
      "1527: lr: 0.0100, train loss: 2.4955, val loss: 2.4623\n",
      "1528: lr: 0.0100, train loss: 2.4300, val loss: 2.4872\n",
      "1529: lr: 0.0100, train loss: 2.4390, val loss: 2.4383\n",
      "1530: lr: 0.0100, train loss: 2.4545, val loss: 2.4766\n",
      "1531: lr: 0.0100, train loss: 2.4674, val loss: 2.4762\n",
      "1532: lr: 0.0100, train loss: 2.4501, val loss: 2.5131\n",
      "1533: lr: 0.0100, train loss: 2.4808, val loss: 2.4615\n",
      "1534: lr: 0.0100, train loss: 2.4156, val loss: 2.5565\n",
      "1535: lr: 0.0100, train loss: 2.4320, val loss: 2.4981\n",
      "1536: lr: 0.0100, train loss: 2.4586, val loss: 2.4911\n",
      "1537: lr: 0.0100, train loss: 2.4560, val loss: 2.4996\n",
      "1538: lr: 0.0100, train loss: 2.4563, val loss: 2.4823\n",
      "1539: lr: 0.0100, train loss: 2.4368, val loss: 2.4432\n",
      "1540: lr: 0.0100, train loss: 2.4157, val loss: 2.5155\n",
      "1541: lr: 0.0100, train loss: 2.4945, val loss: 2.5097\n",
      "1542: lr: 0.0100, train loss: 2.4655, val loss: 2.5418\n",
      "1543: lr: 0.0100, train loss: 2.4505, val loss: 2.5275\n",
      "1544: lr: 0.0100, train loss: 2.4558, val loss: 2.4945\n",
      "1545: lr: 0.0100, train loss: 2.4764, val loss: 2.4934\n",
      "1546: lr: 0.0100, train loss: 2.5005, val loss: 2.4375\n",
      "1547: lr: 0.0100, train loss: 2.4888, val loss: 2.4579\n",
      "1548: lr: 0.0100, train loss: 2.4213, val loss: 2.4969\n",
      "1549: lr: 0.0100, train loss: 2.4722, val loss: 2.4984\n",
      "1550: lr: 0.0100, train loss: 2.4671, val loss: 2.5095\n",
      "1551: lr: 0.0100, train loss: 2.4318, val loss: 2.4622\n",
      "1552: lr: 0.0100, train loss: 2.4264, val loss: 2.4513\n",
      "1553: lr: 0.0100, train loss: 2.4594, val loss: 2.4759\n",
      "1554: lr: 0.0100, train loss: 2.4515, val loss: 2.4944\n",
      "1555: lr: 0.0100, train loss: 2.4315, val loss: 2.4928\n",
      "1556: lr: 0.0100, train loss: 2.4484, val loss: 2.4599\n",
      "1557: lr: 0.0100, train loss: 2.4010, val loss: 2.5341\n",
      "1558: lr: 0.0100, train loss: 2.4704, val loss: 2.5493\n",
      "1559: lr: 0.0100, train loss: 2.5081, val loss: 2.4902\n",
      "1560: lr: 0.0100, train loss: 2.4434, val loss: 2.4573\n",
      "1561: lr: 0.0100, train loss: 2.4783, val loss: 2.5095\n",
      "1562: lr: 0.0100, train loss: 2.4815, val loss: 2.4818\n",
      "1563: lr: 0.0100, train loss: 2.4781, val loss: 2.5193\n",
      "1564: lr: 0.0100, train loss: 2.4569, val loss: 2.5040\n",
      "1565: lr: 0.0100, train loss: 2.4920, val loss: 2.4508\n",
      "1566: lr: 0.0100, train loss: 2.4524, val loss: 2.4470\n",
      "1567: lr: 0.0100, train loss: 2.4388, val loss: 2.4760\n",
      "1568: lr: 0.0100, train loss: 2.4789, val loss: 2.4295\n",
      "1569: lr: 0.0100, train loss: 2.4637, val loss: 2.4791\n",
      "1570: lr: 0.0100, train loss: 2.4306, val loss: 2.4943\n",
      "1571: lr: 0.0100, train loss: 2.4356, val loss: 2.5212\n",
      "1572: lr: 0.0100, train loss: 2.4719, val loss: 2.4816\n",
      "1573: lr: 0.0100, train loss: 2.4399, val loss: 2.4950\n",
      "1574: lr: 0.0100, train loss: 2.4461, val loss: 2.4626\n",
      "1575: lr: 0.0100, train loss: 2.4778, val loss: 2.5249\n",
      "1576: lr: 0.0100, train loss: 2.4533, val loss: 2.5216\n",
      "1577: lr: 0.0100, train loss: 2.4551, val loss: 2.5157\n",
      "1578: lr: 0.0100, train loss: 2.4543, val loss: 2.5347\n",
      "1579: lr: 0.0100, train loss: 2.4408, val loss: 2.5114\n",
      "1580: lr: 0.0100, train loss: 2.4862, val loss: 2.4885\n",
      "1581: lr: 0.0100, train loss: 2.4666, val loss: 2.4874\n",
      "1582: lr: 0.0100, train loss: 2.4567, val loss: 2.4493\n",
      "1583: lr: 0.0100, train loss: 2.4501, val loss: 2.5602\n",
      "1584: lr: 0.0100, train loss: 2.4280, val loss: 2.4834\n",
      "1585: lr: 0.0100, train loss: 2.4477, val loss: 2.4629\n",
      "1586: lr: 0.0100, train loss: 2.4266, val loss: 2.4861\n",
      "1587: lr: 0.0100, train loss: 2.4533, val loss: 2.5173\n",
      "1588: lr: 0.0100, train loss: 2.4275, val loss: 2.5244\n",
      "1589: lr: 0.0100, train loss: 2.4401, val loss: 2.5046\n",
      "1590: lr: 0.0100, train loss: 2.5124, val loss: 2.4972\n",
      "1591: lr: 0.0100, train loss: 2.4832, val loss: 2.5014\n",
      "1592: lr: 0.0100, train loss: 2.4347, val loss: 2.4903\n",
      "1593: lr: 0.0100, train loss: 2.4229, val loss: 2.5618\n",
      "1594: lr: 0.0100, train loss: 2.4792, val loss: 2.4801\n",
      "1595: lr: 0.0100, train loss: 2.4694, val loss: 2.5731\n",
      "1596: lr: 0.0100, train loss: 2.4627, val loss: 2.5134\n",
      "1597: lr: 0.0100, train loss: 2.4349, val loss: 2.4793\n",
      "1598: lr: 0.0100, train loss: 2.4912, val loss: 2.4704\n",
      "1599: lr: 0.0100, train loss: 2.4656, val loss: 2.5024\n",
      "1600: lr: 0.0100, train loss: 2.4755, val loss: 2.5218\n",
      "1601: lr: 0.0100, train loss: 2.4314, val loss: 2.4521\n",
      "1602: lr: 0.0100, train loss: 2.4696, val loss: 2.4191\n",
      "1603: lr: 0.0100, train loss: 2.4487, val loss: 2.4801\n",
      "1604: lr: 0.0100, train loss: 2.4181, val loss: 2.4886\n",
      "1605: lr: 0.0100, train loss: 2.4380, val loss: 2.4769\n",
      "1606: lr: 0.0100, train loss: 2.4566, val loss: 2.4421\n",
      "1607: lr: 0.0100, train loss: 2.3987, val loss: 2.4782\n",
      "1608: lr: 0.0100, train loss: 2.4831, val loss: 2.4267\n",
      "1609: lr: 0.0100, train loss: 2.4703, val loss: 2.4768\n",
      "1610: lr: 0.0100, train loss: 2.4291, val loss: 2.5196\n",
      "1611: lr: 0.0100, train loss: 2.4475, val loss: 2.4552\n",
      "1612: lr: 0.0100, train loss: 2.4649, val loss: 2.4878\n",
      "1613: lr: 0.0100, train loss: 2.4727, val loss: 2.4864\n",
      "1614: lr: 0.0100, train loss: 2.4424, val loss: 2.4405\n",
      "1615: lr: 0.0100, train loss: 2.4577, val loss: 2.5299\n",
      "1616: lr: 0.0100, train loss: 2.4750, val loss: 2.5059\n",
      "1617: lr: 0.0100, train loss: 2.4111, val loss: 2.4729\n",
      "1618: lr: 0.0100, train loss: 2.4582, val loss: 2.4811\n",
      "1619: lr: 0.0100, train loss: 2.4556, val loss: 2.4716\n",
      "1620: lr: 0.0100, train loss: 2.4773, val loss: 2.5175\n",
      "1621: lr: 0.0100, train loss: 2.4441, val loss: 2.5155\n",
      "1622: lr: 0.0100, train loss: 2.4244, val loss: 2.5247\n",
      "1623: lr: 0.0100, train loss: 2.4811, val loss: 2.4772\n",
      "1624: lr: 0.0100, train loss: 2.4358, val loss: 2.4348\n",
      "1625: lr: 0.0100, train loss: 2.4537, val loss: 2.5157\n",
      "1626: lr: 0.0100, train loss: 2.4340, val loss: 2.4782\n",
      "1627: lr: 0.0100, train loss: 2.4777, val loss: 2.5077\n",
      "1628: lr: 0.0100, train loss: 2.4260, val loss: 2.4990\n",
      "1629: lr: 0.0100, train loss: 2.4068, val loss: 2.4542\n",
      "1630: lr: 0.0100, train loss: 2.4294, val loss: 2.4890\n",
      "1631: lr: 0.0100, train loss: 2.4021, val loss: 2.5094\n",
      "1632: lr: 0.0100, train loss: 2.4393, val loss: 2.5338\n",
      "1633: lr: 0.0100, train loss: 2.4944, val loss: 2.5112\n",
      "1634: lr: 0.0100, train loss: 2.4531, val loss: 2.4609\n",
      "1635: lr: 0.0100, train loss: 2.4691, val loss: 2.5390\n",
      "1636: lr: 0.0100, train loss: 2.4527, val loss: 2.4844\n",
      "1637: lr: 0.0100, train loss: 2.4965, val loss: 2.4899\n",
      "1638: lr: 0.0100, train loss: 2.4619, val loss: 2.4807\n",
      "1639: lr: 0.0100, train loss: 2.4731, val loss: 2.4944\n",
      "1640: lr: 0.0100, train loss: 2.4619, val loss: 2.5548\n",
      "1641: lr: 0.0100, train loss: 2.4964, val loss: 2.4689\n",
      "1642: lr: 0.0100, train loss: 2.4596, val loss: 2.4678\n",
      "1643: lr: 0.0100, train loss: 2.4736, val loss: 2.5411\n",
      "1644: lr: 0.0100, train loss: 2.4664, val loss: 2.5045\n",
      "1645: lr: 0.0100, train loss: 2.4730, val loss: 2.4839\n",
      "1646: lr: 0.0100, train loss: 2.4716, val loss: 2.5127\n",
      "1647: lr: 0.0100, train loss: 2.4281, val loss: 2.5348\n",
      "1648: lr: 0.0100, train loss: 2.4586, val loss: 2.5315\n",
      "1649: lr: 0.0100, train loss: 2.4270, val loss: 2.4911\n",
      "1650: lr: 0.0100, train loss: 2.4392, val loss: 2.4678\n",
      "1651: lr: 0.0100, train loss: 2.4683, val loss: 2.5234\n",
      "1652: lr: 0.0100, train loss: 2.4496, val loss: 2.4786\n",
      "1653: lr: 0.0100, train loss: 2.4989, val loss: 2.4632\n",
      "1654: lr: 0.0100, train loss: 2.4858, val loss: 2.4630\n",
      "1655: lr: 0.0100, train loss: 2.4584, val loss: 2.4236\n",
      "1656: lr: 0.0100, train loss: 2.4565, val loss: 2.4761\n",
      "1657: lr: 0.0100, train loss: 2.4670, val loss: 2.5131\n",
      "1658: lr: 0.0100, train loss: 2.4779, val loss: 2.4954\n",
      "1659: lr: 0.0100, train loss: 2.4670, val loss: 2.5116\n",
      "1660: lr: 0.0100, train loss: 2.4221, val loss: 2.5206\n",
      "1661: lr: 0.0100, train loss: 2.4909, val loss: 2.5134\n",
      "1662: lr: 0.0100, train loss: 2.4738, val loss: 2.5339\n",
      "1663: lr: 0.0100, train loss: 2.5017, val loss: 2.5324\n",
      "1664: lr: 0.0100, train loss: 2.4861, val loss: 2.5182\n",
      "1665: lr: 0.0100, train loss: 2.5007, val loss: 2.4586\n",
      "1666: lr: 0.0100, train loss: 2.4258, val loss: 2.5444\n",
      "1667: lr: 0.0100, train loss: 2.4574, val loss: 2.5032\n",
      "1668: lr: 0.0100, train loss: 2.4497, val loss: 2.4543\n",
      "1669: lr: 0.0100, train loss: 2.4368, val loss: 2.5066\n",
      "1670: lr: 0.0100, train loss: 2.4509, val loss: 2.5362\n",
      "1671: lr: 0.0100, train loss: 2.4638, val loss: 2.4484\n",
      "1672: lr: 0.0100, train loss: 2.4749, val loss: 2.4357\n",
      "1673: lr: 0.0100, train loss: 2.4119, val loss: 2.4955\n",
      "1674: lr: 0.0100, train loss: 2.4959, val loss: 2.5344\n",
      "1675: lr: 0.0100, train loss: 2.4670, val loss: 2.5423\n",
      "1676: lr: 0.0100, train loss: 2.4609, val loss: 2.5305\n",
      "1677: lr: 0.0100, train loss: 2.4476, val loss: 2.5185\n",
      "1678: lr: 0.0100, train loss: 2.4357, val loss: 2.4812\n",
      "1679: lr: 0.0100, train loss: 2.4415, val loss: 2.4796\n",
      "1680: lr: 0.0100, train loss: 2.4578, val loss: 2.5034\n",
      "1681: lr: 0.0100, train loss: 2.4655, val loss: 2.4734\n",
      "1682: lr: 0.0100, train loss: 2.4535, val loss: 2.5298\n",
      "1683: lr: 0.0100, train loss: 2.4527, val loss: 2.5578\n",
      "1684: lr: 0.0100, train loss: 2.4767, val loss: 2.5543\n",
      "1685: lr: 0.0100, train loss: 2.4633, val loss: 2.4523\n",
      "1686: lr: 0.0100, train loss: 2.4624, val loss: 2.4934\n",
      "1687: lr: 0.0100, train loss: 2.4370, val loss: 2.4991\n",
      "1688: lr: 0.0100, train loss: 2.4505, val loss: 2.4785\n",
      "1689: lr: 0.0100, train loss: 2.4409, val loss: 2.4805\n",
      "1690: lr: 0.0100, train loss: 2.4899, val loss: 2.4528\n",
      "1691: lr: 0.0100, train loss: 2.4663, val loss: 2.4757\n",
      "1692: lr: 0.0100, train loss: 2.4610, val loss: 2.4503\n",
      "1693: lr: 0.0100, train loss: 2.4266, val loss: 2.4722\n",
      "1694: lr: 0.0100, train loss: 2.4308, val loss: 2.5402\n",
      "1695: lr: 0.0100, train loss: 2.4851, val loss: 2.5094\n",
      "1696: lr: 0.0100, train loss: 2.4087, val loss: 2.4555\n",
      "1697: lr: 0.0100, train loss: 2.4621, val loss: 2.4284\n",
      "1698: lr: 0.0100, train loss: 2.4636, val loss: 2.4824\n",
      "1699: lr: 0.0100, train loss: 2.4603, val loss: 2.4835\n",
      "1700: lr: 0.0100, train loss: 2.4865, val loss: 2.5387\n",
      "1701: lr: 0.0100, train loss: 2.4616, val loss: 2.4721\n",
      "1702: lr: 0.0100, train loss: 2.4802, val loss: 2.4653\n",
      "1703: lr: 0.0100, train loss: 2.5320, val loss: 2.4879\n",
      "1704: lr: 0.0100, train loss: 2.4429, val loss: 2.4878\n",
      "1705: lr: 0.0100, train loss: 2.4426, val loss: 2.4410\n",
      "1706: lr: 0.0100, train loss: 2.4311, val loss: 2.4780\n",
      "1707: lr: 0.0100, train loss: 2.4865, val loss: 2.5397\n",
      "1708: lr: 0.0100, train loss: 2.4842, val loss: 2.4927\n",
      "1709: lr: 0.0100, train loss: 2.4155, val loss: 2.4928\n",
      "1710: lr: 0.0100, train loss: 2.4175, val loss: 2.4829\n",
      "1711: lr: 0.0100, train loss: 2.4598, val loss: 2.4921\n",
      "1712: lr: 0.0100, train loss: 2.4180, val loss: 2.5003\n",
      "1713: lr: 0.0100, train loss: 2.4295, val loss: 2.5028\n",
      "1714: lr: 0.0100, train loss: 2.4565, val loss: 2.4213\n",
      "1715: lr: 0.0100, train loss: 2.4696, val loss: 2.4992\n",
      "1716: lr: 0.0100, train loss: 2.4850, val loss: 2.4657\n",
      "1717: lr: 0.0100, train loss: 2.4580, val loss: 2.4419\n",
      "1718: lr: 0.0100, train loss: 2.4361, val loss: 2.4876\n",
      "1719: lr: 0.0100, train loss: 2.4550, val loss: 2.5466\n",
      "1720: lr: 0.0100, train loss: 2.4446, val loss: 2.4601\n",
      "1721: lr: 0.0100, train loss: 2.4732, val loss: 2.4661\n",
      "1722: lr: 0.0100, train loss: 2.4987, val loss: 2.5326\n",
      "1723: lr: 0.0100, train loss: 2.4356, val loss: 2.5219\n",
      "1724: lr: 0.0100, train loss: 2.5135, val loss: 2.4548\n",
      "1725: lr: 0.0100, train loss: 2.4925, val loss: 2.4796\n",
      "1726: lr: 0.0100, train loss: 2.4452, val loss: 2.4606\n",
      "1727: lr: 0.0100, train loss: 2.4504, val loss: 2.4645\n",
      "1728: lr: 0.0100, train loss: 2.4231, val loss: 2.4408\n",
      "1729: lr: 0.0100, train loss: 2.4465, val loss: 2.4859\n",
      "1730: lr: 0.0100, train loss: 2.4948, val loss: 2.5025\n",
      "1731: lr: 0.0100, train loss: 2.4795, val loss: 2.4496\n",
      "1732: lr: 0.0100, train loss: 2.4586, val loss: 2.5172\n",
      "1733: lr: 0.0100, train loss: 2.4490, val loss: 2.4686\n",
      "1734: lr: 0.0100, train loss: 2.4380, val loss: 2.4928\n",
      "1735: lr: 0.0100, train loss: 2.4341, val loss: 2.4821\n",
      "1736: lr: 0.0100, train loss: 2.4768, val loss: 2.5331\n",
      "1737: lr: 0.0100, train loss: 2.4854, val loss: 2.4929\n",
      "1738: lr: 0.0100, train loss: 2.4562, val loss: 2.4756\n",
      "1739: lr: 0.0100, train loss: 2.5056, val loss: 2.5133\n",
      "1740: lr: 0.0100, train loss: 2.4893, val loss: 2.4849\n",
      "1741: lr: 0.0100, train loss: 2.4946, val loss: 2.5256\n",
      "1742: lr: 0.0100, train loss: 2.4295, val loss: 2.5058\n",
      "1743: lr: 0.0100, train loss: 2.4263, val loss: 2.4810\n",
      "1744: lr: 0.0100, train loss: 2.4392, val loss: 2.5460\n",
      "1745: lr: 0.0100, train loss: 2.4321, val loss: 2.4967\n",
      "1746: lr: 0.0100, train loss: 2.4833, val loss: 2.5212\n",
      "1747: lr: 0.0100, train loss: 2.4673, val loss: 2.4640\n",
      "1748: lr: 0.0100, train loss: 2.4310, val loss: 2.4789\n",
      "1749: lr: 0.0100, train loss: 2.4633, val loss: 2.4700\n",
      "1750: lr: 0.0100, train loss: 2.4478, val loss: 2.4667\n",
      "1751: lr: 0.0100, train loss: 2.4487, val loss: 2.4798\n",
      "1752: lr: 0.0100, train loss: 2.4327, val loss: 2.4915\n",
      "1753: lr: 0.0100, train loss: 2.4777, val loss: 2.4790\n",
      "1754: lr: 0.0100, train loss: 2.4398, val loss: 2.5283\n",
      "1755: lr: 0.0100, train loss: 2.4793, val loss: 2.4858\n",
      "1756: lr: 0.0100, train loss: 2.4806, val loss: 2.5219\n",
      "1757: lr: 0.0100, train loss: 2.4233, val loss: 2.5223\n",
      "1758: lr: 0.0100, train loss: 2.5087, val loss: 2.5157\n",
      "1759: lr: 0.0100, train loss: 2.4563, val loss: 2.5040\n",
      "1760: lr: 0.0100, train loss: 2.4779, val loss: 2.4237\n",
      "1761: lr: 0.0100, train loss: 2.4794, val loss: 2.4908\n",
      "1762: lr: 0.0100, train loss: 2.4768, val loss: 2.4850\n",
      "1763: lr: 0.0100, train loss: 2.4178, val loss: 2.4615\n",
      "1764: lr: 0.0100, train loss: 2.4015, val loss: 2.4758\n",
      "1765: lr: 0.0100, train loss: 2.4591, val loss: 2.4128\n",
      "1766: lr: 0.0100, train loss: 2.4920, val loss: 2.4646\n",
      "1767: lr: 0.0100, train loss: 2.4430, val loss: 2.4773\n",
      "1768: lr: 0.0100, train loss: 2.4550, val loss: 2.5069\n",
      "1769: lr: 0.0100, train loss: 2.5313, val loss: 2.4768\n",
      "1770: lr: 0.0100, train loss: 2.5005, val loss: 2.5077\n",
      "1771: lr: 0.0100, train loss: 2.4452, val loss: 2.5262\n",
      "1772: lr: 0.0100, train loss: 2.4536, val loss: 2.4952\n",
      "1773: lr: 0.0100, train loss: 2.4183, val loss: 2.5061\n",
      "1774: lr: 0.0100, train loss: 2.4397, val loss: 2.4522\n",
      "1775: lr: 0.0100, train loss: 2.4536, val loss: 2.4428\n",
      "1776: lr: 0.0100, train loss: 2.4610, val loss: 2.5101\n",
      "1777: lr: 0.0100, train loss: 2.4936, val loss: 2.5369\n",
      "1778: lr: 0.0100, train loss: 2.4712, val loss: 2.4495\n",
      "1779: lr: 0.0100, train loss: 2.5131, val loss: 2.4932\n",
      "1780: lr: 0.0100, train loss: 2.4550, val loss: 2.5001\n",
      "1781: lr: 0.0100, train loss: 2.4674, val loss: 2.4428\n",
      "1782: lr: 0.0100, train loss: 2.4590, val loss: 2.5231\n",
      "1783: lr: 0.0100, train loss: 2.4918, val loss: 2.4858\n",
      "1784: lr: 0.0100, train loss: 2.4613, val loss: 2.4860\n",
      "1785: lr: 0.0100, train loss: 2.4871, val loss: 2.4977\n",
      "1786: lr: 0.0100, train loss: 2.4705, val loss: 2.4729\n",
      "1787: lr: 0.0100, train loss: 2.4685, val loss: 2.4675\n",
      "1788: lr: 0.0100, train loss: 2.3948, val loss: 2.5164\n",
      "1789: lr: 0.0100, train loss: 2.4468, val loss: 2.4678\n",
      "1790: lr: 0.0100, train loss: 2.4976, val loss: 2.4667\n",
      "1791: lr: 0.0100, train loss: 2.4256, val loss: 2.4404\n",
      "1792: lr: 0.0100, train loss: 2.4757, val loss: 2.5051\n",
      "1793: lr: 0.0100, train loss: 2.4490, val loss: 2.5025\n",
      "1794: lr: 0.0100, train loss: 2.4362, val loss: 2.5258\n",
      "1795: lr: 0.0100, train loss: 2.4411, val loss: 2.4653\n",
      "1796: lr: 0.0100, train loss: 2.4408, val loss: 2.4862\n",
      "1797: lr: 0.0100, train loss: 2.4793, val loss: 2.4820\n",
      "1798: lr: 0.0100, train loss: 2.4586, val loss: 2.5235\n",
      "1799: lr: 0.0100, train loss: 2.5155, val loss: 2.4745\n",
      "1800: lr: 0.0100, train loss: 2.4116, val loss: 2.5354\n",
      "1801: lr: 0.0100, train loss: 2.4567, val loss: 2.5094\n",
      "1802: lr: 0.0100, train loss: 2.4662, val loss: 2.5882\n",
      "1803: lr: 0.0100, train loss: 2.4640, val loss: 2.4829\n",
      "1804: lr: 0.0100, train loss: 2.4827, val loss: 2.4889\n",
      "1805: lr: 0.0100, train loss: 2.4428, val loss: 2.5028\n",
      "1806: lr: 0.0100, train loss: 2.4829, val loss: 2.4625\n",
      "1807: lr: 0.0100, train loss: 2.4471, val loss: 2.4590\n",
      "1808: lr: 0.0100, train loss: 2.4737, val loss: 2.5215\n",
      "1809: lr: 0.0100, train loss: 2.4176, val loss: 2.4874\n",
      "1810: lr: 0.0100, train loss: 2.4304, val loss: 2.4863\n",
      "1811: lr: 0.0100, train loss: 2.4796, val loss: 2.4992\n",
      "1812: lr: 0.0100, train loss: 2.5091, val loss: 2.5248\n",
      "1813: lr: 0.0100, train loss: 2.4577, val loss: 2.5074\n",
      "1814: lr: 0.0100, train loss: 2.4537, val loss: 2.4350\n",
      "1815: lr: 0.0100, train loss: 2.4966, val loss: 2.5083\n",
      "1816: lr: 0.0100, train loss: 2.4811, val loss: 2.4307\n",
      "1817: lr: 0.0100, train loss: 2.4417, val loss: 2.5119\n",
      "1818: lr: 0.0100, train loss: 2.4942, val loss: 2.4779\n",
      "1819: lr: 0.0100, train loss: 2.4382, val loss: 2.5316\n",
      "1820: lr: 0.0100, train loss: 2.4819, val loss: 2.4591\n",
      "1821: lr: 0.0100, train loss: 2.4464, val loss: 2.4243\n",
      "1822: lr: 0.0100, train loss: 2.4351, val loss: 2.4880\n",
      "1823: lr: 0.0100, train loss: 2.4677, val loss: 2.4683\n",
      "1824: lr: 0.0100, train loss: 2.4090, val loss: 2.5072\n",
      "1825: lr: 0.0100, train loss: 2.4423, val loss: 2.4508\n",
      "1826: lr: 0.0100, train loss: 2.4428, val loss: 2.4615\n",
      "1827: lr: 0.0100, train loss: 2.4443, val loss: 2.4810\n",
      "1828: lr: 0.0100, train loss: 2.4641, val loss: 2.5291\n",
      "1829: lr: 0.0100, train loss: 2.4406, val loss: 2.4959\n",
      "1830: lr: 0.0100, train loss: 2.4744, val loss: 2.4338\n",
      "1831: lr: 0.0100, train loss: 2.4405, val loss: 2.4310\n",
      "1832: lr: 0.0100, train loss: 2.4786, val loss: 2.4832\n",
      "1833: lr: 0.0100, train loss: 2.4624, val loss: 2.4555\n",
      "1834: lr: 0.0100, train loss: 2.4714, val loss: 2.4834\n",
      "1835: lr: 0.0100, train loss: 2.4570, val loss: 2.5165\n",
      "1836: lr: 0.0100, train loss: 2.4234, val loss: 2.4625\n",
      "1837: lr: 0.0100, train loss: 2.4619, val loss: 2.4801\n",
      "1838: lr: 0.0100, train loss: 2.4303, val loss: 2.4745\n",
      "1839: lr: 0.0100, train loss: 2.4383, val loss: 2.4829\n",
      "1840: lr: 0.0100, train loss: 2.4770, val loss: 2.5085\n",
      "1841: lr: 0.0100, train loss: 2.4526, val loss: 2.4371\n",
      "1842: lr: 0.0100, train loss: 2.4607, val loss: 2.4506\n",
      "1843: lr: 0.0100, train loss: 2.4201, val loss: 2.4770\n",
      "1844: lr: 0.0100, train loss: 2.4890, val loss: 2.4856\n",
      "1845: lr: 0.0100, train loss: 2.4607, val loss: 2.4343\n",
      "1846: lr: 0.0100, train loss: 2.4264, val loss: 2.5074\n",
      "1847: lr: 0.0100, train loss: 2.4548, val loss: 2.5620\n",
      "1848: lr: 0.0100, train loss: 2.4836, val loss: 2.4496\n",
      "1849: lr: 0.0100, train loss: 2.4816, val loss: 2.4551\n",
      "1850: lr: 0.0100, train loss: 2.4339, val loss: 2.4699\n",
      "1851: lr: 0.0100, train loss: 2.4941, val loss: 2.4858\n",
      "1852: lr: 0.0100, train loss: 2.5218, val loss: 2.5453\n",
      "1853: lr: 0.0100, train loss: 2.4451, val loss: 2.4615\n",
      "1854: lr: 0.0100, train loss: 2.4634, val loss: 2.4714\n",
      "1855: lr: 0.0100, train loss: 2.4725, val loss: 2.4937\n",
      "1856: lr: 0.0100, train loss: 2.4672, val loss: 2.4534\n",
      "1857: lr: 0.0100, train loss: 2.4129, val loss: 2.4767\n",
      "1858: lr: 0.0100, train loss: 2.4653, val loss: 2.4615\n",
      "1859: lr: 0.0100, train loss: 2.4349, val loss: 2.4946\n",
      "1860: lr: 0.0100, train loss: 2.4652, val loss: 2.4542\n",
      "1861: lr: 0.0100, train loss: 2.4565, val loss: 2.4913\n",
      "1862: lr: 0.0100, train loss: 2.4476, val loss: 2.4745\n",
      "1863: lr: 0.0100, train loss: 2.4196, val loss: 2.5372\n",
      "1864: lr: 0.0100, train loss: 2.3986, val loss: 2.5270\n",
      "1865: lr: 0.0100, train loss: 2.4736, val loss: 2.5085\n",
      "1866: lr: 0.0100, train loss: 2.4930, val loss: 2.4824\n",
      "1867: lr: 0.0100, train loss: 2.4634, val loss: 2.4587\n",
      "1868: lr: 0.0100, train loss: 2.4460, val loss: 2.4664\n",
      "1869: lr: 0.0100, train loss: 2.4390, val loss: 2.5154\n",
      "1870: lr: 0.0100, train loss: 2.4466, val loss: 2.4702\n",
      "1871: lr: 0.0100, train loss: 2.5101, val loss: 2.4747\n",
      "1872: lr: 0.0100, train loss: 2.4716, val loss: 2.5036\n",
      "1873: lr: 0.0100, train loss: 2.4579, val loss: 2.4245\n",
      "1874: lr: 0.0100, train loss: 2.4731, val loss: 2.4738\n",
      "1875: lr: 0.0100, train loss: 2.4636, val loss: 2.5431\n",
      "1876: lr: 0.0100, train loss: 2.4745, val loss: 2.4725\n",
      "1877: lr: 0.0100, train loss: 2.4508, val loss: 2.4644\n",
      "1878: lr: 0.0100, train loss: 2.4205, val loss: 2.4663\n",
      "1879: lr: 0.0100, train loss: 2.4863, val loss: 2.4344\n",
      "1880: lr: 0.0100, train loss: 2.4464, val loss: 2.4682\n",
      "1881: lr: 0.0100, train loss: 2.5044, val loss: 2.5109\n",
      "1882: lr: 0.0100, train loss: 2.4233, val loss: 2.4642\n",
      "1883: lr: 0.0100, train loss: 2.4614, val loss: 2.5319\n",
      "1884: lr: 0.0100, train loss: 2.4909, val loss: 2.4942\n",
      "1885: lr: 0.0100, train loss: 2.4526, val loss: 2.4594\n",
      "1886: lr: 0.0100, train loss: 2.4713, val loss: 2.4832\n",
      "1887: lr: 0.0100, train loss: 2.4541, val loss: 2.4832\n",
      "1888: lr: 0.0100, train loss: 2.4319, val loss: 2.4669\n",
      "1889: lr: 0.0100, train loss: 2.4666, val loss: 2.4933\n",
      "1890: lr: 0.0100, train loss: 2.4579, val loss: 2.4688\n",
      "1891: lr: 0.0100, train loss: 2.4890, val loss: 2.4837\n",
      "1892: lr: 0.0100, train loss: 2.4671, val loss: 2.4861\n",
      "1893: lr: 0.0100, train loss: 2.4618, val loss: 2.4815\n",
      "1894: lr: 0.0100, train loss: 2.4724, val loss: 2.4996\n",
      "1895: lr: 0.0100, train loss: 2.4384, val loss: 2.4471\n",
      "1896: lr: 0.0100, train loss: 2.4349, val loss: 2.5298\n",
      "1897: lr: 0.0100, train loss: 2.4815, val loss: 2.4841\n",
      "1898: lr: 0.0100, train loss: 2.4366, val loss: 2.5060\n",
      "1899: lr: 0.0100, train loss: 2.4780, val loss: 2.4883\n",
      "1900: lr: 0.0100, train loss: 2.4492, val loss: 2.4805\n",
      "1901: lr: 0.0100, train loss: 2.4431, val loss: 2.4357\n",
      "1902: lr: 0.0100, train loss: 2.4832, val loss: 2.4997\n",
      "1903: lr: 0.0100, train loss: 2.4708, val loss: 2.5218\n",
      "1904: lr: 0.0100, train loss: 2.4732, val loss: 2.4843\n",
      "1905: lr: 0.0100, train loss: 2.5008, val loss: 2.4752\n",
      "1906: lr: 0.0100, train loss: 2.4513, val loss: 2.4743\n",
      "1907: lr: 0.0100, train loss: 2.5100, val loss: 2.5040\n",
      "1908: lr: 0.0100, train loss: 2.4788, val loss: 2.4309\n",
      "1909: lr: 0.0100, train loss: 2.4617, val loss: 2.4706\n",
      "1910: lr: 0.0100, train loss: 2.4703, val loss: 2.4614\n",
      "1911: lr: 0.0100, train loss: 2.4737, val loss: 2.4934\n",
      "1912: lr: 0.0100, train loss: 2.4552, val loss: 2.4527\n",
      "1913: lr: 0.0100, train loss: 2.4245, val loss: 2.4828\n",
      "1914: lr: 0.0100, train loss: 2.4946, val loss: 2.4717\n",
      "1915: lr: 0.0100, train loss: 2.4474, val loss: 2.5075\n",
      "1916: lr: 0.0100, train loss: 2.4702, val loss: 2.5378\n",
      "1917: lr: 0.0100, train loss: 2.4590, val loss: 2.4984\n",
      "1918: lr: 0.0100, train loss: 2.4608, val loss: 2.4855\n",
      "1919: lr: 0.0100, train loss: 2.3955, val loss: 2.4689\n",
      "1920: lr: 0.0100, train loss: 2.4554, val loss: 2.4434\n",
      "1921: lr: 0.0100, train loss: 2.4539, val loss: 2.5205\n",
      "1922: lr: 0.0100, train loss: 2.4981, val loss: 2.4548\n",
      "1923: lr: 0.0100, train loss: 2.4588, val loss: 2.5275\n",
      "1924: lr: 0.0100, train loss: 2.4329, val loss: 2.4754\n",
      "1925: lr: 0.0100, train loss: 2.4779, val loss: 2.4878\n",
      "1926: lr: 0.0100, train loss: 2.4569, val loss: 2.5219\n",
      "1927: lr: 0.0100, train loss: 2.4586, val loss: 2.5039\n",
      "1928: lr: 0.0100, train loss: 2.4484, val loss: 2.4697\n",
      "1929: lr: 0.0100, train loss: 2.4844, val loss: 2.4833\n",
      "1930: lr: 0.0100, train loss: 2.4307, val loss: 2.4532\n",
      "1931: lr: 0.0100, train loss: 2.4984, val loss: 2.4444\n",
      "1932: lr: 0.0100, train loss: 2.4637, val loss: 2.5107\n",
      "1933: lr: 0.0100, train loss: 2.5047, val loss: 2.4763\n",
      "1934: lr: 0.0100, train loss: 2.4286, val loss: 2.4294\n",
      "1935: lr: 0.0100, train loss: 2.4783, val loss: 2.4982\n",
      "1936: lr: 0.0100, train loss: 2.4308, val loss: 2.4572\n",
      "1937: lr: 0.0100, train loss: 2.4884, val loss: 2.4968\n",
      "1938: lr: 0.0100, train loss: 2.4152, val loss: 2.4785\n",
      "1939: lr: 0.0100, train loss: 2.4698, val loss: 2.4521\n",
      "1940: lr: 0.0100, train loss: 2.5046, val loss: 2.4742\n",
      "1941: lr: 0.0100, train loss: 2.4824, val loss: 2.5325\n",
      "1942: lr: 0.0100, train loss: 2.4595, val loss: 2.4970\n",
      "1943: lr: 0.0100, train loss: 2.4323, val loss: 2.4732\n",
      "1944: lr: 0.0100, train loss: 2.5207, val loss: 2.5132\n",
      "1945: lr: 0.0100, train loss: 2.4540, val loss: 2.5086\n",
      "1946: lr: 0.0100, train loss: 2.4949, val loss: 2.4806\n",
      "1947: lr: 0.0100, train loss: 2.4647, val loss: 2.5224\n",
      "1948: lr: 0.0100, train loss: 2.4704, val loss: 2.5746\n",
      "1949: lr: 0.0100, train loss: 2.4170, val loss: 2.4590\n",
      "1950: lr: 0.0100, train loss: 2.4803, val loss: 2.4324\n",
      "1951: lr: 0.0100, train loss: 2.4703, val loss: 2.5022\n",
      "1952: lr: 0.0100, train loss: 2.4467, val loss: 2.4666\n",
      "1953: lr: 0.0100, train loss: 2.4628, val loss: 2.5185\n",
      "1954: lr: 0.0100, train loss: 2.4503, val loss: 2.4863\n",
      "1955: lr: 0.0100, train loss: 2.4775, val loss: 2.5111\n",
      "1956: lr: 0.0100, train loss: 2.4279, val loss: 2.4773\n",
      "1957: lr: 0.0100, train loss: 2.5501, val loss: 2.4958\n",
      "1958: lr: 0.0100, train loss: 2.4810, val loss: 2.4450\n",
      "1959: lr: 0.0100, train loss: 2.4636, val loss: 2.4585\n",
      "1960: lr: 0.0100, train loss: 2.5139, val loss: 2.4706\n",
      "1961: lr: 0.0100, train loss: 2.4286, val loss: 2.4350\n",
      "1962: lr: 0.0100, train loss: 2.4440, val loss: 2.5226\n",
      "1963: lr: 0.0100, train loss: 2.4912, val loss: 2.4578\n",
      "1964: lr: 0.0100, train loss: 2.4534, val loss: 2.4786\n",
      "1965: lr: 0.0100, train loss: 2.4839, val loss: 2.5134\n",
      "1966: lr: 0.0100, train loss: 2.4714, val loss: 2.4966\n",
      "1967: lr: 0.0100, train loss: 2.4309, val loss: 2.5093\n",
      "1968: lr: 0.0100, train loss: 2.4861, val loss: 2.5010\n",
      "1969: lr: 0.0100, train loss: 2.4446, val loss: 2.4994\n",
      "1970: lr: 0.0100, train loss: 2.4503, val loss: 2.4769\n",
      "1971: lr: 0.0100, train loss: 2.4292, val loss: 2.5255\n",
      "1972: lr: 0.0100, train loss: 2.4614, val loss: 2.4681\n",
      "1973: lr: 0.0100, train loss: 2.4641, val loss: 2.4531\n",
      "1974: lr: 0.0100, train loss: 2.4219, val loss: 2.4959\n",
      "1975: lr: 0.0100, train loss: 2.4578, val loss: 2.5712\n",
      "1976: lr: 0.0100, train loss: 2.4604, val loss: 2.5391\n",
      "1977: lr: 0.0100, train loss: 2.4333, val loss: 2.4961\n",
      "1978: lr: 0.0100, train loss: 2.4673, val loss: 2.5186\n",
      "1979: lr: 0.0100, train loss: 2.4668, val loss: 2.5043\n",
      "1980: lr: 0.0100, train loss: 2.4491, val loss: 2.4950\n",
      "1981: lr: 0.0100, train loss: 2.4653, val loss: 2.4792\n",
      "1982: lr: 0.0100, train loss: 2.4463, val loss: 2.5439\n",
      "1983: lr: 0.0100, train loss: 2.4481, val loss: 2.4758\n",
      "1984: lr: 0.0100, train loss: 2.4603, val loss: 2.5066\n",
      "1985: lr: 0.0100, train loss: 2.4921, val loss: 2.5008\n",
      "1986: lr: 0.0100, train loss: 2.4620, val loss: 2.4691\n",
      "1987: lr: 0.0100, train loss: 2.4622, val loss: 2.4896\n",
      "1988: lr: 0.0100, train loss: 2.4667, val loss: 2.5592\n",
      "1989: lr: 0.0100, train loss: 2.4876, val loss: 2.4801\n",
      "1990: lr: 0.0100, train loss: 2.4902, val loss: 2.5055\n",
      "1991: lr: 0.0100, train loss: 2.4527, val loss: 2.4809\n",
      "1992: lr: 0.0100, train loss: 2.4594, val loss: 2.4901\n",
      "1993: lr: 0.0100, train loss: 2.4501, val loss: 2.4496\n",
      "1994: lr: 0.0100, train loss: 2.4661, val loss: 2.4384\n",
      "1995: lr: 0.0100, train loss: 2.4243, val loss: 2.4377\n",
      "1996: lr: 0.0100, train loss: 2.4677, val loss: 2.4330\n",
      "1997: lr: 0.0100, train loss: 2.4789, val loss: 2.5166\n",
      "1998: lr: 0.0100, train loss: 2.4394, val loss: 2.4909\n",
      "1999: lr: 0.0100, train loss: 2.4023, val loss: 2.5136\n",
      "2000: lr: 0.0100, train loss: 2.4441, val loss: 2.5138\n",
      "2001: lr: 0.0100, train loss: 2.4062, val loss: 2.4804\n",
      "2002: lr: 0.0100, train loss: 2.4898, val loss: 2.4589\n",
      "2003: lr: 0.0100, train loss: 2.4659, val loss: 2.4744\n",
      "2004: lr: 0.0100, train loss: 2.4610, val loss: 2.5360\n",
      "2005: lr: 0.0100, train loss: 2.4329, val loss: 2.5396\n",
      "2006: lr: 0.0100, train loss: 2.4572, val loss: 2.4683\n",
      "2007: lr: 0.0100, train loss: 2.4471, val loss: 2.4462\n",
      "2008: lr: 0.0100, train loss: 2.4388, val loss: 2.4669\n",
      "2009: lr: 0.0100, train loss: 2.4750, val loss: 2.5122\n",
      "2010: lr: 0.0100, train loss: 2.4488, val loss: 2.4809\n",
      "2011: lr: 0.0100, train loss: 2.4278, val loss: 2.4934\n",
      "2012: lr: 0.0100, train loss: 2.4834, val loss: 2.4650\n",
      "2013: lr: 0.0100, train loss: 2.4675, val loss: 2.5078\n",
      "2014: lr: 0.0100, train loss: 2.4556, val loss: 2.4645\n",
      "2015: lr: 0.0100, train loss: 2.4350, val loss: 2.5019\n",
      "2016: lr: 0.0100, train loss: 2.4884, val loss: 2.4968\n",
      "2017: lr: 0.0100, train loss: 2.4568, val loss: 2.5052\n",
      "2018: lr: 0.0100, train loss: 2.4744, val loss: 2.5060\n",
      "2019: lr: 0.0100, train loss: 2.4948, val loss: 2.4991\n",
      "2020: lr: 0.0100, train loss: 2.4825, val loss: 2.4957\n",
      "2021: lr: 0.0100, train loss: 2.4842, val loss: 2.5297\n",
      "2022: lr: 0.0100, train loss: 2.4768, val loss: 2.5185\n",
      "2023: lr: 0.0100, train loss: 2.5125, val loss: 2.5355\n",
      "2024: lr: 0.0100, train loss: 2.4602, val loss: 2.4479\n",
      "2025: lr: 0.0100, train loss: 2.4466, val loss: 2.4703\n",
      "2026: lr: 0.0100, train loss: 2.4343, val loss: 2.4895\n",
      "2027: lr: 0.0100, train loss: 2.4530, val loss: 2.4600\n",
      "2028: lr: 0.0100, train loss: 2.4685, val loss: 2.4332\n",
      "2029: lr: 0.0100, train loss: 2.4674, val loss: 2.5282\n",
      "2030: lr: 0.0100, train loss: 2.4869, val loss: 2.4013\n",
      "2031: lr: 0.0100, train loss: 2.4795, val loss: 2.5045\n",
      "2032: lr: 0.0100, train loss: 2.4234, val loss: 2.5158\n",
      "2033: lr: 0.0100, train loss: 2.4840, val loss: 2.4554\n",
      "2034: lr: 0.0100, train loss: 2.4404, val loss: 2.5469\n",
      "2035: lr: 0.0100, train loss: 2.4241, val loss: 2.4778\n",
      "2036: lr: 0.0100, train loss: 2.4591, val loss: 2.4172\n",
      "2037: lr: 0.0100, train loss: 2.4329, val loss: 2.4673\n",
      "2038: lr: 0.0100, train loss: 2.4849, val loss: 2.4726\n",
      "2039: lr: 0.0100, train loss: 2.4998, val loss: 2.4824\n",
      "2040: lr: 0.0100, train loss: 2.5004, val loss: 2.4275\n",
      "2041: lr: 0.0100, train loss: 2.4992, val loss: 2.4830\n",
      "2042: lr: 0.0100, train loss: 2.4590, val loss: 2.4707\n",
      "2043: lr: 0.0100, train loss: 2.4201, val loss: 2.5152\n",
      "2044: lr: 0.0100, train loss: 2.5000, val loss: 2.5101\n",
      "2045: lr: 0.0100, train loss: 2.4467, val loss: 2.4926\n",
      "2046: lr: 0.0100, train loss: 2.4498, val loss: 2.4681\n",
      "2047: lr: 0.0100, train loss: 2.4460, val loss: 2.4322\n",
      "2048: lr: 0.0100, train loss: 2.4453, val loss: 2.4840\n",
      "2049: lr: 0.0100, train loss: 2.4183, val loss: 2.4389\n",
      "2050: lr: 0.0100, train loss: 2.4734, val loss: 2.4408\n",
      "2051: lr: 0.0100, train loss: 2.4500, val loss: 2.4780\n",
      "2052: lr: 0.0100, train loss: 2.4627, val loss: 2.4835\n",
      "2053: lr: 0.0100, train loss: 2.4459, val loss: 2.5212\n",
      "2054: lr: 0.0100, train loss: 2.4884, val loss: 2.4859\n",
      "2055: lr: 0.0100, train loss: 2.4641, val loss: 2.4894\n",
      "2056: lr: 0.0100, train loss: 2.4611, val loss: 2.4844\n",
      "2057: lr: 0.0100, train loss: 2.4037, val loss: 2.5006\n",
      "2058: lr: 0.0100, train loss: 2.4984, val loss: 2.5204\n",
      "2059: lr: 0.0100, train loss: 2.4501, val loss: 2.4688\n",
      "2060: lr: 0.0100, train loss: 2.4267, val loss: 2.4441\n",
      "2061: lr: 0.0100, train loss: 2.4765, val loss: 2.4880\n",
      "2062: lr: 0.0100, train loss: 2.4712, val loss: 2.4938\n",
      "2063: lr: 0.0100, train loss: 2.4935, val loss: 2.4522\n",
      "2064: lr: 0.0100, train loss: 2.4798, val loss: 2.5365\n",
      "2065: lr: 0.0100, train loss: 2.4197, val loss: 2.4916\n",
      "2066: lr: 0.0100, train loss: 2.4681, val loss: 2.4924\n",
      "2067: lr: 0.0100, train loss: 2.4590, val loss: 2.4973\n",
      "2068: lr: 0.0100, train loss: 2.5006, val loss: 2.5027\n",
      "2069: lr: 0.0100, train loss: 2.4642, val loss: 2.5104\n",
      "2070: lr: 0.0100, train loss: 2.4400, val loss: 2.5153\n",
      "2071: lr: 0.0100, train loss: 2.4958, val loss: 2.5125\n",
      "2072: lr: 0.0100, train loss: 2.4734, val loss: 2.4764\n",
      "2073: lr: 0.0100, train loss: 2.4743, val loss: 2.4572\n",
      "2074: lr: 0.0100, train loss: 2.4616, val loss: 2.5308\n",
      "2075: lr: 0.0100, train loss: 2.4995, val loss: 2.4439\n",
      "2076: lr: 0.0100, train loss: 2.4612, val loss: 2.4777\n",
      "2077: lr: 0.0100, train loss: 2.4297, val loss: 2.4706\n",
      "2078: lr: 0.0100, train loss: 2.4345, val loss: 2.4724\n",
      "2079: lr: 0.0100, train loss: 2.4289, val loss: 2.4706\n",
      "2080: lr: 0.0100, train loss: 2.4673, val loss: 2.5054\n",
      "2081: lr: 0.0100, train loss: 2.4197, val loss: 2.5023\n",
      "2082: lr: 0.0100, train loss: 2.4336, val loss: 2.4989\n",
      "2083: lr: 0.0100, train loss: 2.4851, val loss: 2.4695\n",
      "2084: lr: 0.0100, train loss: 2.4851, val loss: 2.5152\n",
      "2085: lr: 0.0100, train loss: 2.4730, val loss: 2.4764\n",
      "2086: lr: 0.0100, train loss: 2.4789, val loss: 2.4681\n",
      "2087: lr: 0.0100, train loss: 2.4498, val loss: 2.4742\n",
      "2088: lr: 0.0100, train loss: 2.5011, val loss: 2.4980\n",
      "2089: lr: 0.0100, train loss: 2.4624, val loss: 2.4578\n",
      "2090: lr: 0.0100, train loss: 2.4876, val loss: 2.5135\n",
      "2091: lr: 0.0100, train loss: 2.4518, val loss: 2.4673\n",
      "2092: lr: 0.0100, train loss: 2.4546, val loss: 2.4547\n",
      "2093: lr: 0.0100, train loss: 2.4305, val loss: 2.4621\n",
      "2094: lr: 0.0100, train loss: 2.4446, val loss: 2.5287\n",
      "2095: lr: 0.0100, train loss: 2.4946, val loss: 2.4841\n",
      "2096: lr: 0.0100, train loss: 2.4643, val loss: 2.4680\n",
      "2097: lr: 0.0100, train loss: 2.4514, val loss: 2.4814\n",
      "2098: lr: 0.0100, train loss: 2.4442, val loss: 2.4631\n",
      "2099: lr: 0.0100, train loss: 2.4877, val loss: 2.4583\n",
      "2100: lr: 0.0100, train loss: 2.4308, val loss: 2.4544\n",
      "2101: lr: 0.0100, train loss: 2.4886, val loss: 2.4595\n",
      "2102: lr: 0.0100, train loss: 2.4997, val loss: 2.4546\n",
      "2103: lr: 0.0100, train loss: 2.4443, val loss: 2.5556\n",
      "2104: lr: 0.0100, train loss: 2.4578, val loss: 2.4747\n",
      "2105: lr: 0.0100, train loss: 2.5015, val loss: 2.5488\n",
      "2106: lr: 0.0100, train loss: 2.5120, val loss: 2.4918\n",
      "2107: lr: 0.0100, train loss: 2.4597, val loss: 2.5505\n",
      "2108: lr: 0.0100, train loss: 2.4741, val loss: 2.5302\n",
      "2109: lr: 0.0100, train loss: 2.4345, val loss: 2.5273\n",
      "2110: lr: 0.0100, train loss: 2.4643, val loss: 2.4958\n",
      "2111: lr: 0.0100, train loss: 2.4474, val loss: 2.4526\n",
      "2112: lr: 0.0100, train loss: 2.4561, val loss: 2.4795\n",
      "2113: lr: 0.0100, train loss: 2.4686, val loss: 2.5482\n",
      "2114: lr: 0.0100, train loss: 2.4556, val loss: 2.5118\n",
      "2115: lr: 0.0100, train loss: 2.4593, val loss: 2.4951\n",
      "2116: lr: 0.0100, train loss: 2.4303, val loss: 2.5079\n",
      "2117: lr: 0.0100, train loss: 2.4666, val loss: 2.5014\n",
      "2118: lr: 0.0100, train loss: 2.4654, val loss: 2.4826\n",
      "2119: lr: 0.0100, train loss: 2.4186, val loss: 2.4831\n",
      "2120: lr: 0.0100, train loss: 2.4615, val loss: 2.5010\n",
      "2121: lr: 0.0100, train loss: 2.4608, val loss: 2.5119\n",
      "2122: lr: 0.0100, train loss: 2.4246, val loss: 2.4973\n",
      "2123: lr: 0.0100, train loss: 2.4804, val loss: 2.4813\n",
      "2124: lr: 0.0100, train loss: 2.4560, val loss: 2.5184\n",
      "2125: lr: 0.0100, train loss: 2.4597, val loss: 2.4605\n",
      "2126: lr: 0.0100, train loss: 2.4508, val loss: 2.5360\n",
      "2127: lr: 0.0100, train loss: 2.4537, val loss: 2.5115\n",
      "2128: lr: 0.0100, train loss: 2.4559, val loss: 2.4862\n",
      "2129: lr: 0.0100, train loss: 2.4154, val loss: 2.4924\n",
      "2130: lr: 0.0100, train loss: 2.4598, val loss: 2.4667\n",
      "2131: lr: 0.0100, train loss: 2.4358, val loss: 2.4795\n",
      "2132: lr: 0.0100, train loss: 2.4364, val loss: 2.4514\n",
      "2133: lr: 0.0100, train loss: 2.4788, val loss: 2.5012\n",
      "2134: lr: 0.0100, train loss: 2.4389, val loss: 2.5297\n",
      "2135: lr: 0.0100, train loss: 2.4551, val loss: 2.4622\n",
      "2136: lr: 0.0100, train loss: 2.4324, val loss: 2.4417\n",
      "2137: lr: 0.0100, train loss: 2.4606, val loss: 2.4498\n",
      "2138: lr: 0.0100, train loss: 2.4505, val loss: 2.4677\n",
      "2139: lr: 0.0100, train loss: 2.4526, val loss: 2.4627\n",
      "2140: lr: 0.0100, train loss: 2.4467, val loss: 2.5392\n",
      "2141: lr: 0.0100, train loss: 2.4846, val loss: 2.5045\n",
      "2142: lr: 0.0100, train loss: 2.4338, val loss: 2.5499\n",
      "2143: lr: 0.0100, train loss: 2.4344, val loss: 2.3816\n",
      "2144: lr: 0.0100, train loss: 2.4769, val loss: 2.5050\n",
      "2145: lr: 0.0100, train loss: 2.4632, val loss: 2.4618\n",
      "2146: lr: 0.0100, train loss: 2.4500, val loss: 2.5115\n",
      "2147: lr: 0.0100, train loss: 2.4653, val loss: 2.5033\n",
      "2148: lr: 0.0100, train loss: 2.4160, val loss: 2.5387\n",
      "2149: lr: 0.0100, train loss: 2.4592, val loss: 2.4857\n",
      "2150: lr: 0.0100, train loss: 2.4591, val loss: 2.4987\n",
      "2151: lr: 0.0100, train loss: 2.4255, val loss: 2.5068\n",
      "2152: lr: 0.0100, train loss: 2.4716, val loss: 2.4987\n",
      "2153: lr: 0.0100, train loss: 2.4382, val loss: 2.4719\n",
      "2154: lr: 0.0100, train loss: 2.4238, val loss: 2.5207\n",
      "2155: lr: 0.0100, train loss: 2.4437, val loss: 2.4892\n",
      "2156: lr: 0.0100, train loss: 2.4439, val loss: 2.4479\n",
      "2157: lr: 0.0100, train loss: 2.4490, val loss: 2.4830\n",
      "2158: lr: 0.0100, train loss: 2.4997, val loss: 2.5373\n",
      "2159: lr: 0.0100, train loss: 2.4663, val loss: 2.4906\n",
      "2160: lr: 0.0100, train loss: 2.4617, val loss: 2.5461\n",
      "2161: lr: 0.0100, train loss: 2.4682, val loss: 2.4879\n",
      "2162: lr: 0.0100, train loss: 2.4260, val loss: 2.4885\n",
      "2163: lr: 0.0100, train loss: 2.4253, val loss: 2.4398\n",
      "2164: lr: 0.0100, train loss: 2.4312, val loss: 2.4813\n",
      "2165: lr: 0.0100, train loss: 2.4235, val loss: 2.4466\n",
      "2166: lr: 0.0100, train loss: 2.4593, val loss: 2.5032\n",
      "2167: lr: 0.0100, train loss: 2.4739, val loss: 2.4493\n",
      "2168: lr: 0.0100, train loss: 2.4504, val loss: 2.5037\n",
      "2169: lr: 0.0100, train loss: 2.4363, val loss: 2.4910\n",
      "2170: lr: 0.0100, train loss: 2.4651, val loss: 2.5058\n",
      "2171: lr: 0.0100, train loss: 2.4792, val loss: 2.4793\n",
      "2172: lr: 0.0100, train loss: 2.4612, val loss: 2.5143\n",
      "2173: lr: 0.0100, train loss: 2.4362, val loss: 2.4857\n",
      "2174: lr: 0.0100, train loss: 2.4108, val loss: 2.4664\n",
      "2175: lr: 0.0100, train loss: 2.4285, val loss: 2.4406\n",
      "2176: lr: 0.0100, train loss: 2.4586, val loss: 2.4397\n",
      "2177: lr: 0.0100, train loss: 2.4776, val loss: 2.4666\n",
      "2178: lr: 0.0100, train loss: 2.5311, val loss: 2.4636\n",
      "2179: lr: 0.0100, train loss: 2.4773, val loss: 2.4291\n",
      "2180: lr: 0.0100, train loss: 2.5043, val loss: 2.4907\n",
      "2181: lr: 0.0100, train loss: 2.5028, val loss: 2.4759\n",
      "2182: lr: 0.0100, train loss: 2.4574, val loss: 2.4795\n",
      "2183: lr: 0.0100, train loss: 2.4461, val loss: 2.4950\n",
      "2184: lr: 0.0100, train loss: 2.4444, val loss: 2.5343\n",
      "2185: lr: 0.0100, train loss: 2.4350, val loss: 2.4457\n",
      "2186: lr: 0.0100, train loss: 2.4381, val loss: 2.4409\n",
      "2187: lr: 0.0100, train loss: 2.4655, val loss: 2.4522\n",
      "2188: lr: 0.0100, train loss: 2.4516, val loss: 2.5055\n",
      "2189: lr: 0.0100, train loss: 2.4793, val loss: 2.4825\n",
      "2190: lr: 0.0100, train loss: 2.4631, val loss: 2.5231\n",
      "2191: lr: 0.0100, train loss: 2.4783, val loss: 2.4801\n",
      "2192: lr: 0.0100, train loss: 2.4363, val loss: 2.5403\n",
      "2193: lr: 0.0100, train loss: 2.4185, val loss: 2.4718\n",
      "2194: lr: 0.0100, train loss: 2.5014, val loss: 2.4729\n",
      "2195: lr: 0.0100, train loss: 2.4285, val loss: 2.4575\n",
      "2196: lr: 0.0100, train loss: 2.4369, val loss: 2.5179\n",
      "2197: lr: 0.0100, train loss: 2.4922, val loss: 2.4736\n",
      "2198: lr: 0.0100, train loss: 2.4350, val loss: 2.4820\n",
      "2199: lr: 0.0100, train loss: 2.4682, val loss: 2.4975\n",
      "2200: lr: 0.0100, train loss: 2.5047, val loss: 2.4417\n",
      "2201: lr: 0.0100, train loss: 2.5099, val loss: 2.4248\n",
      "2202: lr: 0.0100, train loss: 2.4406, val loss: 2.5128\n",
      "2203: lr: 0.0100, train loss: 2.4641, val loss: 2.5011\n",
      "2204: lr: 0.0100, train loss: 2.4445, val loss: 2.4679\n",
      "2205: lr: 0.0100, train loss: 2.4446, val loss: 2.4579\n",
      "2206: lr: 0.0100, train loss: 2.4328, val loss: 2.4282\n",
      "2207: lr: 0.0100, train loss: 2.4620, val loss: 2.5049\n",
      "2208: lr: 0.0100, train loss: 2.4230, val loss: 2.5038\n",
      "2209: lr: 0.0100, train loss: 2.4756, val loss: 2.5131\n",
      "2210: lr: 0.0100, train loss: 2.4156, val loss: 2.4921\n",
      "2211: lr: 0.0100, train loss: 2.4363, val loss: 2.4730\n",
      "2212: lr: 0.0100, train loss: 2.4366, val loss: 2.4388\n",
      "2213: lr: 0.0100, train loss: 2.4007, val loss: 2.5194\n",
      "2214: lr: 0.0100, train loss: 2.3997, val loss: 2.4955\n",
      "2215: lr: 0.0100, train loss: 2.4997, val loss: 2.5043\n",
      "2216: lr: 0.0100, train loss: 2.4729, val loss: 2.4746\n",
      "2217: lr: 0.0100, train loss: 2.4416, val loss: 2.5144\n",
      "2218: lr: 0.0100, train loss: 2.4389, val loss: 2.5466\n",
      "2219: lr: 0.0100, train loss: 2.5064, val loss: 2.4575\n",
      "2220: lr: 0.0100, train loss: 2.4550, val loss: 2.4334\n",
      "2221: lr: 0.0100, train loss: 2.4793, val loss: 2.4965\n",
      "2222: lr: 0.0100, train loss: 2.4603, val loss: 2.5367\n",
      "2223: lr: 0.0100, train loss: 2.5153, val loss: 2.4853\n",
      "2224: lr: 0.0100, train loss: 2.4683, val loss: 2.4675\n",
      "2225: lr: 0.0100, train loss: 2.4914, val loss: 2.4366\n",
      "2226: lr: 0.0100, train loss: 2.4305, val loss: 2.5056\n",
      "2227: lr: 0.0100, train loss: 2.4486, val loss: 2.4775\n",
      "2228: lr: 0.0100, train loss: 2.4950, val loss: 2.5201\n",
      "2229: lr: 0.0100, train loss: 2.4700, val loss: 2.4819\n",
      "2230: lr: 0.0100, train loss: 2.4877, val loss: 2.4643\n",
      "2231: lr: 0.0100, train loss: 2.4795, val loss: 2.4540\n",
      "2232: lr: 0.0100, train loss: 2.4740, val loss: 2.4730\n",
      "2233: lr: 0.0100, train loss: 2.4583, val loss: 2.4313\n",
      "2234: lr: 0.0100, train loss: 2.4666, val loss: 2.5163\n",
      "2235: lr: 0.0100, train loss: 2.4793, val loss: 2.5369\n",
      "2236: lr: 0.0100, train loss: 2.4676, val loss: 2.4852\n",
      "2237: lr: 0.0100, train loss: 2.4307, val loss: 2.4650\n",
      "2238: lr: 0.0100, train loss: 2.4692, val loss: 2.5246\n",
      "2239: lr: 0.0100, train loss: 2.4497, val loss: 2.4867\n",
      "2240: lr: 0.0100, train loss: 2.4280, val loss: 2.4989\n",
      "2241: lr: 0.0100, train loss: 2.4630, val loss: 2.4946\n",
      "2242: lr: 0.0100, train loss: 2.4725, val loss: 2.5078\n",
      "2243: lr: 0.0100, train loss: 2.4941, val loss: 2.4793\n",
      "2244: lr: 0.0100, train loss: 2.5114, val loss: 2.5049\n",
      "2245: lr: 0.0100, train loss: 2.4536, val loss: 2.4449\n",
      "2246: lr: 0.0100, train loss: 2.4686, val loss: 2.4466\n",
      "2247: lr: 0.0100, train loss: 2.4961, val loss: 2.5009\n",
      "2248: lr: 0.0100, train loss: 2.4905, val loss: 2.4557\n",
      "2249: lr: 0.0100, train loss: 2.4618, val loss: 2.4775\n",
      "2250: lr: 0.0100, train loss: 2.4919, val loss: 2.5057\n",
      "2251: lr: 0.0100, train loss: 2.4711, val loss: 2.5204\n",
      "2252: lr: 0.0100, train loss: 2.4104, val loss: 2.4817\n",
      "2253: lr: 0.0100, train loss: 2.4723, val loss: 2.5039\n",
      "2254: lr: 0.0100, train loss: 2.4148, val loss: 2.4755\n",
      "2255: lr: 0.0100, train loss: 2.4782, val loss: 2.4695\n",
      "2256: lr: 0.0100, train loss: 2.4796, val loss: 2.4926\n",
      "2257: lr: 0.0100, train loss: 2.4246, val loss: 2.4835\n",
      "2258: lr: 0.0100, train loss: 2.4966, val loss: 2.4727\n",
      "2259: lr: 0.0100, train loss: 2.4921, val loss: 2.5126\n",
      "2260: lr: 0.0100, train loss: 2.4947, val loss: 2.4800\n",
      "2261: lr: 0.0100, train loss: 2.4326, val loss: 2.4828\n",
      "2262: lr: 0.0100, train loss: 2.4431, val loss: 2.4747\n",
      "2263: lr: 0.0100, train loss: 2.4293, val loss: 2.4992\n",
      "2264: lr: 0.0100, train loss: 2.4271, val loss: 2.4483\n",
      "2265: lr: 0.0100, train loss: 2.4675, val loss: 2.5213\n",
      "2266: lr: 0.0100, train loss: 2.4614, val loss: 2.5093\n",
      "2267: lr: 0.0100, train loss: 2.4666, val loss: 2.4800\n",
      "2268: lr: 0.0100, train loss: 2.4328, val loss: 2.4202\n",
      "2269: lr: 0.0100, train loss: 2.4392, val loss: 2.4619\n",
      "2270: lr: 0.0100, train loss: 2.4658, val loss: 2.5208\n",
      "2271: lr: 0.0100, train loss: 2.4801, val loss: 2.4972\n",
      "2272: lr: 0.0100, train loss: 2.4993, val loss: 2.4380\n",
      "2273: lr: 0.0100, train loss: 2.4046, val loss: 2.5056\n",
      "2274: lr: 0.0100, train loss: 2.4457, val loss: 2.4500\n",
      "2275: lr: 0.0100, train loss: 2.4454, val loss: 2.5043\n",
      "2276: lr: 0.0100, train loss: 2.4452, val loss: 2.4714\n",
      "2277: lr: 0.0100, train loss: 2.4498, val loss: 2.4575\n",
      "2278: lr: 0.0100, train loss: 2.4958, val loss: 2.4749\n",
      "2279: lr: 0.0100, train loss: 2.4777, val loss: 2.4763\n",
      "2280: lr: 0.0100, train loss: 2.4641, val loss: 2.4556\n",
      "2281: lr: 0.0100, train loss: 2.4351, val loss: 2.4778\n",
      "2282: lr: 0.0100, train loss: 2.4607, val loss: 2.5052\n",
      "2283: lr: 0.0100, train loss: 2.4880, val loss: 2.4913\n",
      "2284: lr: 0.0100, train loss: 2.4502, val loss: 2.4775\n",
      "2285: lr: 0.0100, train loss: 2.4483, val loss: 2.4643\n",
      "2286: lr: 0.0100, train loss: 2.4221, val loss: 2.4824\n",
      "2287: lr: 0.0100, train loss: 2.4806, val loss: 2.4927\n",
      "2288: lr: 0.0100, train loss: 2.4501, val loss: 2.4774\n",
      "2289: lr: 0.0100, train loss: 2.4791, val loss: 2.4771\n",
      "2290: lr: 0.0100, train loss: 2.4804, val loss: 2.5002\n",
      "2291: lr: 0.0100, train loss: 2.4486, val loss: 2.5321\n",
      "2292: lr: 0.0100, train loss: 2.4370, val loss: 2.4894\n",
      "2293: lr: 0.0100, train loss: 2.4799, val loss: 2.4909\n",
      "2294: lr: 0.0100, train loss: 2.4795, val loss: 2.4727\n",
      "2295: lr: 0.0100, train loss: 2.4932, val loss: 2.4615\n",
      "2296: lr: 0.0100, train loss: 2.4852, val loss: 2.5254\n",
      "2297: lr: 0.0100, train loss: 2.4524, val loss: 2.4544\n",
      "2298: lr: 0.0100, train loss: 2.4954, val loss: 2.5178\n",
      "2299: lr: 0.0100, train loss: 2.4353, val loss: 2.4643\n",
      "2300: lr: 0.0100, train loss: 2.4432, val loss: 2.4794\n",
      "2301: lr: 0.0100, train loss: 2.4807, val loss: 2.5010\n",
      "2302: lr: 0.0100, train loss: 2.4421, val loss: 2.4915\n",
      "2303: lr: 0.0100, train loss: 2.4973, val loss: 2.4506\n",
      "2304: lr: 0.0100, train loss: 2.5126, val loss: 2.4913\n",
      "2305: lr: 0.0100, train loss: 2.4695, val loss: 2.4674\n",
      "2306: lr: 0.0100, train loss: 2.4841, val loss: 2.5239\n",
      "2307: lr: 0.0100, train loss: 2.4719, val loss: 2.4466\n",
      "2308: lr: 0.0100, train loss: 2.4493, val loss: 2.4793\n",
      "2309: lr: 0.0100, train loss: 2.4632, val loss: 2.4849\n",
      "2310: lr: 0.0100, train loss: 2.4510, val loss: 2.4343\n",
      "2311: lr: 0.0100, train loss: 2.4541, val loss: 2.4785\n",
      "2312: lr: 0.0100, train loss: 2.4574, val loss: 2.4671\n",
      "2313: lr: 0.0100, train loss: 2.4667, val loss: 2.4975\n",
      "2314: lr: 0.0100, train loss: 2.4696, val loss: 2.5082\n",
      "2315: lr: 0.0100, train loss: 2.4533, val loss: 2.5464\n",
      "2316: lr: 0.0100, train loss: 2.4682, val loss: 2.5258\n",
      "2317: lr: 0.0100, train loss: 2.4397, val loss: 2.5108\n",
      "2318: lr: 0.0100, train loss: 2.4440, val loss: 2.4793\n",
      "2319: lr: 0.0100, train loss: 2.4602, val loss: 2.5343\n",
      "2320: lr: 0.0100, train loss: 2.4490, val loss: 2.4518\n",
      "2321: lr: 0.0100, train loss: 2.4712, val loss: 2.5253\n",
      "2322: lr: 0.0100, train loss: 2.5039, val loss: 2.5453\n",
      "2323: lr: 0.0100, train loss: 2.4873, val loss: 2.5220\n",
      "2324: lr: 0.0100, train loss: 2.4666, val loss: 2.4757\n",
      "2325: lr: 0.0100, train loss: 2.4952, val loss: 2.4563\n",
      "2326: lr: 0.0100, train loss: 2.4417, val loss: 2.4447\n",
      "2327: lr: 0.0100, train loss: 2.4558, val loss: 2.4803\n",
      "2328: lr: 0.0100, train loss: 2.4610, val loss: 2.4791\n",
      "2329: lr: 0.0100, train loss: 2.4662, val loss: 2.4926\n",
      "2330: lr: 0.0100, train loss: 2.4802, val loss: 2.5048\n",
      "2331: lr: 0.0100, train loss: 2.4371, val loss: 2.4951\n",
      "2332: lr: 0.0100, train loss: 2.4471, val loss: 2.4883\n",
      "2333: lr: 0.0100, train loss: 2.4455, val loss: 2.4782\n",
      "2334: lr: 0.0100, train loss: 2.4477, val loss: 2.5281\n",
      "2335: lr: 0.0100, train loss: 2.4834, val loss: 2.5085\n",
      "2336: lr: 0.0100, train loss: 2.4888, val loss: 2.4791\n",
      "2337: lr: 0.0100, train loss: 2.4848, val loss: 2.5171\n",
      "2338: lr: 0.0100, train loss: 2.4480, val loss: 2.5276\n",
      "2339: lr: 0.0100, train loss: 2.4422, val loss: 2.4635\n",
      "2340: lr: 0.0100, train loss: 2.4571, val loss: 2.4468\n",
      "2341: lr: 0.0100, train loss: 2.4472, val loss: 2.5371\n",
      "2342: lr: 0.0100, train loss: 2.4270, val loss: 2.5078\n",
      "2343: lr: 0.0100, train loss: 2.4371, val loss: 2.4827\n",
      "2344: lr: 0.0100, train loss: 2.4790, val loss: 2.5105\n",
      "2345: lr: 0.0100, train loss: 2.4474, val loss: 2.5071\n",
      "2346: lr: 0.0100, train loss: 2.4086, val loss: 2.5068\n",
      "2347: lr: 0.0100, train loss: 2.4699, val loss: 2.4619\n",
      "2348: lr: 0.0100, train loss: 2.4981, val loss: 2.4793\n",
      "2349: lr: 0.0100, train loss: 2.4523, val loss: 2.5217\n",
      "2350: lr: 0.0100, train loss: 2.4475, val loss: 2.4760\n",
      "2351: lr: 0.0100, train loss: 2.4289, val loss: 2.4418\n",
      "2352: lr: 0.0100, train loss: 2.4245, val loss: 2.5184\n",
      "2353: lr: 0.0100, train loss: 2.4390, val loss: 2.4991\n",
      "2354: lr: 0.0100, train loss: 2.4953, val loss: 2.5110\n",
      "2355: lr: 0.0100, train loss: 2.4423, val loss: 2.5130\n",
      "2356: lr: 0.0100, train loss: 2.4688, val loss: 2.5229\n",
      "2357: lr: 0.0100, train loss: 2.4814, val loss: 2.5170\n",
      "2358: lr: 0.0100, train loss: 2.4872, val loss: 2.5072\n",
      "2359: lr: 0.0100, train loss: 2.4529, val loss: 2.4782\n",
      "2360: lr: 0.0100, train loss: 2.5111, val loss: 2.4875\n",
      "2361: lr: 0.0100, train loss: 2.4634, val loss: 2.4620\n",
      "2362: lr: 0.0100, train loss: 2.4848, val loss: 2.4742\n",
      "2363: lr: 0.0100, train loss: 2.4562, val loss: 2.4935\n",
      "2364: lr: 0.0100, train loss: 2.5036, val loss: 2.5037\n",
      "2365: lr: 0.0100, train loss: 2.4373, val loss: 2.5087\n",
      "2366: lr: 0.0100, train loss: 2.5048, val loss: 2.5135\n",
      "2367: lr: 0.0100, train loss: 2.4426, val loss: 2.4722\n",
      "2368: lr: 0.0100, train loss: 2.4675, val loss: 2.4963\n",
      "2369: lr: 0.0100, train loss: 2.4607, val loss: 2.4894\n",
      "2370: lr: 0.0100, train loss: 2.4851, val loss: 2.4697\n",
      "2371: lr: 0.0100, train loss: 2.4538, val loss: 2.4841\n",
      "2372: lr: 0.0100, train loss: 2.4404, val loss: 2.5007\n",
      "2373: lr: 0.0100, train loss: 2.4275, val loss: 2.4968\n",
      "2374: lr: 0.0100, train loss: 2.4873, val loss: 2.5371\n",
      "2375: lr: 0.0100, train loss: 2.4945, val loss: 2.4688\n",
      "2376: lr: 0.0100, train loss: 2.4482, val loss: 2.4673\n",
      "2377: lr: 0.0100, train loss: 2.4618, val loss: 2.4970\n",
      "2378: lr: 0.0100, train loss: 2.4663, val loss: 2.5188\n",
      "2379: lr: 0.0100, train loss: 2.4450, val loss: 2.5116\n",
      "2380: lr: 0.0100, train loss: 2.4461, val loss: 2.4562\n",
      "2381: lr: 0.0100, train loss: 2.4508, val loss: 2.4455\n",
      "2382: lr: 0.0100, train loss: 2.4817, val loss: 2.5061\n",
      "2383: lr: 0.0100, train loss: 2.4321, val loss: 2.5290\n",
      "2384: lr: 0.0100, train loss: 2.4536, val loss: 2.5068\n",
      "2385: lr: 0.0100, train loss: 2.5264, val loss: 2.5432\n",
      "2386: lr: 0.0100, train loss: 2.4227, val loss: 2.4702\n",
      "2387: lr: 0.0100, train loss: 2.4764, val loss: 2.4528\n",
      "2388: lr: 0.0100, train loss: 2.4407, val loss: 2.4638\n",
      "2389: lr: 0.0100, train loss: 2.4733, val loss: 2.4928\n",
      "2390: lr: 0.0100, train loss: 2.4634, val loss: 2.4810\n",
      "2391: lr: 0.0100, train loss: 2.4840, val loss: 2.5091\n",
      "2392: lr: 0.0100, train loss: 2.4665, val loss: 2.4885\n",
      "2393: lr: 0.0100, train loss: 2.4670, val loss: 2.5607\n",
      "2394: lr: 0.0100, train loss: 2.3976, val loss: 2.5168\n",
      "2395: lr: 0.0100, train loss: 2.4665, val loss: 2.5547\n",
      "2396: lr: 0.0100, train loss: 2.4597, val loss: 2.4955\n",
      "2397: lr: 0.0100, train loss: 2.4752, val loss: 2.4529\n",
      "2398: lr: 0.0100, train loss: 2.4570, val loss: 2.4867\n",
      "2399: lr: 0.0100, train loss: 2.4228, val loss: 2.5482\n",
      "2400: lr: 0.0100, train loss: 2.4269, val loss: 2.4969\n",
      "2401: lr: 0.0100, train loss: 2.4554, val loss: 2.4831\n",
      "2402: lr: 0.0100, train loss: 2.4394, val loss: 2.4973\n",
      "2403: lr: 0.0100, train loss: 2.4689, val loss: 2.5157\n",
      "2404: lr: 0.0100, train loss: 2.4816, val loss: 2.4750\n",
      "2405: lr: 0.0100, train loss: 2.4801, val loss: 2.5217\n",
      "2406: lr: 0.0100, train loss: 2.4796, val loss: 2.5045\n",
      "2407: lr: 0.0100, train loss: 2.4143, val loss: 2.5004\n",
      "2408: lr: 0.0100, train loss: 2.4575, val loss: 2.4634\n",
      "2409: lr: 0.0100, train loss: 2.4808, val loss: 2.5083\n",
      "2410: lr: 0.0100, train loss: 2.4610, val loss: 2.4488\n",
      "2411: lr: 0.0100, train loss: 2.5216, val loss: 2.5067\n",
      "2412: lr: 0.0100, train loss: 2.4730, val loss: 2.5019\n",
      "2413: lr: 0.0100, train loss: 2.4216, val loss: 2.4663\n",
      "2414: lr: 0.0100, train loss: 2.5048, val loss: 2.4388\n",
      "2415: lr: 0.0100, train loss: 2.4731, val loss: 2.4475\n",
      "2416: lr: 0.0100, train loss: 2.4732, val loss: 2.5103\n",
      "2417: lr: 0.0100, train loss: 2.5139, val loss: 2.4738\n",
      "2418: lr: 0.0100, train loss: 2.4750, val loss: 2.4928\n",
      "2419: lr: 0.0100, train loss: 2.4974, val loss: 2.4802\n",
      "2420: lr: 0.0100, train loss: 2.5035, val loss: 2.4572\n",
      "2421: lr: 0.0100, train loss: 2.4873, val loss: 2.5075\n",
      "2422: lr: 0.0100, train loss: 2.4492, val loss: 2.4898\n",
      "2423: lr: 0.0100, train loss: 2.3779, val loss: 2.4407\n",
      "2424: lr: 0.0100, train loss: 2.4907, val loss: 2.4577\n",
      "2425: lr: 0.0100, train loss: 2.3894, val loss: 2.4444\n",
      "2426: lr: 0.0100, train loss: 2.4393, val loss: 2.5714\n",
      "2427: lr: 0.0100, train loss: 2.4807, val loss: 2.4886\n",
      "2428: lr: 0.0100, train loss: 2.4783, val loss: 2.4555\n",
      "2429: lr: 0.0100, train loss: 2.4226, val loss: 2.4917\n",
      "2430: lr: 0.0100, train loss: 2.5008, val loss: 2.5061\n",
      "2431: lr: 0.0100, train loss: 2.4358, val loss: 2.5199\n",
      "2432: lr: 0.0100, train loss: 2.4030, val loss: 2.4419\n",
      "2433: lr: 0.0100, train loss: 2.4754, val loss: 2.4628\n",
      "2434: lr: 0.0100, train loss: 2.4502, val loss: 2.4994\n",
      "2435: lr: 0.0100, train loss: 2.4807, val loss: 2.4641\n",
      "2436: lr: 0.0100, train loss: 2.4679, val loss: 2.4723\n",
      "2437: lr: 0.0100, train loss: 2.4498, val loss: 2.4804\n",
      "2438: lr: 0.0100, train loss: 2.5138, val loss: 2.5290\n",
      "2439: lr: 0.0100, train loss: 2.4270, val loss: 2.4836\n",
      "2440: lr: 0.0100, train loss: 2.4699, val loss: 2.4991\n",
      "2441: lr: 0.0100, train loss: 2.4773, val loss: 2.5068\n",
      "2442: lr: 0.0100, train loss: 2.4590, val loss: 2.4504\n",
      "2443: lr: 0.0100, train loss: 2.4959, val loss: 2.4763\n",
      "2444: lr: 0.0100, train loss: 2.4379, val loss: 2.5096\n",
      "2445: lr: 0.0100, train loss: 2.4434, val loss: 2.4817\n",
      "2446: lr: 0.0100, train loss: 2.4197, val loss: 2.5102\n",
      "2447: lr: 0.0100, train loss: 2.4532, val loss: 2.4910\n",
      "2448: lr: 0.0100, train loss: 2.4346, val loss: 2.4939\n",
      "2449: lr: 0.0100, train loss: 2.4447, val loss: 2.5195\n",
      "2450: lr: 0.0100, train loss: 2.4284, val loss: 2.4880\n",
      "2451: lr: 0.0100, train loss: 2.4483, val loss: 2.4674\n",
      "2452: lr: 0.0100, train loss: 2.4741, val loss: 2.4934\n",
      "2453: lr: 0.0100, train loss: 2.4497, val loss: 2.4725\n",
      "2454: lr: 0.0100, train loss: 2.4607, val loss: 2.5055\n",
      "2455: lr: 0.0100, train loss: 2.4532, val loss: 2.4733\n",
      "2456: lr: 0.0100, train loss: 2.4741, val loss: 2.4510\n",
      "2457: lr: 0.0100, train loss: 2.4984, val loss: 2.5194\n",
      "2458: lr: 0.0100, train loss: 2.4883, val loss: 2.4884\n",
      "2459: lr: 0.0100, train loss: 2.4235, val loss: 2.4879\n",
      "2460: lr: 0.0100, train loss: 2.4343, val loss: 2.5549\n",
      "2461: lr: 0.0100, train loss: 2.4679, val loss: 2.4714\n",
      "2462: lr: 0.0100, train loss: 2.4772, val loss: 2.4800\n",
      "2463: lr: 0.0100, train loss: 2.4460, val loss: 2.4748\n",
      "2464: lr: 0.0100, train loss: 2.4719, val loss: 2.4708\n",
      "2465: lr: 0.0100, train loss: 2.4709, val loss: 2.5045\n",
      "2466: lr: 0.0100, train loss: 2.4351, val loss: 2.5152\n",
      "2467: lr: 0.0100, train loss: 2.4816, val loss: 2.4507\n",
      "2468: lr: 0.0100, train loss: 2.5201, val loss: 2.4648\n",
      "2469: lr: 0.0100, train loss: 2.4493, val loss: 2.5078\n",
      "2470: lr: 0.0100, train loss: 2.4711, val loss: 2.5205\n",
      "2471: lr: 0.0100, train loss: 2.4490, val loss: 2.4818\n",
      "2472: lr: 0.0100, train loss: 2.4573, val loss: 2.4680\n",
      "2473: lr: 0.0100, train loss: 2.4725, val loss: 2.5252\n",
      "2474: lr: 0.0100, train loss: 2.4589, val loss: 2.4563\n",
      "2475: lr: 0.0100, train loss: 2.4783, val loss: 2.4786\n",
      "2476: lr: 0.0100, train loss: 2.4732, val loss: 2.4739\n",
      "2477: lr: 0.0100, train loss: 2.4071, val loss: 2.4891\n",
      "2478: lr: 0.0100, train loss: 2.4454, val loss: 2.4751\n",
      "2479: lr: 0.0100, train loss: 2.4511, val loss: 2.4918\n",
      "2480: lr: 0.0100, train loss: 2.4207, val loss: 2.4557\n",
      "2481: lr: 0.0100, train loss: 2.4950, val loss: 2.4680\n",
      "2482: lr: 0.0100, train loss: 2.4734, val loss: 2.4716\n",
      "2483: lr: 0.0100, train loss: 2.4507, val loss: 2.5139\n",
      "2484: lr: 0.0100, train loss: 2.4971, val loss: 2.4941\n",
      "2485: lr: 0.0100, train loss: 2.4409, val loss: 2.5039\n",
      "2486: lr: 0.0100, train loss: 2.4813, val loss: 2.4656\n",
      "2487: lr: 0.0100, train loss: 2.4797, val loss: 2.4573\n",
      "2488: lr: 0.0100, train loss: 2.4779, val loss: 2.5015\n",
      "2489: lr: 0.0100, train loss: 2.4276, val loss: 2.4819\n",
      "2490: lr: 0.0100, train loss: 2.4906, val loss: 2.5252\n",
      "2491: lr: 0.0100, train loss: 2.4570, val loss: 2.5013\n",
      "2492: lr: 0.0100, train loss: 2.4444, val loss: 2.4672\n",
      "2493: lr: 0.0100, train loss: 2.4957, val loss: 2.5340\n",
      "2494: lr: 0.0100, train loss: 2.4475, val loss: 2.4773\n",
      "2495: lr: 0.0100, train loss: 2.5080, val loss: 2.5218\n",
      "2496: lr: 0.0100, train loss: 2.4604, val loss: 2.4610\n",
      "2497: lr: 0.0100, train loss: 2.4267, val loss: 2.4783\n",
      "2498: lr: 0.0100, train loss: 2.4457, val loss: 2.4697\n",
      "2499: lr: 0.0100, train loss: 2.5195, val loss: 2.5029\n",
      "2500: lr: 0.0100, train loss: 2.4852, val loss: 2.4878\n",
      "2501: lr: 0.0100, train loss: 2.4743, val loss: 2.5121\n",
      "2502: lr: 0.0100, train loss: 2.4645, val loss: 2.4855\n",
      "2503: lr: 0.0100, train loss: 2.4651, val loss: 2.5130\n",
      "2504: lr: 0.0100, train loss: 2.4804, val loss: 2.4473\n",
      "2505: lr: 0.0100, train loss: 2.4682, val loss: 2.4711\n",
      "2506: lr: 0.0100, train loss: 2.4510, val loss: 2.4611\n",
      "2507: lr: 0.0100, train loss: 2.4140, val loss: 2.5058\n",
      "2508: lr: 0.0100, train loss: 2.4169, val loss: 2.4696\n",
      "2509: lr: 0.0100, train loss: 2.4136, val loss: 2.5047\n",
      "2510: lr: 0.0100, train loss: 2.4144, val loss: 2.4843\n",
      "2511: lr: 0.0100, train loss: 2.4720, val loss: 2.5204\n",
      "2512: lr: 0.0100, train loss: 2.4694, val loss: 2.5166\n",
      "2513: lr: 0.0100, train loss: 2.4379, val loss: 2.4985\n",
      "2514: lr: 0.0100, train loss: 2.4600, val loss: 2.5403\n",
      "2515: lr: 0.0100, train loss: 2.4411, val loss: 2.4647\n",
      "2516: lr: 0.0100, train loss: 2.4788, val loss: 2.4791\n",
      "2517: lr: 0.0100, train loss: 2.4483, val loss: 2.5056\n",
      "2518: lr: 0.0100, train loss: 2.4953, val loss: 2.5283\n",
      "2519: lr: 0.0100, train loss: 2.4370, val loss: 2.4629\n",
      "2520: lr: 0.0100, train loss: 2.4582, val loss: 2.4695\n",
      "2521: lr: 0.0100, train loss: 2.4036, val loss: 2.5270\n",
      "2522: lr: 0.0100, train loss: 2.5165, val loss: 2.4650\n",
      "2523: lr: 0.0100, train loss: 2.4878, val loss: 2.5013\n",
      "2524: lr: 0.0100, train loss: 2.4723, val loss: 2.4921\n",
      "2525: lr: 0.0100, train loss: 2.5022, val loss: 2.4720\n",
      "2526: lr: 0.0100, train loss: 2.4854, val loss: 2.4946\n",
      "2527: lr: 0.0100, train loss: 2.4403, val loss: 2.5044\n",
      "2528: lr: 0.0100, train loss: 2.5024, val loss: 2.4673\n",
      "2529: lr: 0.0100, train loss: 2.5011, val loss: 2.5109\n",
      "2530: lr: 0.0100, train loss: 2.4664, val loss: 2.4932\n",
      "2531: lr: 0.0100, train loss: 2.4347, val loss: 2.4737\n",
      "2532: lr: 0.0100, train loss: 2.4572, val loss: 2.4809\n",
      "2533: lr: 0.0100, train loss: 2.4756, val loss: 2.4597\n",
      "2534: lr: 0.0100, train loss: 2.4512, val loss: 2.5083\n",
      "2535: lr: 0.0100, train loss: 2.4035, val loss: 2.4939\n",
      "2536: lr: 0.0100, train loss: 2.4448, val loss: 2.5403\n",
      "2537: lr: 0.0100, train loss: 2.4603, val loss: 2.4994\n",
      "2538: lr: 0.0100, train loss: 2.4566, val loss: 2.5146\n",
      "2539: lr: 0.0100, train loss: 2.4814, val loss: 2.5110\n",
      "2540: lr: 0.0100, train loss: 2.4535, val loss: 2.4881\n",
      "2541: lr: 0.0100, train loss: 2.5190, val loss: 2.4820\n",
      "2542: lr: 0.0100, train loss: 2.4581, val loss: 2.4801\n",
      "2543: lr: 0.0100, train loss: 2.5092, val loss: 2.5145\n",
      "2544: lr: 0.0100, train loss: 2.4899, val loss: 2.4648\n",
      "2545: lr: 0.0100, train loss: 2.4423, val loss: 2.5146\n",
      "2546: lr: 0.0100, train loss: 2.4712, val loss: 2.4991\n",
      "2547: lr: 0.0100, train loss: 2.4521, val loss: 2.4639\n",
      "2548: lr: 0.0100, train loss: 2.4803, val loss: 2.5189\n",
      "2549: lr: 0.0100, train loss: 2.4948, val loss: 2.5210\n",
      "2550: lr: 0.0100, train loss: 2.4230, val loss: 2.4735\n",
      "2551: lr: 0.0100, train loss: 2.4263, val loss: 2.5105\n",
      "2552: lr: 0.0100, train loss: 2.4512, val loss: 2.5383\n",
      "2553: lr: 0.0100, train loss: 2.4722, val loss: 2.4808\n",
      "2554: lr: 0.0100, train loss: 2.4474, val loss: 2.5271\n",
      "2555: lr: 0.0100, train loss: 2.4331, val loss: 2.5496\n",
      "2556: lr: 0.0100, train loss: 2.4349, val loss: 2.4880\n",
      "2557: lr: 0.0100, train loss: 2.4023, val loss: 2.5203\n",
      "2558: lr: 0.0100, train loss: 2.3981, val loss: 2.4827\n",
      "2559: lr: 0.0100, train loss: 2.4991, val loss: 2.5136\n",
      "2560: lr: 0.0100, train loss: 2.4643, val loss: 2.4557\n",
      "2561: lr: 0.0100, train loss: 2.4669, val loss: 2.5126\n",
      "2562: lr: 0.0100, train loss: 2.4321, val loss: 2.4837\n",
      "2563: lr: 0.0100, train loss: 2.4855, val loss: 2.4703\n",
      "2564: lr: 0.0100, train loss: 2.4568, val loss: 2.4663\n",
      "2565: lr: 0.0100, train loss: 2.4986, val loss: 2.5688\n",
      "2566: lr: 0.0100, train loss: 2.4850, val loss: 2.5003\n",
      "2567: lr: 0.0100, train loss: 2.5075, val loss: 2.4862\n",
      "2568: lr: 0.0100, train loss: 2.4742, val loss: 2.4658\n",
      "2569: lr: 0.0100, train loss: 2.4342, val loss: 2.4506\n",
      "2570: lr: 0.0100, train loss: 2.4625, val loss: 2.5043\n",
      "2571: lr: 0.0100, train loss: 2.4825, val loss: 2.5614\n",
      "2572: lr: 0.0100, train loss: 2.4728, val loss: 2.5070\n",
      "2573: lr: 0.0100, train loss: 2.4737, val loss: 2.4810\n",
      "2574: lr: 0.0100, train loss: 2.4894, val loss: 2.4728\n",
      "2575: lr: 0.0100, train loss: 2.4668, val loss: 2.4545\n",
      "2576: lr: 0.0100, train loss: 2.4564, val loss: 2.4731\n",
      "2577: lr: 0.0100, train loss: 2.4591, val loss: 2.5094\n",
      "2578: lr: 0.0100, train loss: 2.4726, val loss: 2.4801\n",
      "2579: lr: 0.0100, train loss: 2.4437, val loss: 2.4691\n",
      "2580: lr: 0.0100, train loss: 2.4514, val loss: 2.4822\n",
      "2581: lr: 0.0100, train loss: 2.5087, val loss: 2.4697\n",
      "2582: lr: 0.0100, train loss: 2.4442, val loss: 2.4368\n",
      "2583: lr: 0.0100, train loss: 2.4535, val loss: 2.4408\n",
      "2584: lr: 0.0100, train loss: 2.4470, val loss: 2.5271\n",
      "2585: lr: 0.0100, train loss: 2.4631, val loss: 2.5379\n",
      "2586: lr: 0.0100, train loss: 2.4458, val loss: 2.5148\n",
      "2587: lr: 0.0100, train loss: 2.4116, val loss: 2.4975\n",
      "2588: lr: 0.0100, train loss: 2.4902, val loss: 2.4520\n",
      "2589: lr: 0.0100, train loss: 2.4186, val loss: 2.4774\n",
      "2590: lr: 0.0100, train loss: 2.4196, val loss: 2.4783\n",
      "2591: lr: 0.0100, train loss: 2.5039, val loss: 2.5356\n",
      "2592: lr: 0.0100, train loss: 2.4674, val loss: 2.4939\n",
      "2593: lr: 0.0100, train loss: 2.4640, val loss: 2.5170\n",
      "2594: lr: 0.0100, train loss: 2.4336, val loss: 2.5465\n",
      "2595: lr: 0.0100, train loss: 2.4516, val loss: 2.5360\n",
      "2596: lr: 0.0100, train loss: 2.4404, val loss: 2.5198\n",
      "2597: lr: 0.0100, train loss: 2.4797, val loss: 2.5401\n",
      "2598: lr: 0.0100, train loss: 2.4463, val loss: 2.4753\n",
      "2599: lr: 0.0100, train loss: 2.4720, val loss: 2.5148\n",
      "2600: lr: 0.0100, train loss: 2.4943, val loss: 2.4806\n",
      "2601: lr: 0.0100, train loss: 2.4255, val loss: 2.5050\n",
      "2602: lr: 0.0100, train loss: 2.4556, val loss: 2.5392\n",
      "2603: lr: 0.0100, train loss: 2.4874, val loss: 2.4290\n",
      "2604: lr: 0.0100, train loss: 2.4766, val loss: 2.4503\n",
      "2605: lr: 0.0100, train loss: 2.4314, val loss: 2.5076\n",
      "2606: lr: 0.0100, train loss: 2.4484, val loss: 2.5187\n",
      "2607: lr: 0.0100, train loss: 2.4803, val loss: 2.5293\n",
      "2608: lr: 0.0100, train loss: 2.4563, val loss: 2.5026\n",
      "2609: lr: 0.0100, train loss: 2.4535, val loss: 2.5174\n",
      "2610: lr: 0.0100, train loss: 2.4725, val loss: 2.5056\n",
      "2611: lr: 0.0100, train loss: 2.4215, val loss: 2.4761\n",
      "2612: lr: 0.0100, train loss: 2.4465, val loss: 2.4986\n",
      "2613: lr: 0.0100, train loss: 2.4742, val loss: 2.5640\n",
      "2614: lr: 0.0100, train loss: 2.4860, val loss: 2.5158\n",
      "2615: lr: 0.0100, train loss: 2.4556, val loss: 2.4713\n",
      "2616: lr: 0.0100, train loss: 2.4900, val loss: 2.4667\n",
      "2617: lr: 0.0100, train loss: 2.4530, val loss: 2.5158\n",
      "2618: lr: 0.0100, train loss: 2.4535, val loss: 2.5096\n",
      "2619: lr: 0.0100, train loss: 2.4932, val loss: 2.4680\n",
      "2620: lr: 0.0100, train loss: 2.4555, val loss: 2.5030\n",
      "2621: lr: 0.0100, train loss: 2.4304, val loss: 2.4648\n",
      "2622: lr: 0.0100, train loss: 2.4835, val loss: 2.4393\n",
      "2623: lr: 0.0100, train loss: 2.4785, val loss: 2.4900\n",
      "2624: lr: 0.0100, train loss: 2.4433, val loss: 2.4956\n",
      "2625: lr: 0.0100, train loss: 2.4648, val loss: 2.4674\n",
      "2626: lr: 0.0100, train loss: 2.4360, val loss: 2.4847\n",
      "2627: lr: 0.0100, train loss: 2.4924, val loss: 2.4676\n",
      "2628: lr: 0.0100, train loss: 2.4641, val loss: 2.4676\n",
      "2629: lr: 0.0100, train loss: 2.4943, val loss: 2.4364\n",
      "2630: lr: 0.0100, train loss: 2.4031, val loss: 2.4486\n",
      "2631: lr: 0.0100, train loss: 2.4929, val loss: 2.5113\n",
      "2632: lr: 0.0100, train loss: 2.4895, val loss: 2.5025\n",
      "2633: lr: 0.0100, train loss: 2.4136, val loss: 2.5066\n",
      "2634: lr: 0.0100, train loss: 2.4636, val loss: 2.4835\n",
      "2635: lr: 0.0100, train loss: 2.4503, val loss: 2.4405\n",
      "2636: lr: 0.0100, train loss: 2.4626, val loss: 2.5079\n",
      "2637: lr: 0.0100, train loss: 2.4775, val loss: 2.4665\n",
      "2638: lr: 0.0100, train loss: 2.4669, val loss: 2.4790\n",
      "2639: lr: 0.0100, train loss: 2.4788, val loss: 2.4144\n",
      "2640: lr: 0.0100, train loss: 2.4549, val loss: 2.5119\n",
      "2641: lr: 0.0100, train loss: 2.4305, val loss: 2.5378\n",
      "2642: lr: 0.0100, train loss: 2.4685, val loss: 2.4439\n",
      "2643: lr: 0.0100, train loss: 2.4867, val loss: 2.4545\n",
      "2644: lr: 0.0100, train loss: 2.4605, val loss: 2.4627\n",
      "2645: lr: 0.0100, train loss: 2.4526, val loss: 2.4353\n",
      "2646: lr: 0.0100, train loss: 2.4927, val loss: 2.4852\n",
      "2647: lr: 0.0100, train loss: 2.4761, val loss: 2.5003\n",
      "2648: lr: 0.0100, train loss: 2.4738, val loss: 2.4650\n",
      "2649: lr: 0.0100, train loss: 2.4266, val loss: 2.4687\n",
      "2650: lr: 0.0100, train loss: 2.4572, val loss: 2.4027\n",
      "2651: lr: 0.0100, train loss: 2.4473, val loss: 2.5414\n",
      "2652: lr: 0.0100, train loss: 2.4862, val loss: 2.3973\n",
      "2653: lr: 0.0100, train loss: 2.4487, val loss: 2.5438\n",
      "2654: lr: 0.0100, train loss: 2.4588, val loss: 2.4779\n",
      "2655: lr: 0.0100, train loss: 2.4786, val loss: 2.4703\n",
      "2656: lr: 0.0100, train loss: 2.4931, val loss: 2.4628\n",
      "2657: lr: 0.0100, train loss: 2.4457, val loss: 2.5238\n",
      "2658: lr: 0.0100, train loss: 2.4904, val loss: 2.5056\n",
      "2659: lr: 0.0100, train loss: 2.4472, val loss: 2.5044\n",
      "2660: lr: 0.0100, train loss: 2.4713, val loss: 2.5289\n",
      "2661: lr: 0.0100, train loss: 2.4704, val loss: 2.4875\n",
      "2662: lr: 0.0100, train loss: 2.4416, val loss: 2.4574\n",
      "2663: lr: 0.0100, train loss: 2.4588, val loss: 2.4492\n",
      "2664: lr: 0.0100, train loss: 2.4616, val loss: 2.4848\n",
      "2665: lr: 0.0100, train loss: 2.4456, val loss: 2.5007\n",
      "2666: lr: 0.0100, train loss: 2.4913, val loss: 2.5113\n",
      "2667: lr: 0.0100, train loss: 2.4285, val loss: 2.4812\n",
      "2668: lr: 0.0100, train loss: 2.4690, val loss: 2.4374\n",
      "2669: lr: 0.0100, train loss: 2.4845, val loss: 2.4494\n",
      "2670: lr: 0.0100, train loss: 2.4375, val loss: 2.4771\n",
      "2671: lr: 0.0100, train loss: 2.4567, val loss: 2.4841\n",
      "2672: lr: 0.0100, train loss: 2.4486, val loss: 2.4698\n",
      "2673: lr: 0.0100, train loss: 2.4535, val loss: 2.5110\n",
      "2674: lr: 0.0100, train loss: 2.4571, val loss: 2.4331\n",
      "2675: lr: 0.0100, train loss: 2.4375, val loss: 2.4589\n",
      "2676: lr: 0.0100, train loss: 2.4417, val loss: 2.4903\n",
      "2677: lr: 0.0100, train loss: 2.4628, val loss: 2.4473\n",
      "2678: lr: 0.0100, train loss: 2.4546, val loss: 2.5006\n",
      "2679: lr: 0.0100, train loss: 2.4568, val loss: 2.4734\n",
      "2680: lr: 0.0100, train loss: 2.4426, val loss: 2.5092\n",
      "2681: lr: 0.0100, train loss: 2.4372, val loss: 2.4434\n",
      "2682: lr: 0.0100, train loss: 2.4585, val loss: 2.4820\n",
      "2683: lr: 0.0100, train loss: 2.4817, val loss: 2.5081\n",
      "2684: lr: 0.0100, train loss: 2.4476, val loss: 2.5053\n",
      "2685: lr: 0.0100, train loss: 2.4759, val loss: 2.4639\n",
      "2686: lr: 0.0100, train loss: 2.4378, val loss: 2.5029\n",
      "2687: lr: 0.0100, train loss: 2.4779, val loss: 2.4857\n",
      "2688: lr: 0.0100, train loss: 2.4795, val loss: 2.5124\n",
      "2689: lr: 0.0100, train loss: 2.4557, val loss: 2.4850\n",
      "2690: lr: 0.0100, train loss: 2.4769, val loss: 2.4792\n",
      "2691: lr: 0.0100, train loss: 2.4763, val loss: 2.4846\n",
      "2692: lr: 0.0100, train loss: 2.4629, val loss: 2.4648\n",
      "2693: lr: 0.0100, train loss: 2.4319, val loss: 2.5148\n",
      "2694: lr: 0.0100, train loss: 2.4296, val loss: 2.4496\n",
      "2695: lr: 0.0100, train loss: 2.4842, val loss: 2.5047\n",
      "2696: lr: 0.0100, train loss: 2.4639, val loss: 2.4752\n",
      "2697: lr: 0.0100, train loss: 2.4703, val loss: 2.4393\n",
      "2698: lr: 0.0100, train loss: 2.4496, val loss: 2.4712\n",
      "2699: lr: 0.0100, train loss: 2.4165, val loss: 2.5672\n",
      "2700: lr: 0.0100, train loss: 2.4898, val loss: 2.4943\n",
      "2701: lr: 0.0100, train loss: 2.4095, val loss: 2.5110\n",
      "2702: lr: 0.0100, train loss: 2.4864, val loss: 2.5093\n",
      "2703: lr: 0.0100, train loss: 2.4447, val loss: 2.5190\n",
      "2704: lr: 0.0100, train loss: 2.4223, val loss: 2.4988\n",
      "2705: lr: 0.0100, train loss: 2.4353, val loss: 2.4665\n",
      "2706: lr: 0.0100, train loss: 2.4364, val loss: 2.5249\n",
      "2707: lr: 0.0100, train loss: 2.4383, val loss: 2.4963\n",
      "2708: lr: 0.0100, train loss: 2.4721, val loss: 2.5011\n",
      "2709: lr: 0.0100, train loss: 2.4690, val loss: 2.5022\n",
      "2710: lr: 0.0100, train loss: 2.4756, val loss: 2.4510\n",
      "2711: lr: 0.0100, train loss: 2.4815, val loss: 2.4697\n",
      "2712: lr: 0.0100, train loss: 2.4704, val loss: 2.4833\n",
      "2713: lr: 0.0100, train loss: 2.4765, val loss: 2.4606\n",
      "2714: lr: 0.0100, train loss: 2.4982, val loss: 2.5345\n",
      "2715: lr: 0.0100, train loss: 2.4108, val loss: 2.4694\n",
      "2716: lr: 0.0100, train loss: 2.4882, val loss: 2.4622\n",
      "2717: lr: 0.0100, train loss: 2.5072, val loss: 2.5106\n",
      "2718: lr: 0.0100, train loss: 2.5018, val loss: 2.5073\n",
      "2719: lr: 0.0100, train loss: 2.4777, val loss: 2.4933\n",
      "2720: lr: 0.0100, train loss: 2.4026, val loss: 2.4785\n",
      "2721: lr: 0.0100, train loss: 2.4810, val loss: 2.4419\n",
      "2722: lr: 0.0100, train loss: 2.4704, val loss: 2.4820\n",
      "2723: lr: 0.0100, train loss: 2.4543, val loss: 2.4638\n",
      "2724: lr: 0.0100, train loss: 2.4364, val loss: 2.4426\n",
      "2725: lr: 0.0100, train loss: 2.4795, val loss: 2.5054\n",
      "2726: lr: 0.0100, train loss: 2.4303, val loss: 2.5085\n",
      "2727: lr: 0.0100, train loss: 2.4637, val loss: 2.5521\n",
      "2728: lr: 0.0100, train loss: 2.4932, val loss: 2.4873\n",
      "2729: lr: 0.0100, train loss: 2.4406, val loss: 2.4769\n",
      "2730: lr: 0.0100, train loss: 2.4503, val loss: 2.4414\n",
      "2731: lr: 0.0100, train loss: 2.4323, val loss: 2.5064\n",
      "2732: lr: 0.0100, train loss: 2.4532, val loss: 2.4867\n",
      "2733: lr: 0.0100, train loss: 2.4252, val loss: 2.4934\n",
      "2734: lr: 0.0100, train loss: 2.4350, val loss: 2.5177\n",
      "2735: lr: 0.0100, train loss: 2.4435, val loss: 2.4569\n",
      "2736: lr: 0.0100, train loss: 2.4821, val loss: 2.5189\n",
      "2737: lr: 0.0100, train loss: 2.4372, val loss: 2.4750\n",
      "2738: lr: 0.0100, train loss: 2.4355, val loss: 2.4634\n",
      "2739: lr: 0.0100, train loss: 2.4279, val loss: 2.4807\n",
      "2740: lr: 0.0100, train loss: 2.4261, val loss: 2.5381\n",
      "2741: lr: 0.0100, train loss: 2.4398, val loss: 2.5349\n",
      "2742: lr: 0.0100, train loss: 2.4466, val loss: 2.5294\n",
      "2743: lr: 0.0100, train loss: 2.4052, val loss: 2.4428\n",
      "2744: lr: 0.0100, train loss: 2.4953, val loss: 2.5359\n",
      "2745: lr: 0.0100, train loss: 2.4274, val loss: 2.5087\n",
      "2746: lr: 0.0100, train loss: 2.4815, val loss: 2.4662\n",
      "2747: lr: 0.0100, train loss: 2.4950, val loss: 2.4904\n",
      "2748: lr: 0.0100, train loss: 2.4688, val loss: 2.5128\n",
      "2749: lr: 0.0100, train loss: 2.4609, val loss: 2.4824\n",
      "2750: lr: 0.0100, train loss: 2.4968, val loss: 2.5256\n",
      "2751: lr: 0.0100, train loss: 2.4349, val loss: 2.4632\n",
      "2752: lr: 0.0100, train loss: 2.4362, val loss: 2.4736\n",
      "2753: lr: 0.0100, train loss: 2.3887, val loss: 2.4950\n",
      "2754: lr: 0.0100, train loss: 2.4853, val loss: 2.5032\n",
      "2755: lr: 0.0100, train loss: 2.4755, val loss: 2.5085\n",
      "2756: lr: 0.0100, train loss: 2.4895, val loss: 2.4596\n",
      "2757: lr: 0.0100, train loss: 2.4642, val loss: 2.5116\n",
      "2758: lr: 0.0100, train loss: 2.4802, val loss: 2.5084\n",
      "2759: lr: 0.0100, train loss: 2.4154, val loss: 2.4930\n",
      "2760: lr: 0.0100, train loss: 2.4904, val loss: 2.4665\n",
      "2761: lr: 0.0100, train loss: 2.4597, val loss: 2.4870\n",
      "2762: lr: 0.0100, train loss: 2.4872, val loss: 2.4673\n",
      "2763: lr: 0.0100, train loss: 2.4420, val loss: 2.4849\n",
      "2764: lr: 0.0100, train loss: 2.4904, val loss: 2.5108\n",
      "2765: lr: 0.0100, train loss: 2.4426, val loss: 2.4627\n",
      "2766: lr: 0.0100, train loss: 2.4477, val loss: 2.4984\n",
      "2767: lr: 0.0100, train loss: 2.4694, val loss: 2.5497\n",
      "2768: lr: 0.0100, train loss: 2.4664, val loss: 2.4850\n",
      "2769: lr: 0.0100, train loss: 2.4928, val loss: 2.5410\n",
      "2770: lr: 0.0100, train loss: 2.4692, val loss: 2.4877\n",
      "2771: lr: 0.0100, train loss: 2.4655, val loss: 2.4973\n",
      "2772: lr: 0.0100, train loss: 2.4515, val loss: 2.4842\n",
      "2773: lr: 0.0100, train loss: 2.4786, val loss: 2.4651\n",
      "2774: lr: 0.0100, train loss: 2.4779, val loss: 2.4689\n",
      "2775: lr: 0.0100, train loss: 2.4234, val loss: 2.5506\n",
      "2776: lr: 0.0100, train loss: 2.4113, val loss: 2.4996\n",
      "2777: lr: 0.0100, train loss: 2.4574, val loss: 2.4664\n",
      "2778: lr: 0.0100, train loss: 2.4990, val loss: 2.5146\n",
      "2779: lr: 0.0100, train loss: 2.4538, val loss: 2.4819\n",
      "2780: lr: 0.0100, train loss: 2.4429, val loss: 2.4654\n",
      "2781: lr: 0.0100, train loss: 2.4477, val loss: 2.5128\n",
      "2782: lr: 0.0100, train loss: 2.5003, val loss: 2.4686\n",
      "2783: lr: 0.0100, train loss: 2.4769, val loss: 2.4709\n",
      "2784: lr: 0.0100, train loss: 2.4462, val loss: 2.5337\n",
      "2785: lr: 0.0100, train loss: 2.4768, val loss: 2.4818\n",
      "2786: lr: 0.0100, train loss: 2.4629, val loss: 2.4999\n",
      "2787: lr: 0.0100, train loss: 2.4449, val loss: 2.5271\n",
      "2788: lr: 0.0100, train loss: 2.4333, val loss: 2.4809\n",
      "2789: lr: 0.0100, train loss: 2.4431, val loss: 2.5115\n",
      "2790: lr: 0.0100, train loss: 2.4715, val loss: 2.5049\n",
      "2791: lr: 0.0100, train loss: 2.4518, val loss: 2.5551\n",
      "2792: lr: 0.0100, train loss: 2.4082, val loss: 2.5413\n",
      "2793: lr: 0.0100, train loss: 2.4453, val loss: 2.4823\n",
      "2794: lr: 0.0100, train loss: 2.4590, val loss: 2.5028\n",
      "2795: lr: 0.0100, train loss: 2.4780, val loss: 2.5184\n",
      "2796: lr: 0.0100, train loss: 2.4376, val loss: 2.5236\n",
      "2797: lr: 0.0100, train loss: 2.4699, val loss: 2.5136\n",
      "2798: lr: 0.0100, train loss: 2.4323, val loss: 2.4575\n",
      "2799: lr: 0.0100, train loss: 2.4479, val loss: 2.5173\n",
      "2800: lr: 0.0100, train loss: 2.4746, val loss: 2.4723\n",
      "2801: lr: 0.0100, train loss: 2.4399, val loss: 2.5314\n",
      "2802: lr: 0.0100, train loss: 2.4590, val loss: 2.4756\n",
      "2803: lr: 0.0100, train loss: 2.4453, val loss: 2.4511\n",
      "2804: lr: 0.0100, train loss: 2.4559, val loss: 2.4925\n",
      "2805: lr: 0.0100, train loss: 2.4369, val loss: 2.4818\n",
      "2806: lr: 0.0100, train loss: 2.4777, val loss: 2.5008\n",
      "2807: lr: 0.0100, train loss: 2.3943, val loss: 2.4790\n",
      "2808: lr: 0.0100, train loss: 2.4775, val loss: 2.4484\n",
      "2809: lr: 0.0100, train loss: 2.4411, val loss: 2.4644\n",
      "2810: lr: 0.0100, train loss: 2.4625, val loss: 2.4116\n",
      "2811: lr: 0.0100, train loss: 2.4719, val loss: 2.4972\n",
      "2812: lr: 0.0100, train loss: 2.4655, val loss: 2.4527\n",
      "2813: lr: 0.0100, train loss: 2.4513, val loss: 2.4806\n",
      "2814: lr: 0.0100, train loss: 2.4435, val loss: 2.4680\n",
      "2815: lr: 0.0100, train loss: 2.4949, val loss: 2.5534\n",
      "2816: lr: 0.0100, train loss: 2.4102, val loss: 2.5404\n",
      "2817: lr: 0.0100, train loss: 2.5153, val loss: 2.4905\n",
      "2818: lr: 0.0100, train loss: 2.4529, val loss: 2.5299\n",
      "2819: lr: 0.0100, train loss: 2.4652, val loss: 2.4775\n",
      "2820: lr: 0.0100, train loss: 2.4840, val loss: 2.4508\n",
      "2821: lr: 0.0100, train loss: 2.4713, val loss: 2.4824\n",
      "2822: lr: 0.0100, train loss: 2.5188, val loss: 2.4937\n",
      "2823: lr: 0.0100, train loss: 2.4525, val loss: 2.4652\n",
      "2824: lr: 0.0100, train loss: 2.4516, val loss: 2.4862\n",
      "2825: lr: 0.0100, train loss: 2.4366, val loss: 2.4966\n",
      "2826: lr: 0.0100, train loss: 2.4520, val loss: 2.4830\n",
      "2827: lr: 0.0100, train loss: 2.4535, val loss: 2.4664\n",
      "2828: lr: 0.0100, train loss: 2.4987, val loss: 2.4887\n",
      "2829: lr: 0.0100, train loss: 2.4471, val loss: 2.5308\n",
      "2830: lr: 0.0100, train loss: 2.4445, val loss: 2.4637\n",
      "2831: lr: 0.0100, train loss: 2.4546, val loss: 2.5129\n",
      "2832: lr: 0.0100, train loss: 2.4783, val loss: 2.4821\n",
      "2833: lr: 0.0100, train loss: 2.4536, val loss: 2.4896\n",
      "2834: lr: 0.0100, train loss: 2.4834, val loss: 2.4464\n",
      "2835: lr: 0.0100, train loss: 2.4411, val loss: 2.5048\n",
      "2836: lr: 0.0100, train loss: 2.4737, val loss: 2.5433\n",
      "2837: lr: 0.0100, train loss: 2.4743, val loss: 2.5441\n",
      "2838: lr: 0.0100, train loss: 2.5090, val loss: 2.5359\n",
      "2839: lr: 0.0100, train loss: 2.4690, val loss: 2.5298\n",
      "2840: lr: 0.0100, train loss: 2.5081, val loss: 2.5211\n",
      "2841: lr: 0.0100, train loss: 2.4544, val loss: 2.5004\n",
      "2842: lr: 0.0100, train loss: 2.4269, val loss: 2.4962\n",
      "2843: lr: 0.0100, train loss: 2.4767, val loss: 2.4642\n",
      "2844: lr: 0.0100, train loss: 2.4319, val loss: 2.4985\n",
      "2845: lr: 0.0100, train loss: 2.4317, val loss: 2.4817\n",
      "2846: lr: 0.0100, train loss: 2.4630, val loss: 2.4488\n",
      "2847: lr: 0.0100, train loss: 2.4482, val loss: 2.4532\n",
      "2848: lr: 0.0100, train loss: 2.4444, val loss: 2.4535\n",
      "2849: lr: 0.0100, train loss: 2.4371, val loss: 2.5419\n",
      "2850: lr: 0.0100, train loss: 2.4196, val loss: 2.4587\n",
      "2851: lr: 0.0100, train loss: 2.4305, val loss: 2.4998\n",
      "2852: lr: 0.0100, train loss: 2.4503, val loss: 2.4684\n",
      "2853: lr: 0.0100, train loss: 2.4373, val loss: 2.4513\n",
      "2854: lr: 0.0100, train loss: 2.4536, val loss: 2.4446\n",
      "2855: lr: 0.0100, train loss: 2.4579, val loss: 2.5045\n",
      "2856: lr: 0.0100, train loss: 2.4335, val loss: 2.4801\n",
      "2857: lr: 0.0100, train loss: 2.4115, val loss: 2.4944\n",
      "2858: lr: 0.0100, train loss: 2.4478, val loss: 2.4860\n",
      "2859: lr: 0.0100, train loss: 2.4217, val loss: 2.4746\n",
      "2860: lr: 0.0100, train loss: 2.4867, val loss: 2.4993\n",
      "2861: lr: 0.0100, train loss: 2.4877, val loss: 2.4486\n",
      "2862: lr: 0.0100, train loss: 2.4399, val loss: 2.5334\n",
      "2863: lr: 0.0100, train loss: 2.4120, val loss: 2.5115\n",
      "2864: lr: 0.0100, train loss: 2.4773, val loss: 2.5542\n",
      "2865: lr: 0.0100, train loss: 2.4373, val loss: 2.5141\n",
      "2866: lr: 0.0100, train loss: 2.4444, val loss: 2.4717\n",
      "2867: lr: 0.0100, train loss: 2.4669, val loss: 2.4548\n",
      "2868: lr: 0.0100, train loss: 2.4377, val loss: 2.4599\n",
      "2869: lr: 0.0100, train loss: 2.4442, val loss: 2.4789\n",
      "2870: lr: 0.0100, train loss: 2.4382, val loss: 2.4992\n",
      "2871: lr: 0.0100, train loss: 2.4660, val loss: 2.4664\n",
      "2872: lr: 0.0100, train loss: 2.4708, val loss: 2.5003\n",
      "2873: lr: 0.0100, train loss: 2.4706, val loss: 2.4858\n",
      "2874: lr: 0.0100, train loss: 2.4504, val loss: 2.4780\n",
      "2875: lr: 0.0100, train loss: 2.4665, val loss: 2.4734\n",
      "2876: lr: 0.0100, train loss: 2.3946, val loss: 2.5146\n",
      "2877: lr: 0.0100, train loss: 2.4540, val loss: 2.4471\n",
      "2878: lr: 0.0100, train loss: 2.4645, val loss: 2.4846\n",
      "2879: lr: 0.0100, train loss: 2.4796, val loss: 2.5053\n",
      "2880: lr: 0.0100, train loss: 2.4819, val loss: 2.4990\n",
      "2881: lr: 0.0100, train loss: 2.4420, val loss: 2.5224\n",
      "2882: lr: 0.0100, train loss: 2.5277, val loss: 2.4853\n",
      "2883: lr: 0.0100, train loss: 2.4433, val loss: 2.5615\n",
      "2884: lr: 0.0100, train loss: 2.4819, val loss: 2.4230\n",
      "2885: lr: 0.0100, train loss: 2.4624, val loss: 2.4801\n",
      "2886: lr: 0.0100, train loss: 2.5083, val loss: 2.4728\n",
      "2887: lr: 0.0100, train loss: 2.4496, val loss: 2.5242\n",
      "2888: lr: 0.0100, train loss: 2.4579, val loss: 2.4895\n",
      "2889: lr: 0.0100, train loss: 2.4497, val loss: 2.4836\n",
      "2890: lr: 0.0100, train loss: 2.4894, val loss: 2.5012\n",
      "2891: lr: 0.0100, train loss: 2.4778, val loss: 2.5160\n",
      "2892: lr: 0.0100, train loss: 2.4674, val loss: 2.4940\n",
      "2893: lr: 0.0100, train loss: 2.4587, val loss: 2.4618\n",
      "2894: lr: 0.0100, train loss: 2.4150, val loss: 2.4817\n",
      "2895: lr: 0.0100, train loss: 2.4644, val loss: 2.5200\n",
      "2896: lr: 0.0100, train loss: 2.4367, val loss: 2.4991\n",
      "2897: lr: 0.0100, train loss: 2.4128, val loss: 2.5356\n",
      "2898: lr: 0.0100, train loss: 2.4188, val loss: 2.4669\n",
      "2899: lr: 0.0100, train loss: 2.4845, val loss: 2.5002\n",
      "2900: lr: 0.0100, train loss: 2.4609, val loss: 2.4840\n",
      "2901: lr: 0.0100, train loss: 2.4860, val loss: 2.4715\n",
      "2902: lr: 0.0100, train loss: 2.4274, val loss: 2.4472\n",
      "2903: lr: 0.0100, train loss: 2.4810, val loss: 2.4558\n",
      "2904: lr: 0.0100, train loss: 2.4747, val loss: 2.4940\n",
      "2905: lr: 0.0100, train loss: 2.4794, val loss: 2.4807\n",
      "2906: lr: 0.0100, train loss: 2.4615, val loss: 2.4950\n",
      "2907: lr: 0.0100, train loss: 2.4937, val loss: 2.4618\n",
      "2908: lr: 0.0100, train loss: 2.5095, val loss: 2.4379\n",
      "2909: lr: 0.0100, train loss: 2.4557, val loss: 2.4662\n",
      "2910: lr: 0.0100, train loss: 2.4544, val loss: 2.5326\n",
      "2911: lr: 0.0100, train loss: 2.4959, val loss: 2.4729\n",
      "2912: lr: 0.0100, train loss: 2.4154, val loss: 2.4949\n",
      "2913: lr: 0.0100, train loss: 2.4569, val loss: 2.4880\n",
      "2914: lr: 0.0100, train loss: 2.4409, val loss: 2.4931\n",
      "2915: lr: 0.0100, train loss: 2.4643, val loss: 2.5088\n",
      "2916: lr: 0.0100, train loss: 2.4715, val loss: 2.4894\n",
      "2917: lr: 0.0100, train loss: 2.4430, val loss: 2.5092\n",
      "2918: lr: 0.0100, train loss: 2.4209, val loss: 2.4895\n",
      "2919: lr: 0.0100, train loss: 2.5123, val loss: 2.4623\n",
      "2920: lr: 0.0100, train loss: 2.4194, val loss: 2.4049\n",
      "2921: lr: 0.0100, train loss: 2.4800, val loss: 2.4790\n",
      "2922: lr: 0.0100, train loss: 2.5131, val loss: 2.4646\n",
      "2923: lr: 0.0100, train loss: 2.4717, val loss: 2.5167\n",
      "2924: lr: 0.0100, train loss: 2.4405, val loss: 2.4925\n",
      "2925: lr: 0.0100, train loss: 2.4678, val loss: 2.5215\n",
      "2926: lr: 0.0100, train loss: 2.4555, val loss: 2.4516\n",
      "2927: lr: 0.0100, train loss: 2.4349, val loss: 2.5356\n",
      "2928: lr: 0.0100, train loss: 2.4305, val loss: 2.4880\n",
      "2929: lr: 0.0100, train loss: 2.4650, val loss: 2.4805\n",
      "2930: lr: 0.0100, train loss: 2.4703, val loss: 2.4628\n",
      "2931: lr: 0.0100, train loss: 2.4773, val loss: 2.4978\n",
      "2932: lr: 0.0100, train loss: 2.4375, val loss: 2.4848\n",
      "2933: lr: 0.0100, train loss: 2.4441, val loss: 2.4451\n",
      "2934: lr: 0.0100, train loss: 2.4517, val loss: 2.4601\n",
      "2935: lr: 0.0100, train loss: 2.4324, val loss: 2.4459\n",
      "2936: lr: 0.0100, train loss: 2.4595, val loss: 2.4675\n",
      "2937: lr: 0.0100, train loss: 2.4690, val loss: 2.5150\n",
      "2938: lr: 0.0100, train loss: 2.4095, val loss: 2.4657\n",
      "2939: lr: 0.0100, train loss: 2.5033, val loss: 2.5257\n",
      "2940: lr: 0.0100, train loss: 2.4755, val loss: 2.4246\n",
      "2941: lr: 0.0100, train loss: 2.4457, val loss: 2.5056\n",
      "2942: lr: 0.0100, train loss: 2.4584, val loss: 2.4663\n",
      "2943: lr: 0.0100, train loss: 2.4556, val loss: 2.4757\n",
      "2944: lr: 0.0100, train loss: 2.3925, val loss: 2.4720\n",
      "2945: lr: 0.0100, train loss: 2.4578, val loss: 2.4740\n",
      "2946: lr: 0.0100, train loss: 2.4513, val loss: 2.5202\n",
      "2947: lr: 0.0100, train loss: 2.4844, val loss: 2.4792\n",
      "2948: lr: 0.0100, train loss: 2.4269, val loss: 2.4811\n",
      "2949: lr: 0.0100, train loss: 2.4217, val loss: 2.4935\n",
      "2950: lr: 0.0100, train loss: 2.4517, val loss: 2.4937\n",
      "2951: lr: 0.0100, train loss: 2.4382, val loss: 2.4682\n",
      "2952: lr: 0.0100, train loss: 2.4765, val loss: 2.4659\n",
      "2953: lr: 0.0100, train loss: 2.4660, val loss: 2.5094\n",
      "2954: lr: 0.0100, train loss: 2.4168, val loss: 2.5407\n",
      "2955: lr: 0.0100, train loss: 2.4373, val loss: 2.4511\n",
      "2956: lr: 0.0100, train loss: 2.4753, val loss: 2.4965\n",
      "2957: lr: 0.0100, train loss: 2.4736, val loss: 2.4766\n",
      "2958: lr: 0.0100, train loss: 2.4643, val loss: 2.5180\n",
      "2959: lr: 0.0100, train loss: 2.4470, val loss: 2.4610\n",
      "2960: lr: 0.0100, train loss: 2.4733, val loss: 2.4773\n",
      "2961: lr: 0.0100, train loss: 2.4391, val loss: 2.4904\n",
      "2962: lr: 0.0100, train loss: 2.4676, val loss: 2.5237\n",
      "2963: lr: 0.0100, train loss: 2.4292, val loss: 2.4743\n",
      "2964: lr: 0.0100, train loss: 2.4688, val loss: 2.4595\n",
      "2965: lr: 0.0100, train loss: 2.4312, val loss: 2.4826\n",
      "2966: lr: 0.0100, train loss: 2.4447, val loss: 2.5425\n",
      "2967: lr: 0.0100, train loss: 2.4553, val loss: 2.4877\n",
      "2968: lr: 0.0100, train loss: 2.4811, val loss: 2.4742\n",
      "2969: lr: 0.0100, train loss: 2.4630, val loss: 2.5067\n",
      "2970: lr: 0.0100, train loss: 2.4381, val loss: 2.4546\n",
      "2971: lr: 0.0100, train loss: 2.4503, val loss: 2.5237\n",
      "2972: lr: 0.0100, train loss: 2.5031, val loss: 2.4822\n",
      "2973: lr: 0.0100, train loss: 2.4076, val loss: 2.5239\n",
      "2974: lr: 0.0100, train loss: 2.4495, val loss: 2.4948\n",
      "2975: lr: 0.0100, train loss: 2.4484, val loss: 2.4932\n",
      "2976: lr: 0.0100, train loss: 2.4399, val loss: 2.5053\n",
      "2977: lr: 0.0100, train loss: 2.4892, val loss: 2.4906\n",
      "2978: lr: 0.0100, train loss: 2.4642, val loss: 2.4794\n",
      "2979: lr: 0.0100, train loss: 2.4499, val loss: 2.5035\n",
      "2980: lr: 0.0100, train loss: 2.4362, val loss: 2.5038\n",
      "2981: lr: 0.0100, train loss: 2.4804, val loss: 2.5161\n",
      "2982: lr: 0.0100, train loss: 2.4449, val loss: 2.4548\n",
      "2983: lr: 0.0100, train loss: 2.4788, val loss: 2.4736\n",
      "2984: lr: 0.0100, train loss: 2.4343, val loss: 2.5111\n",
      "2985: lr: 0.0100, train loss: 2.4669, val loss: 2.5142\n",
      "2986: lr: 0.0100, train loss: 2.4561, val loss: 2.4912\n",
      "2987: lr: 0.0100, train loss: 2.4374, val loss: 2.4897\n",
      "2988: lr: 0.0100, train loss: 2.4796, val loss: 2.4865\n",
      "2989: lr: 0.0100, train loss: 2.4204, val loss: 2.4987\n",
      "2990: lr: 0.0100, train loss: 2.4122, val loss: 2.5113\n",
      "2991: lr: 0.0100, train loss: 2.4649, val loss: 2.4178\n",
      "2992: lr: 0.0100, train loss: 2.4560, val loss: 2.4825\n",
      "2993: lr: 0.0100, train loss: 2.4594, val loss: 2.4973\n",
      "2994: lr: 0.0100, train loss: 2.4483, val loss: 2.4578\n",
      "2995: lr: 0.0100, train loss: 2.4511, val loss: 2.5014\n",
      "2996: lr: 0.0100, train loss: 2.5034, val loss: 2.4613\n",
      "2997: lr: 0.0100, train loss: 2.4925, val loss: 2.5547\n",
      "2998: lr: 0.0100, train loss: 2.4211, val loss: 2.4622\n",
      "2999: lr: 0.0100, train loss: 2.5102, val loss: 2.4779\n",
      "3000: lr: 0.0100, train loss: 2.4751, val loss: 2.4981\n",
      "3001: lr: 0.0100, train loss: 2.4708, val loss: 2.5139\n",
      "3002: lr: 0.0100, train loss: 2.4718, val loss: 2.4912\n",
      "3003: lr: 0.0100, train loss: 2.4783, val loss: 2.4545\n",
      "3004: lr: 0.0100, train loss: 2.4767, val loss: 2.4830\n",
      "3005: lr: 0.0100, train loss: 2.4798, val loss: 2.4875\n",
      "3006: lr: 0.0100, train loss: 2.4411, val loss: 2.4518\n",
      "3007: lr: 0.0100, train loss: 2.4106, val loss: 2.4720\n",
      "3008: lr: 0.0100, train loss: 2.4752, val loss: 2.4976\n",
      "3009: lr: 0.0100, train loss: 2.4258, val loss: 2.4803\n",
      "3010: lr: 0.0100, train loss: 2.4681, val loss: 2.5129\n",
      "3011: lr: 0.0100, train loss: 2.4496, val loss: 2.4696\n",
      "3012: lr: 0.0100, train loss: 2.4087, val loss: 2.5006\n",
      "3013: lr: 0.0100, train loss: 2.4758, val loss: 2.4996\n",
      "3014: lr: 0.0100, train loss: 2.4476, val loss: 2.4443\n",
      "3015: lr: 0.0100, train loss: 2.4938, val loss: 2.4717\n",
      "3016: lr: 0.0100, train loss: 2.4547, val loss: 2.5152\n",
      "3017: lr: 0.0100, train loss: 2.4551, val loss: 2.4894\n",
      "3018: lr: 0.0100, train loss: 2.4598, val loss: 2.4907\n",
      "3019: lr: 0.0100, train loss: 2.4479, val loss: 2.4748\n",
      "3020: lr: 0.0100, train loss: 2.5406, val loss: 2.5012\n",
      "3021: lr: 0.0100, train loss: 2.4260, val loss: 2.4997\n",
      "3022: lr: 0.0100, train loss: 2.4217, val loss: 2.4763\n",
      "3023: lr: 0.0100, train loss: 2.4686, val loss: 2.5532\n",
      "3024: lr: 0.0100, train loss: 2.5017, val loss: 2.5039\n",
      "3025: lr: 0.0100, train loss: 2.4570, val loss: 2.4834\n",
      "3026: lr: 0.0100, train loss: 2.4299, val loss: 2.5380\n",
      "3027: lr: 0.0100, train loss: 2.4670, val loss: 2.5169\n",
      "3028: lr: 0.0100, train loss: 2.4966, val loss: 2.5375\n",
      "3029: lr: 0.0100, train loss: 2.5046, val loss: 2.4667\n",
      "3030: lr: 0.0100, train loss: 2.4578, val loss: 2.5355\n",
      "3031: lr: 0.0100, train loss: 2.4626, val loss: 2.5063\n",
      "3032: lr: 0.0100, train loss: 2.4437, val loss: 2.5385\n",
      "3033: lr: 0.0100, train loss: 2.5122, val loss: 2.4488\n",
      "3034: lr: 0.0100, train loss: 2.4679, val loss: 2.4470\n",
      "3035: lr: 0.0100, train loss: 2.5161, val loss: 2.4703\n",
      "3036: lr: 0.0100, train loss: 2.4559, val loss: 2.4800\n",
      "3037: lr: 0.0100, train loss: 2.4855, val loss: 2.5522\n",
      "3038: lr: 0.0100, train loss: 2.4818, val loss: 2.4909\n",
      "3039: lr: 0.0100, train loss: 2.4568, val loss: 2.4806\n",
      "3040: lr: 0.0100, train loss: 2.4419, val loss: 2.4864\n",
      "3041: lr: 0.0100, train loss: 2.4955, val loss: 2.4803\n",
      "3042: lr: 0.0100, train loss: 2.4879, val loss: 2.4680\n",
      "3043: lr: 0.0100, train loss: 2.5015, val loss: 2.5168\n",
      "3044: lr: 0.0100, train loss: 2.4451, val loss: 2.4871\n",
      "3045: lr: 0.0100, train loss: 2.4881, val loss: 2.4842\n",
      "3046: lr: 0.0100, train loss: 2.4657, val loss: 2.5238\n",
      "3047: lr: 0.0100, train loss: 2.4507, val loss: 2.4819\n",
      "3048: lr: 0.0100, train loss: 2.4676, val loss: 2.4515\n",
      "3049: lr: 0.0100, train loss: 2.4675, val loss: 2.4703\n",
      "3050: lr: 0.0100, train loss: 2.4509, val loss: 2.5038\n",
      "3051: lr: 0.0100, train loss: 2.4118, val loss: 2.4907\n",
      "3052: lr: 0.0100, train loss: 2.4756, val loss: 2.4937\n",
      "3053: lr: 0.0100, train loss: 2.4521, val loss: 2.4936\n",
      "3054: lr: 0.0100, train loss: 2.4329, val loss: 2.4879\n",
      "3055: lr: 0.0100, train loss: 2.4778, val loss: 2.4784\n",
      "3056: lr: 0.0100, train loss: 2.5082, val loss: 2.5175\n",
      "3057: lr: 0.0100, train loss: 2.4565, val loss: 2.4999\n",
      "3058: lr: 0.0100, train loss: 2.4632, val loss: 2.5272\n",
      "3059: lr: 0.0100, train loss: 2.4229, val loss: 2.5143\n",
      "3060: lr: 0.0100, train loss: 2.4578, val loss: 2.4830\n",
      "3061: lr: 0.0100, train loss: 2.4877, val loss: 2.4479\n",
      "3062: lr: 0.0100, train loss: 2.4963, val loss: 2.5226\n",
      "3063: lr: 0.0100, train loss: 2.4798, val loss: 2.4467\n",
      "3064: lr: 0.0100, train loss: 2.4602, val loss: 2.5051\n",
      "3065: lr: 0.0100, train loss: 2.4652, val loss: 2.4848\n",
      "3066: lr: 0.0100, train loss: 2.4427, val loss: 2.4652\n",
      "3067: lr: 0.0100, train loss: 2.4589, val loss: 2.4724\n",
      "3068: lr: 0.0100, train loss: 2.4836, val loss: 2.4880\n",
      "3069: lr: 0.0100, train loss: 2.4567, val loss: 2.4813\n",
      "3070: lr: 0.0100, train loss: 2.4399, val loss: 2.5098\n",
      "3071: lr: 0.0100, train loss: 2.4765, val loss: 2.5092\n",
      "3072: lr: 0.0100, train loss: 2.4354, val loss: 2.4363\n",
      "3073: lr: 0.0100, train loss: 2.4282, val loss: 2.4974\n",
      "3074: lr: 0.0100, train loss: 2.4847, val loss: 2.5213\n",
      "3075: lr: 0.0100, train loss: 2.4711, val loss: 2.4577\n",
      "3076: lr: 0.0100, train loss: 2.4535, val loss: 2.5077\n",
      "3077: lr: 0.0100, train loss: 2.4600, val loss: 2.5185\n",
      "3078: lr: 0.0100, train loss: 2.4475, val loss: 2.4811\n",
      "3079: lr: 0.0100, train loss: 2.4389, val loss: 2.5048\n",
      "3080: lr: 0.0100, train loss: 2.4772, val loss: 2.4586\n",
      "3081: lr: 0.0100, train loss: 2.4379, val loss: 2.4688\n",
      "3082: lr: 0.0100, train loss: 2.4570, val loss: 2.4939\n",
      "3083: lr: 0.0100, train loss: 2.4208, val loss: 2.5266\n",
      "3084: lr: 0.0100, train loss: 2.4377, val loss: 2.4863\n",
      "3085: lr: 0.0100, train loss: 2.4742, val loss: 2.4909\n",
      "3086: lr: 0.0100, train loss: 2.4656, val loss: 2.4646\n",
      "3087: lr: 0.0100, train loss: 2.4848, val loss: 2.5050\n",
      "3088: lr: 0.0100, train loss: 2.4478, val loss: 2.5093\n",
      "3089: lr: 0.0100, train loss: 2.4867, val loss: 2.5430\n",
      "3090: lr: 0.0100, train loss: 2.4505, val loss: 2.4710\n",
      "3091: lr: 0.0100, train loss: 2.4455, val loss: 2.4755\n",
      "3092: lr: 0.0100, train loss: 2.4900, val loss: 2.5028\n",
      "3093: lr: 0.0100, train loss: 2.4722, val loss: 2.4879\n",
      "3094: lr: 0.0100, train loss: 2.4722, val loss: 2.5204\n",
      "3095: lr: 0.0100, train loss: 2.4169, val loss: 2.4701\n",
      "3096: lr: 0.0100, train loss: 2.4337, val loss: 2.5326\n",
      "3097: lr: 0.0100, train loss: 2.4797, val loss: 2.5109\n",
      "3098: lr: 0.0100, train loss: 2.4800, val loss: 2.5234\n",
      "3099: lr: 0.0100, train loss: 2.4464, val loss: 2.5520\n",
      "3100: lr: 0.0100, train loss: 2.5047, val loss: 2.4160\n",
      "3101: lr: 0.0100, train loss: 2.4244, val loss: 2.4883\n",
      "3102: lr: 0.0100, train loss: 2.4141, val loss: 2.5416\n",
      "3103: lr: 0.0100, train loss: 2.4488, val loss: 2.4809\n",
      "3104: lr: 0.0100, train loss: 2.4608, val loss: 2.4401\n",
      "3105: lr: 0.0100, train loss: 2.4837, val loss: 2.4423\n",
      "3106: lr: 0.0100, train loss: 2.4433, val loss: 2.4647\n",
      "3107: lr: 0.0100, train loss: 2.4556, val loss: 2.4927\n",
      "3108: lr: 0.0100, train loss: 2.4594, val loss: 2.5004\n",
      "3109: lr: 0.0100, train loss: 2.4534, val loss: 2.4518\n",
      "3110: lr: 0.0100, train loss: 2.4105, val loss: 2.4777\n",
      "3111: lr: 0.0100, train loss: 2.4806, val loss: 2.4841\n",
      "3112: lr: 0.0100, train loss: 2.4656, val loss: 2.4849\n",
      "3113: lr: 0.0100, train loss: 2.4752, val loss: 2.5191\n",
      "3114: lr: 0.0100, train loss: 2.4242, val loss: 2.4498\n",
      "3115: lr: 0.0100, train loss: 2.4488, val loss: 2.4823\n",
      "3116: lr: 0.0100, train loss: 2.4512, val loss: 2.5169\n",
      "3117: lr: 0.0100, train loss: 2.4375, val loss: 2.5019\n",
      "3118: lr: 0.0100, train loss: 2.4526, val loss: 2.4629\n",
      "3119: lr: 0.0100, train loss: 2.4072, val loss: 2.4678\n",
      "3120: lr: 0.0100, train loss: 2.4571, val loss: 2.4750\n",
      "3121: lr: 0.0100, train loss: 2.4848, val loss: 2.5031\n",
      "3122: lr: 0.0100, train loss: 2.4438, val loss: 2.4798\n",
      "3123: lr: 0.0100, train loss: 2.4656, val loss: 2.5029\n",
      "3124: lr: 0.0100, train loss: 2.4815, val loss: 2.5367\n",
      "3125: lr: 0.0100, train loss: 2.4794, val loss: 2.5505\n",
      "3126: lr: 0.0100, train loss: 2.4674, val loss: 2.4346\n",
      "3127: lr: 0.0100, train loss: 2.4778, val loss: 2.5283\n",
      "3128: lr: 0.0100, train loss: 2.4905, val loss: 2.4427\n",
      "3129: lr: 0.0100, train loss: 2.4660, val loss: 2.4548\n",
      "3130: lr: 0.0100, train loss: 2.4624, val loss: 2.5129\n",
      "3131: lr: 0.0100, train loss: 2.4422, val loss: 2.5383\n",
      "3132: lr: 0.0100, train loss: 2.4446, val loss: 2.4999\n",
      "3133: lr: 0.0100, train loss: 2.4473, val loss: 2.4649\n",
      "3134: lr: 0.0100, train loss: 2.4803, val loss: 2.4718\n",
      "3135: lr: 0.0100, train loss: 2.4688, val loss: 2.4699\n",
      "3136: lr: 0.0100, train loss: 2.4498, val loss: 2.4921\n",
      "3137: lr: 0.0100, train loss: 2.4267, val loss: 2.5035\n",
      "3138: lr: 0.0100, train loss: 2.4100, val loss: 2.5092\n",
      "3139: lr: 0.0100, train loss: 2.4873, val loss: 2.4955\n",
      "3140: lr: 0.0100, train loss: 2.4772, val loss: 2.4760\n",
      "3141: lr: 0.0100, train loss: 2.4625, val loss: 2.5090\n",
      "3142: lr: 0.0100, train loss: 2.4666, val loss: 2.4967\n",
      "3143: lr: 0.0100, train loss: 2.4551, val loss: 2.5063\n",
      "3144: lr: 0.0100, train loss: 2.4656, val loss: 2.4970\n",
      "3145: lr: 0.0100, train loss: 2.4423, val loss: 2.4929\n",
      "3146: lr: 0.0100, train loss: 2.4885, val loss: 2.4139\n",
      "3147: lr: 0.0100, train loss: 2.4676, val loss: 2.5182\n",
      "3148: lr: 0.0100, train loss: 2.4621, val loss: 2.5027\n",
      "3149: lr: 0.0100, train loss: 2.4602, val loss: 2.5085\n",
      "3150: lr: 0.0100, train loss: 2.4685, val loss: 2.4726\n",
      "3151: lr: 0.0100, train loss: 2.4496, val loss: 2.5121\n",
      "3152: lr: 0.0100, train loss: 2.4962, val loss: 2.5018\n",
      "3153: lr: 0.0100, train loss: 2.5151, val loss: 2.5075\n",
      "3154: lr: 0.0100, train loss: 2.5153, val loss: 2.4899\n",
      "3155: lr: 0.0100, train loss: 2.4455, val loss: 2.5194\n",
      "3156: lr: 0.0100, train loss: 2.4380, val loss: 2.4981\n",
      "3157: lr: 0.0100, train loss: 2.4515, val loss: 2.4814\n",
      "3158: lr: 0.0100, train loss: 2.4069, val loss: 2.5025\n",
      "3159: lr: 0.0100, train loss: 2.4586, val loss: 2.4832\n",
      "3160: lr: 0.0100, train loss: 2.4485, val loss: 2.5175\n",
      "3161: lr: 0.0100, train loss: 2.4703, val loss: 2.4412\n",
      "3162: lr: 0.0100, train loss: 2.5203, val loss: 2.4593\n",
      "3163: lr: 0.0100, train loss: 2.4880, val loss: 2.5056\n",
      "3164: lr: 0.0100, train loss: 2.4522, val loss: 2.5012\n",
      "3165: lr: 0.0100, train loss: 2.4417, val loss: 2.4677\n",
      "3166: lr: 0.0100, train loss: 2.4641, val loss: 2.5092\n",
      "3167: lr: 0.0100, train loss: 2.4341, val loss: 2.5009\n",
      "3168: lr: 0.0100, train loss: 2.4512, val loss: 2.4959\n",
      "3169: lr: 0.0100, train loss: 2.5098, val loss: 2.4546\n",
      "3170: lr: 0.0100, train loss: 2.4498, val loss: 2.4575\n",
      "3171: lr: 0.0100, train loss: 2.4898, val loss: 2.4951\n",
      "3172: lr: 0.0100, train loss: 2.4616, val loss: 2.4275\n",
      "3173: lr: 0.0100, train loss: 2.4405, val loss: 2.5203\n",
      "3174: lr: 0.0100, train loss: 2.4656, val loss: 2.4940\n",
      "3175: lr: 0.0100, train loss: 2.4588, val loss: 2.5025\n",
      "3176: lr: 0.0100, train loss: 2.4239, val loss: 2.4609\n",
      "3177: lr: 0.0100, train loss: 2.4842, val loss: 2.5425\n",
      "3178: lr: 0.0100, train loss: 2.4129, val loss: 2.4793\n",
      "3179: lr: 0.0100, train loss: 2.4395, val loss: 2.5106\n",
      "3180: lr: 0.0100, train loss: 2.4788, val loss: 2.4896\n",
      "3181: lr: 0.0100, train loss: 2.4367, val loss: 2.5297\n",
      "3182: lr: 0.0100, train loss: 2.4210, val loss: 2.4706\n",
      "3183: lr: 0.0100, train loss: 2.4542, val loss: 2.4725\n",
      "3184: lr: 0.0100, train loss: 2.4749, val loss: 2.4862\n",
      "3185: lr: 0.0100, train loss: 2.4918, val loss: 2.4556\n",
      "3186: lr: 0.0100, train loss: 2.4541, val loss: 2.4336\n",
      "3187: lr: 0.0100, train loss: 2.4064, val loss: 2.5143\n",
      "3188: lr: 0.0100, train loss: 2.4290, val loss: 2.4242\n",
      "3189: lr: 0.0100, train loss: 2.4625, val loss: 2.4963\n",
      "3190: lr: 0.0100, train loss: 2.4643, val loss: 2.4806\n",
      "3191: lr: 0.0100, train loss: 2.4822, val loss: 2.4328\n",
      "3192: lr: 0.0100, train loss: 2.4483, val loss: 2.5190\n",
      "3193: lr: 0.0100, train loss: 2.4627, val loss: 2.5155\n",
      "3194: lr: 0.0100, train loss: 2.4574, val loss: 2.5098\n",
      "3195: lr: 0.0100, train loss: 2.4910, val loss: 2.4844\n",
      "3196: lr: 0.0100, train loss: 2.4686, val loss: 2.5109\n",
      "3197: lr: 0.0100, train loss: 2.4520, val loss: 2.4990\n",
      "3198: lr: 0.0100, train loss: 2.4506, val loss: 2.4866\n",
      "3199: lr: 0.0100, train loss: 2.4617, val loss: 2.5193\n",
      "3200: lr: 0.0100, train loss: 2.4499, val loss: 2.4281\n",
      "3201: lr: 0.0100, train loss: 2.4941, val loss: 2.5066\n",
      "3202: lr: 0.0100, train loss: 2.4741, val loss: 2.5450\n",
      "3203: lr: 0.0100, train loss: 2.4769, val loss: 2.4119\n",
      "3204: lr: 0.0100, train loss: 2.4715, val loss: 2.5095\n",
      "3205: lr: 0.0100, train loss: 2.4474, val loss: 2.4482\n",
      "3206: lr: 0.0100, train loss: 2.4617, val loss: 2.4776\n",
      "3207: lr: 0.0100, train loss: 2.4844, val loss: 2.4637\n",
      "3208: lr: 0.0100, train loss: 2.4217, val loss: 2.5231\n",
      "3209: lr: 0.0100, train loss: 2.4503, val loss: 2.5301\n",
      "3210: lr: 0.0100, train loss: 2.4650, val loss: 2.4374\n",
      "3211: lr: 0.0100, train loss: 2.4683, val loss: 2.5070\n",
      "3212: lr: 0.0100, train loss: 2.4298, val loss: 2.4863\n",
      "3213: lr: 0.0100, train loss: 2.4198, val loss: 2.5423\n",
      "3214: lr: 0.0100, train loss: 2.4784, val loss: 2.5440\n",
      "3215: lr: 0.0100, train loss: 2.4353, val loss: 2.4365\n",
      "3216: lr: 0.0100, train loss: 2.4518, val loss: 2.4569\n",
      "3217: lr: 0.0100, train loss: 2.4580, val loss: 2.5010\n",
      "3218: lr: 0.0100, train loss: 2.4464, val loss: 2.4689\n",
      "3219: lr: 0.0100, train loss: 2.4970, val loss: 2.5447\n",
      "3220: lr: 0.0100, train loss: 2.4899, val loss: 2.4399\n",
      "3221: lr: 0.0100, train loss: 2.4613, val loss: 2.5136\n",
      "3222: lr: 0.0100, train loss: 2.4605, val loss: 2.4943\n",
      "3223: lr: 0.0100, train loss: 2.4818, val loss: 2.4406\n",
      "3224: lr: 0.0100, train loss: 2.4542, val loss: 2.5192\n",
      "3225: lr: 0.0100, train loss: 2.4534, val loss: 2.4795\n",
      "3226: lr: 0.0100, train loss: 2.4596, val loss: 2.5046\n",
      "3227: lr: 0.0100, train loss: 2.4677, val loss: 2.4360\n",
      "3228: lr: 0.0100, train loss: 2.4817, val loss: 2.4880\n",
      "3229: lr: 0.0100, train loss: 2.4129, val loss: 2.4843\n",
      "3230: lr: 0.0100, train loss: 2.3976, val loss: 2.4203\n",
      "3231: lr: 0.0100, train loss: 2.4597, val loss: 2.5252\n",
      "3232: lr: 0.0100, train loss: 2.4459, val loss: 2.4316\n",
      "3233: lr: 0.0100, train loss: 2.4701, val loss: 2.4961\n",
      "3234: lr: 0.0100, train loss: 2.4437, val loss: 2.5010\n",
      "3235: lr: 0.0100, train loss: 2.4768, val loss: 2.4932\n",
      "3236: lr: 0.0100, train loss: 2.4749, val loss: 2.5063\n",
      "3237: lr: 0.0100, train loss: 2.4707, val loss: 2.5197\n",
      "3238: lr: 0.0100, train loss: 2.4525, val loss: 2.4962\n",
      "3239: lr: 0.0100, train loss: 2.4993, val loss: 2.5398\n",
      "3240: lr: 0.0100, train loss: 2.4753, val loss: 2.5009\n",
      "3241: lr: 0.0100, train loss: 2.4830, val loss: 2.4022\n",
      "3242: lr: 0.0100, train loss: 2.4618, val loss: 2.5016\n",
      "3243: lr: 0.0100, train loss: 2.5211, val loss: 2.4617\n",
      "3244: lr: 0.0100, train loss: 2.4437, val loss: 2.4704\n",
      "3245: lr: 0.0100, train loss: 2.4502, val loss: 2.5341\n",
      "3246: lr: 0.0100, train loss: 2.4655, val loss: 2.5127\n",
      "3247: lr: 0.0100, train loss: 2.5005, val loss: 2.4854\n",
      "3248: lr: 0.0100, train loss: 2.4870, val loss: 2.4707\n",
      "3249: lr: 0.0100, train loss: 2.4297, val loss: 2.5273\n",
      "3250: lr: 0.0100, train loss: 2.4688, val loss: 2.4632\n",
      "3251: lr: 0.0100, train loss: 2.4610, val loss: 2.4969\n",
      "3252: lr: 0.0100, train loss: 2.4256, val loss: 2.4792\n",
      "3253: lr: 0.0100, train loss: 2.4537, val loss: 2.4822\n",
      "3254: lr: 0.0100, train loss: 2.4042, val loss: 2.4991\n",
      "3255: lr: 0.0100, train loss: 2.4871, val loss: 2.4460\n",
      "3256: lr: 0.0100, train loss: 2.4701, val loss: 2.4908\n",
      "3257: lr: 0.0100, train loss: 2.4653, val loss: 2.4472\n",
      "3258: lr: 0.0100, train loss: 2.5008, val loss: 2.4745\n",
      "3259: lr: 0.0100, train loss: 2.4363, val loss: 2.5047\n",
      "3260: lr: 0.0100, train loss: 2.4719, val loss: 2.4707\n",
      "3261: lr: 0.0100, train loss: 2.4558, val loss: 2.5133\n",
      "3262: lr: 0.0100, train loss: 2.4880, val loss: 2.4581\n",
      "3263: lr: 0.0100, train loss: 2.4696, val loss: 2.4517\n",
      "3264: lr: 0.0100, train loss: 2.4268, val loss: 2.5107\n",
      "3265: lr: 0.0100, train loss: 2.4904, val loss: 2.4628\n",
      "3266: lr: 0.0100, train loss: 2.4389, val loss: 2.5161\n",
      "3267: lr: 0.0100, train loss: 2.4829, val loss: 2.4826\n",
      "3268: lr: 0.0100, train loss: 2.4457, val loss: 2.4366\n",
      "3269: lr: 0.0100, train loss: 2.4252, val loss: 2.4788\n",
      "3270: lr: 0.0100, train loss: 2.4538, val loss: 2.5583\n",
      "3271: lr: 0.0100, train loss: 2.4395, val loss: 2.5204\n",
      "3272: lr: 0.0100, train loss: 2.4907, val loss: 2.4884\n",
      "3273: lr: 0.0100, train loss: 2.4469, val loss: 2.4972\n",
      "3274: lr: 0.0100, train loss: 2.4697, val loss: 2.4229\n",
      "3275: lr: 0.0100, train loss: 2.5170, val loss: 2.4974\n",
      "3276: lr: 0.0100, train loss: 2.4562, val loss: 2.4803\n",
      "3277: lr: 0.0100, train loss: 2.4232, val loss: 2.4917\n",
      "3278: lr: 0.0100, train loss: 2.4193, val loss: 2.5320\n",
      "3279: lr: 0.0100, train loss: 2.4852, val loss: 2.5037\n",
      "3280: lr: 0.0100, train loss: 2.4731, val loss: 2.4925\n",
      "3281: lr: 0.0100, train loss: 2.4651, val loss: 2.5317\n",
      "3282: lr: 0.0100, train loss: 2.4661, val loss: 2.4817\n",
      "3283: lr: 0.0100, train loss: 2.4680, val loss: 2.4717\n",
      "3284: lr: 0.0100, train loss: 2.5103, val loss: 2.4520\n",
      "3285: lr: 0.0100, train loss: 2.5013, val loss: 2.5227\n",
      "3286: lr: 0.0100, train loss: 2.4606, val loss: 2.4694\n",
      "3287: lr: 0.0100, train loss: 2.4295, val loss: 2.4714\n",
      "3288: lr: 0.0100, train loss: 2.4617, val loss: 2.5168\n",
      "3289: lr: 0.0100, train loss: 2.4139, val loss: 2.5003\n",
      "3290: lr: 0.0100, train loss: 2.4755, val loss: 2.4522\n",
      "3291: lr: 0.0100, train loss: 2.5035, val loss: 2.4832\n",
      "3292: lr: 0.0100, train loss: 2.4447, val loss: 2.4863\n",
      "3293: lr: 0.0100, train loss: 2.4666, val loss: 2.4423\n",
      "3294: lr: 0.0100, train loss: 2.4296, val loss: 2.4784\n",
      "3295: lr: 0.0100, train loss: 2.4357, val loss: 2.4765\n",
      "3296: lr: 0.0100, train loss: 2.4584, val loss: 2.4756\n",
      "3297: lr: 0.0100, train loss: 2.4414, val loss: 2.4351\n",
      "3298: lr: 0.0100, train loss: 2.4419, val loss: 2.4671\n",
      "3299: lr: 0.0100, train loss: 2.4145, val loss: 2.4675\n",
      "3300: lr: 0.0100, train loss: 2.4792, val loss: 2.5045\n",
      "3301: lr: 0.0100, train loss: 2.4479, val loss: 2.4782\n",
      "3302: lr: 0.0100, train loss: 2.4590, val loss: 2.4524\n",
      "3303: lr: 0.0100, train loss: 2.5392, val loss: 2.5010\n",
      "3304: lr: 0.0100, train loss: 2.4869, val loss: 2.4946\n",
      "3305: lr: 0.0100, train loss: 2.4380, val loss: 2.5302\n",
      "3306: lr: 0.0100, train loss: 2.4493, val loss: 2.5214\n",
      "3307: lr: 0.0100, train loss: 2.4520, val loss: 2.4781\n",
      "3308: lr: 0.0100, train loss: 2.4509, val loss: 2.4535\n",
      "3309: lr: 0.0100, train loss: 2.4436, val loss: 2.4948\n",
      "3310: lr: 0.0100, train loss: 2.4840, val loss: 2.5212\n",
      "3311: lr: 0.0100, train loss: 2.4231, val loss: 2.5030\n",
      "3312: lr: 0.0100, train loss: 2.4395, val loss: 2.4768\n",
      "3313: lr: 0.0100, train loss: 2.4515, val loss: 2.5252\n",
      "3314: lr: 0.0100, train loss: 2.4767, val loss: 2.4697\n",
      "3315: lr: 0.0100, train loss: 2.4698, val loss: 2.4457\n",
      "3316: lr: 0.0100, train loss: 2.4729, val loss: 2.4845\n",
      "3317: lr: 0.0100, train loss: 2.5044, val loss: 2.4674\n",
      "3318: lr: 0.0100, train loss: 2.4880, val loss: 2.4683\n",
      "3319: lr: 0.0100, train loss: 2.4655, val loss: 2.4544\n",
      "3320: lr: 0.0100, train loss: 2.4479, val loss: 2.5399\n",
      "3321: lr: 0.0100, train loss: 2.4708, val loss: 2.4391\n",
      "3322: lr: 0.0100, train loss: 2.4711, val loss: 2.4742\n",
      "3323: lr: 0.0100, train loss: 2.4186, val loss: 2.4734\n",
      "3324: lr: 0.0100, train loss: 2.4486, val loss: 2.4633\n",
      "3325: lr: 0.0100, train loss: 2.4452, val loss: 2.4364\n",
      "3326: lr: 0.0100, train loss: 2.4274, val loss: 2.5110\n",
      "3327: lr: 0.0100, train loss: 2.4393, val loss: 2.5123\n",
      "3328: lr: 0.0100, train loss: 2.4995, val loss: 2.5361\n",
      "3329: lr: 0.0100, train loss: 2.4570, val loss: 2.4833\n",
      "3330: lr: 0.0100, train loss: 2.4699, val loss: 2.4648\n",
      "3331: lr: 0.0100, train loss: 2.4279, val loss: 2.4866\n",
      "3332: lr: 0.0100, train loss: 2.4496, val loss: 2.5008\n",
      "3333: lr: 0.0100, train loss: 2.4712, val loss: 2.4774\n",
      "3334: lr: 0.0100, train loss: 2.4484, val loss: 2.5176\n",
      "3335: lr: 0.0100, train loss: 2.5000, val loss: 2.4821\n",
      "3336: lr: 0.0100, train loss: 2.4444, val loss: 2.4646\n",
      "3337: lr: 0.0100, train loss: 2.4320, val loss: 2.5287\n",
      "3338: lr: 0.0100, train loss: 2.4674, val loss: 2.5697\n",
      "3339: lr: 0.0100, train loss: 2.4662, val loss: 2.4871\n",
      "3340: lr: 0.0100, train loss: 2.4732, val loss: 2.5688\n",
      "3341: lr: 0.0100, train loss: 2.4359, val loss: 2.4209\n",
      "3342: lr: 0.0100, train loss: 2.4345, val loss: 2.5187\n",
      "3343: lr: 0.0100, train loss: 2.4822, val loss: 2.4443\n",
      "3344: lr: 0.0100, train loss: 2.4767, val loss: 2.4677\n",
      "3345: lr: 0.0100, train loss: 2.3980, val loss: 2.4542\n",
      "3346: lr: 0.0100, train loss: 2.4614, val loss: 2.5560\n",
      "3347: lr: 0.0100, train loss: 2.4371, val loss: 2.5043\n",
      "3348: lr: 0.0100, train loss: 2.4391, val loss: 2.5016\n",
      "3349: lr: 0.0100, train loss: 2.4353, val loss: 2.4295\n",
      "3350: lr: 0.0100, train loss: 2.4814, val loss: 2.5239\n",
      "3351: lr: 0.0100, train loss: 2.4741, val loss: 2.5201\n",
      "3352: lr: 0.0100, train loss: 2.4747, val loss: 2.5185\n",
      "3353: lr: 0.0100, train loss: 2.4514, val loss: 2.4340\n",
      "3354: lr: 0.0100, train loss: 2.3918, val loss: 2.5282\n",
      "3355: lr: 0.0100, train loss: 2.4814, val loss: 2.4846\n",
      "3356: lr: 0.0100, train loss: 2.5029, val loss: 2.4901\n",
      "3357: lr: 0.0100, train loss: 2.4729, val loss: 2.4881\n",
      "3358: lr: 0.0100, train loss: 2.4589, val loss: 2.4748\n",
      "3359: lr: 0.0100, train loss: 2.4782, val loss: 2.4720\n",
      "3360: lr: 0.0100, train loss: 2.4548, val loss: 2.4851\n",
      "3361: lr: 0.0100, train loss: 2.4762, val loss: 2.4951\n",
      "3362: lr: 0.0100, train loss: 2.4696, val loss: 2.5030\n",
      "3363: lr: 0.0100, train loss: 2.4655, val loss: 2.4904\n",
      "3364: lr: 0.0100, train loss: 2.4910, val loss: 2.4923\n",
      "3365: lr: 0.0100, train loss: 2.4750, val loss: 2.5166\n",
      "3366: lr: 0.0100, train loss: 2.4177, val loss: 2.4822\n",
      "3367: lr: 0.0100, train loss: 2.4651, val loss: 2.4745\n",
      "3368: lr: 0.0100, train loss: 2.4098, val loss: 2.4829\n",
      "3369: lr: 0.0100, train loss: 2.4616, val loss: 2.5402\n",
      "3370: lr: 0.0100, train loss: 2.4480, val loss: 2.4728\n",
      "3371: lr: 0.0100, train loss: 2.5009, val loss: 2.4758\n",
      "3372: lr: 0.0100, train loss: 2.4847, val loss: 2.4808\n",
      "3373: lr: 0.0100, train loss: 2.4925, val loss: 2.4651\n",
      "3374: lr: 0.0100, train loss: 2.4531, val loss: 2.5819\n",
      "3375: lr: 0.0100, train loss: 2.4521, val loss: 2.5242\n",
      "3376: lr: 0.0100, train loss: 2.4245, val loss: 2.4698\n",
      "3377: lr: 0.0100, train loss: 2.4744, val loss: 2.5059\n",
      "3378: lr: 0.0100, train loss: 2.4726, val loss: 2.4422\n",
      "3379: lr: 0.0100, train loss: 2.4447, val loss: 2.5052\n",
      "3380: lr: 0.0100, train loss: 2.4575, val loss: 2.4569\n",
      "3381: lr: 0.0100, train loss: 2.4968, val loss: 2.5115\n",
      "3382: lr: 0.0100, train loss: 2.4590, val loss: 2.5034\n",
      "3383: lr: 0.0100, train loss: 2.4554, val loss: 2.4717\n",
      "3384: lr: 0.0100, train loss: 2.4272, val loss: 2.5181\n",
      "3385: lr: 0.0100, train loss: 2.4535, val loss: 2.4725\n",
      "3386: lr: 0.0100, train loss: 2.4602, val loss: 2.4741\n",
      "3387: lr: 0.0100, train loss: 2.4277, val loss: 2.5256\n",
      "3388: lr: 0.0100, train loss: 2.4943, val loss: 2.5045\n",
      "3389: lr: 0.0100, train loss: 2.4615, val loss: 2.5009\n",
      "3390: lr: 0.0100, train loss: 2.4086, val loss: 2.5012\n",
      "3391: lr: 0.0100, train loss: 2.4312, val loss: 2.4879\n",
      "3392: lr: 0.0100, train loss: 2.4663, val loss: 2.5374\n",
      "3393: lr: 0.0100, train loss: 2.4840, val loss: 2.5162\n",
      "3394: lr: 0.0100, train loss: 2.4469, val loss: 2.4972\n",
      "3395: lr: 0.0100, train loss: 2.4518, val loss: 2.4657\n",
      "3396: lr: 0.0100, train loss: 2.4436, val loss: 2.4416\n",
      "3397: lr: 0.0100, train loss: 2.4522, val loss: 2.5067\n",
      "3398: lr: 0.0100, train loss: 2.4689, val loss: 2.5043\n",
      "3399: lr: 0.0100, train loss: 2.4722, val loss: 2.4970\n",
      "3400: lr: 0.0100, train loss: 2.4638, val loss: 2.4952\n",
      "3401: lr: 0.0100, train loss: 2.4375, val loss: 2.5093\n",
      "3402: lr: 0.0100, train loss: 2.4279, val loss: 2.4909\n",
      "3403: lr: 0.0100, train loss: 2.4675, val loss: 2.4968\n",
      "3404: lr: 0.0100, train loss: 2.4646, val loss: 2.4773\n",
      "3405: lr: 0.0100, train loss: 2.5067, val loss: 2.5239\n",
      "3406: lr: 0.0100, train loss: 2.4990, val loss: 2.4772\n",
      "3407: lr: 0.0100, train loss: 2.4766, val loss: 2.5371\n",
      "3408: lr: 0.0100, train loss: 2.4687, val loss: 2.4997\n",
      "3409: lr: 0.0100, train loss: 2.4961, val loss: 2.4341\n",
      "3410: lr: 0.0100, train loss: 2.4814, val loss: 2.5121\n",
      "3411: lr: 0.0100, train loss: 2.5181, val loss: 2.4661\n",
      "3412: lr: 0.0100, train loss: 2.4240, val loss: 2.5122\n",
      "3413: lr: 0.0100, train loss: 2.4711, val loss: 2.4604\n",
      "3414: lr: 0.0100, train loss: 2.4564, val loss: 2.4684\n",
      "3415: lr: 0.0100, train loss: 2.5219, val loss: 2.5200\n",
      "3416: lr: 0.0100, train loss: 2.4388, val loss: 2.4990\n",
      "3417: lr: 0.0100, train loss: 2.4636, val loss: 2.4908\n",
      "3418: lr: 0.0100, train loss: 2.4864, val loss: 2.5052\n",
      "3419: lr: 0.0100, train loss: 2.4454, val loss: 2.5052\n",
      "3420: lr: 0.0100, train loss: 2.4287, val loss: 2.5599\n",
      "3421: lr: 0.0100, train loss: 2.4829, val loss: 2.4970\n",
      "3422: lr: 0.0100, train loss: 2.4556, val loss: 2.4626\n",
      "3423: lr: 0.0100, train loss: 2.4837, val loss: 2.4821\n",
      "3424: lr: 0.0100, train loss: 2.4046, val loss: 2.4872\n",
      "3425: lr: 0.0100, train loss: 2.4479, val loss: 2.5074\n",
      "3426: lr: 0.0100, train loss: 2.4841, val loss: 2.5002\n",
      "3427: lr: 0.0100, train loss: 2.4628, val loss: 2.4843\n",
      "3428: lr: 0.0100, train loss: 2.4395, val loss: 2.5099\n",
      "3429: lr: 0.0100, train loss: 2.4233, val loss: 2.4912\n",
      "3430: lr: 0.0100, train loss: 2.4747, val loss: 2.4512\n",
      "3431: lr: 0.0100, train loss: 2.4320, val loss: 2.4273\n",
      "3432: lr: 0.0100, train loss: 2.4647, val loss: 2.4599\n",
      "3433: lr: 0.0100, train loss: 2.3776, val loss: 2.4702\n",
      "3434: lr: 0.0100, train loss: 2.4977, val loss: 2.4168\n",
      "3435: lr: 0.0100, train loss: 2.5075, val loss: 2.4931\n",
      "3436: lr: 0.0100, train loss: 2.4731, val loss: 2.5071\n",
      "3437: lr: 0.0100, train loss: 2.4551, val loss: 2.5236\n",
      "3438: lr: 0.0100, train loss: 2.4693, val loss: 2.4471\n",
      "3439: lr: 0.0100, train loss: 2.4405, val loss: 2.4946\n",
      "3440: lr: 0.0100, train loss: 2.5029, val loss: 2.5401\n",
      "3441: lr: 0.0100, train loss: 2.4577, val loss: 2.4664\n",
      "3442: lr: 0.0100, train loss: 2.4019, val loss: 2.5313\n",
      "3443: lr: 0.0100, train loss: 2.4684, val loss: 2.4753\n",
      "3444: lr: 0.0100, train loss: 2.4714, val loss: 2.4917\n",
      "3445: lr: 0.0100, train loss: 2.4739, val loss: 2.4902\n",
      "3446: lr: 0.0100, train loss: 2.4385, val loss: 2.5004\n",
      "3447: lr: 0.0100, train loss: 2.4198, val loss: 2.4509\n",
      "3448: lr: 0.0100, train loss: 2.4677, val loss: 2.4358\n",
      "3449: lr: 0.0100, train loss: 2.4588, val loss: 2.4668\n",
      "3450: lr: 0.0100, train loss: 2.4276, val loss: 2.4959\n",
      "3451: lr: 0.0100, train loss: 2.4476, val loss: 2.5470\n",
      "3452: lr: 0.0100, train loss: 2.4777, val loss: 2.4935\n",
      "3453: lr: 0.0100, train loss: 2.4730, val loss: 2.4984\n",
      "3454: lr: 0.0100, train loss: 2.4460, val loss: 2.4969\n",
      "3455: lr: 0.0100, train loss: 2.4522, val loss: 2.4778\n",
      "3456: lr: 0.0100, train loss: 2.4531, val loss: 2.4690\n",
      "3457: lr: 0.0100, train loss: 2.4224, val loss: 2.4574\n",
      "3458: lr: 0.0100, train loss: 2.4551, val loss: 2.4822\n",
      "3459: lr: 0.0100, train loss: 2.4437, val loss: 2.4946\n",
      "3460: lr: 0.0100, train loss: 2.4560, val loss: 2.5219\n",
      "3461: lr: 0.0100, train loss: 2.4662, val loss: 2.4648\n",
      "3462: lr: 0.0100, train loss: 2.4267, val loss: 2.4738\n",
      "3463: lr: 0.0100, train loss: 2.4515, val loss: 2.4998\n",
      "3464: lr: 0.0100, train loss: 2.4662, val loss: 2.4552\n",
      "3465: lr: 0.0100, train loss: 2.4276, val loss: 2.5336\n",
      "3466: lr: 0.0100, train loss: 2.4283, val loss: 2.5353\n",
      "3467: lr: 0.0100, train loss: 2.4435, val loss: 2.5060\n",
      "3468: lr: 0.0100, train loss: 2.4642, val loss: 2.5025\n",
      "3469: lr: 0.0100, train loss: 2.4671, val loss: 2.4657\n",
      "3470: lr: 0.0100, train loss: 2.4828, val loss: 2.4498\n",
      "3471: lr: 0.0100, train loss: 2.4557, val loss: 2.5343\n",
      "3472: lr: 0.0100, train loss: 2.4742, val loss: 2.4962\n",
      "3473: lr: 0.0100, train loss: 2.4494, val loss: 2.4780\n",
      "3474: lr: 0.0100, train loss: 2.4643, val loss: 2.5239\n",
      "3475: lr: 0.0100, train loss: 2.4299, val loss: 2.5120\n",
      "3476: lr: 0.0100, train loss: 2.4573, val loss: 2.4848\n",
      "3477: lr: 0.0100, train loss: 2.4323, val loss: 2.5055\n",
      "3478: lr: 0.0100, train loss: 2.4194, val loss: 2.5166\n",
      "3479: lr: 0.0100, train loss: 2.4779, val loss: 2.4795\n",
      "3480: lr: 0.0100, train loss: 2.4809, val loss: 2.4988\n",
      "3481: lr: 0.0100, train loss: 2.4189, val loss: 2.5159\n",
      "3482: lr: 0.0100, train loss: 2.4510, val loss: 2.5555\n",
      "3483: lr: 0.0100, train loss: 2.4609, val loss: 2.5063\n",
      "3484: lr: 0.0100, train loss: 2.4824, val loss: 2.4412\n",
      "3485: lr: 0.0100, train loss: 2.4923, val loss: 2.5283\n",
      "3486: lr: 0.0100, train loss: 2.4127, val loss: 2.4680\n",
      "3487: lr: 0.0100, train loss: 2.4469, val loss: 2.4778\n",
      "3488: lr: 0.0100, train loss: 2.4659, val loss: 2.4803\n",
      "3489: lr: 0.0100, train loss: 2.4538, val loss: 2.5256\n",
      "3490: lr: 0.0100, train loss: 2.4498, val loss: 2.4680\n",
      "3491: lr: 0.0100, train loss: 2.4876, val loss: 2.4714\n",
      "3492: lr: 0.0100, train loss: 2.4484, val loss: 2.5268\n",
      "3493: lr: 0.0100, train loss: 2.4414, val loss: 2.4246\n",
      "3494: lr: 0.0100, train loss: 2.4742, val loss: 2.5040\n",
      "3495: lr: 0.0100, train loss: 2.4681, val loss: 2.5127\n",
      "3496: lr: 0.0100, train loss: 2.4945, val loss: 2.4223\n",
      "3497: lr: 0.0100, train loss: 2.4582, val loss: 2.5282\n",
      "3498: lr: 0.0100, train loss: 2.4457, val loss: 2.4999\n",
      "3499: lr: 0.0100, train loss: 2.4624, val loss: 2.4958\n",
      "3500: lr: 0.0100, train loss: 2.4299, val loss: 2.4756\n",
      "3501: lr: 0.0100, train loss: 2.4443, val loss: 2.4965\n",
      "3502: lr: 0.0100, train loss: 2.4643, val loss: 2.5037\n",
      "3503: lr: 0.0100, train loss: 2.5081, val loss: 2.4922\n",
      "3504: lr: 0.0100, train loss: 2.4419, val loss: 2.4736\n",
      "3505: lr: 0.0100, train loss: 2.4603, val loss: 2.4791\n",
      "3506: lr: 0.0100, train loss: 2.4547, val loss: 2.5132\n",
      "3507: lr: 0.0100, train loss: 2.4530, val loss: 2.4908\n",
      "3508: lr: 0.0100, train loss: 2.4843, val loss: 2.4598\n",
      "3509: lr: 0.0100, train loss: 2.4765, val loss: 2.4558\n",
      "3510: lr: 0.0100, train loss: 2.4577, val loss: 2.5012\n",
      "3511: lr: 0.0100, train loss: 2.4462, val loss: 2.4401\n",
      "3512: lr: 0.0100, train loss: 2.4795, val loss: 2.5177\n",
      "3513: lr: 0.0100, train loss: 2.4476, val loss: 2.4771\n",
      "3514: lr: 0.0100, train loss: 2.4574, val loss: 2.5544\n",
      "3515: lr: 0.0100, train loss: 2.4422, val loss: 2.5201\n",
      "3516: lr: 0.0100, train loss: 2.4619, val loss: 2.4629\n",
      "3517: lr: 0.0100, train loss: 2.4751, val loss: 2.4736\n",
      "3518: lr: 0.0100, train loss: 2.4243, val loss: 2.4650\n",
      "3519: lr: 0.0100, train loss: 2.5035, val loss: 2.4990\n",
      "3520: lr: 0.0100, train loss: 2.4574, val loss: 2.4520\n",
      "3521: lr: 0.0100, train loss: 2.4689, val loss: 2.4787\n",
      "3522: lr: 0.0100, train loss: 2.4413, val loss: 2.5045\n",
      "3523: lr: 0.0100, train loss: 2.4421, val loss: 2.4916\n",
      "3524: lr: 0.0100, train loss: 2.4407, val loss: 2.4981\n",
      "3525: lr: 0.0100, train loss: 2.4571, val loss: 2.5092\n",
      "3526: lr: 0.0100, train loss: 2.4526, val loss: 2.4896\n",
      "3527: lr: 0.0100, train loss: 2.4690, val loss: 2.4918\n",
      "3528: lr: 0.0100, train loss: 2.4476, val loss: 2.4688\n",
      "3529: lr: 0.0100, train loss: 2.4728, val loss: 2.4983\n",
      "3530: lr: 0.0100, train loss: 2.4637, val loss: 2.5086\n",
      "3531: lr: 0.0100, train loss: 2.4488, val loss: 2.5270\n",
      "3532: lr: 0.0100, train loss: 2.4714, val loss: 2.4750\n",
      "3533: lr: 0.0100, train loss: 2.4432, val loss: 2.4894\n",
      "3534: lr: 0.0100, train loss: 2.4424, val loss: 2.5241\n",
      "3535: lr: 0.0100, train loss: 2.4285, val loss: 2.4753\n",
      "3536: lr: 0.0100, train loss: 2.4951, val loss: 2.4561\n",
      "3537: lr: 0.0100, train loss: 2.4887, val loss: 2.4588\n",
      "3538: lr: 0.0100, train loss: 2.4699, val loss: 2.5288\n",
      "3539: lr: 0.0100, train loss: 2.4610, val loss: 2.4483\n",
      "3540: lr: 0.0100, train loss: 2.4788, val loss: 2.4911\n",
      "3541: lr: 0.0100, train loss: 2.4789, val loss: 2.4938\n",
      "3542: lr: 0.0100, train loss: 2.4515, val loss: 2.4871\n",
      "3543: lr: 0.0100, train loss: 2.4340, val loss: 2.4753\n",
      "3544: lr: 0.0100, train loss: 2.4223, val loss: 2.5412\n",
      "3545: lr: 0.0100, train loss: 2.4624, val loss: 2.4679\n",
      "3546: lr: 0.0100, train loss: 2.5009, val loss: 2.4822\n",
      "3547: lr: 0.0100, train loss: 2.4457, val loss: 2.4333\n",
      "3548: lr: 0.0100, train loss: 2.4401, val loss: 2.4208\n",
      "3549: lr: 0.0100, train loss: 2.4152, val loss: 2.4540\n",
      "3550: lr: 0.0100, train loss: 2.4567, val loss: 2.4993\n",
      "3551: lr: 0.0100, train loss: 2.4041, val loss: 2.5199\n",
      "3552: lr: 0.0100, train loss: 2.4806, val loss: 2.4669\n",
      "3553: lr: 0.0100, train loss: 2.4280, val loss: 2.5476\n",
      "3554: lr: 0.0100, train loss: 2.4582, val loss: 2.4791\n",
      "3555: lr: 0.0100, train loss: 2.4573, val loss: 2.5308\n",
      "3556: lr: 0.0100, train loss: 2.4480, val loss: 2.5137\n",
      "3557: lr: 0.0100, train loss: 2.4469, val loss: 2.4826\n",
      "3558: lr: 0.0100, train loss: 2.4399, val loss: 2.4937\n",
      "3559: lr: 0.0100, train loss: 2.4872, val loss: 2.4517\n",
      "3560: lr: 0.0100, train loss: 2.4785, val loss: 2.4952\n",
      "3561: lr: 0.0100, train loss: 2.5000, val loss: 2.4562\n",
      "3562: lr: 0.0100, train loss: 2.4780, val loss: 2.4734\n",
      "3563: lr: 0.0100, train loss: 2.4416, val loss: 2.4840\n",
      "3564: lr: 0.0100, train loss: 2.4285, val loss: 2.5194\n",
      "3565: lr: 0.0100, train loss: 2.3707, val loss: 2.5192\n",
      "3566: lr: 0.0100, train loss: 2.4654, val loss: 2.4721\n",
      "3567: lr: 0.0100, train loss: 2.4756, val loss: 2.4937\n",
      "3568: lr: 0.0100, train loss: 2.4530, val loss: 2.4708\n",
      "3569: lr: 0.0100, train loss: 2.4727, val loss: 2.4566\n",
      "3570: lr: 0.0100, train loss: 2.4791, val loss: 2.4634\n",
      "3571: lr: 0.0100, train loss: 2.4062, val loss: 2.4690\n",
      "3572: lr: 0.0100, train loss: 2.4384, val loss: 2.4825\n",
      "3573: lr: 0.0100, train loss: 2.4369, val loss: 2.4900\n",
      "3574: lr: 0.0100, train loss: 2.4020, val loss: 2.5062\n",
      "3575: lr: 0.0100, train loss: 2.4639, val loss: 2.4786\n",
      "3576: lr: 0.0100, train loss: 2.4800, val loss: 2.5277\n",
      "3577: lr: 0.0100, train loss: 2.4566, val loss: 2.5123\n",
      "3578: lr: 0.0100, train loss: 2.4766, val loss: 2.4466\n",
      "3579: lr: 0.0100, train loss: 2.4601, val loss: 2.4888\n",
      "3580: lr: 0.0100, train loss: 2.5241, val loss: 2.4838\n",
      "3581: lr: 0.0100, train loss: 2.4624, val loss: 2.4749\n",
      "3582: lr: 0.0100, train loss: 2.4233, val loss: 2.4604\n",
      "3583: lr: 0.0100, train loss: 2.4387, val loss: 2.5348\n",
      "3584: lr: 0.0100, train loss: 2.4593, val loss: 2.4594\n",
      "3585: lr: 0.0100, train loss: 2.4465, val loss: 2.5136\n",
      "3586: lr: 0.0100, train loss: 2.4406, val loss: 2.4617\n",
      "3587: lr: 0.0100, train loss: 2.4691, val loss: 2.5198\n",
      "3588: lr: 0.0100, train loss: 2.4798, val loss: 2.4944\n",
      "3589: lr: 0.0100, train loss: 2.4526, val loss: 2.4644\n",
      "3590: lr: 0.0100, train loss: 2.4723, val loss: 2.5182\n",
      "3591: lr: 0.0100, train loss: 2.4406, val loss: 2.5224\n",
      "3592: lr: 0.0100, train loss: 2.5166, val loss: 2.4261\n",
      "3593: lr: 0.0100, train loss: 2.4311, val loss: 2.4629\n",
      "3594: lr: 0.0100, train loss: 2.4472, val loss: 2.4656\n",
      "3595: lr: 0.0100, train loss: 2.4712, val loss: 2.4755\n",
      "3596: lr: 0.0100, train loss: 2.4697, val loss: 2.4606\n",
      "3597: lr: 0.0100, train loss: 2.4737, val loss: 2.5062\n",
      "3598: lr: 0.0100, train loss: 2.4531, val loss: 2.4967\n",
      "3599: lr: 0.0100, train loss: 2.4713, val loss: 2.5141\n",
      "3600: lr: 0.0100, train loss: 2.4707, val loss: 2.5307\n",
      "3601: lr: 0.0100, train loss: 2.4800, val loss: 2.4955\n",
      "3602: lr: 0.0100, train loss: 2.4443, val loss: 2.4688\n",
      "3603: lr: 0.0100, train loss: 2.4413, val loss: 2.4765\n",
      "3604: lr: 0.0100, train loss: 2.4217, val loss: 2.5185\n",
      "3605: lr: 0.0100, train loss: 2.4462, val loss: 2.5096\n",
      "3606: lr: 0.0100, train loss: 2.4814, val loss: 2.5013\n",
      "3607: lr: 0.0100, train loss: 2.4824, val loss: 2.5444\n",
      "3608: lr: 0.0100, train loss: 2.4857, val loss: 2.4332\n",
      "3609: lr: 0.0100, train loss: 2.4467, val loss: 2.4773\n",
      "3610: lr: 0.0100, train loss: 2.4665, val loss: 2.4666\n",
      "3611: lr: 0.0100, train loss: 2.4353, val loss: 2.4759\n",
      "3612: lr: 0.0100, train loss: 2.4474, val loss: 2.5068\n",
      "3613: lr: 0.0100, train loss: 2.5195, val loss: 2.4621\n",
      "3614: lr: 0.0100, train loss: 2.4400, val loss: 2.5168\n",
      "3615: lr: 0.0100, train loss: 2.4253, val loss: 2.5013\n",
      "3616: lr: 0.0100, train loss: 2.4734, val loss: 2.4834\n",
      "3617: lr: 0.0100, train loss: 2.4383, val loss: 2.5297\n",
      "3618: lr: 0.0100, train loss: 2.4681, val loss: 2.4870\n",
      "3619: lr: 0.0100, train loss: 2.4728, val loss: 2.4859\n",
      "3620: lr: 0.0100, train loss: 2.4605, val loss: 2.5263\n",
      "3621: lr: 0.0100, train loss: 2.4243, val loss: 2.4436\n",
      "3622: lr: 0.0100, train loss: 2.4871, val loss: 2.5219\n",
      "3623: lr: 0.0100, train loss: 2.4449, val loss: 2.4277\n",
      "3624: lr: 0.0100, train loss: 2.4456, val loss: 2.4766\n",
      "3625: lr: 0.0100, train loss: 2.4893, val loss: 2.4162\n",
      "3626: lr: 0.0100, train loss: 2.5023, val loss: 2.4913\n",
      "3627: lr: 0.0100, train loss: 2.4778, val loss: 2.5674\n",
      "3628: lr: 0.0100, train loss: 2.4735, val loss: 2.4850\n",
      "3629: lr: 0.0100, train loss: 2.4211, val loss: 2.5122\n",
      "3630: lr: 0.0100, train loss: 2.4538, val loss: 2.4608\n",
      "3631: lr: 0.0100, train loss: 2.4503, val loss: 2.5344\n",
      "3632: lr: 0.0100, train loss: 2.4154, val loss: 2.4879\n",
      "3633: lr: 0.0100, train loss: 2.4542, val loss: 2.4520\n",
      "3634: lr: 0.0100, train loss: 2.4579, val loss: 2.4740\n",
      "3635: lr: 0.0100, train loss: 2.4280, val loss: 2.4438\n",
      "3636: lr: 0.0100, train loss: 2.4444, val loss: 2.5130\n",
      "3637: lr: 0.0100, train loss: 2.4408, val loss: 2.4665\n",
      "3638: lr: 0.0100, train loss: 2.4371, val loss: 2.5057\n",
      "3639: lr: 0.0100, train loss: 2.4227, val loss: 2.4580\n",
      "3640: lr: 0.0100, train loss: 2.4503, val loss: 2.4511\n",
      "3641: lr: 0.0100, train loss: 2.4469, val loss: 2.5185\n",
      "3642: lr: 0.0100, train loss: 2.4814, val loss: 2.4236\n",
      "3643: lr: 0.0100, train loss: 2.4615, val loss: 2.4601\n",
      "3644: lr: 0.0100, train loss: 2.4513, val loss: 2.5363\n",
      "3645: lr: 0.0100, train loss: 2.4856, val loss: 2.5107\n",
      "3646: lr: 0.0100, train loss: 2.4406, val loss: 2.5641\n",
      "3647: lr: 0.0100, train loss: 2.4591, val loss: 2.5001\n",
      "3648: lr: 0.0100, train loss: 2.4909, val loss: 2.4481\n",
      "3649: lr: 0.0100, train loss: 2.4731, val loss: 2.5626\n",
      "3650: lr: 0.0100, train loss: 2.4710, val loss: 2.4860\n",
      "3651: lr: 0.0100, train loss: 2.4502, val loss: 2.4972\n",
      "3652: lr: 0.0100, train loss: 2.4821, val loss: 2.4689\n",
      "3653: lr: 0.0100, train loss: 2.4583, val loss: 2.4652\n",
      "3654: lr: 0.0100, train loss: 2.4485, val loss: 2.4669\n",
      "3655: lr: 0.0100, train loss: 2.4630, val loss: 2.4911\n",
      "3656: lr: 0.0100, train loss: 2.4594, val loss: 2.5241\n",
      "3657: lr: 0.0100, train loss: 2.4958, val loss: 2.5086\n",
      "3658: lr: 0.0100, train loss: 2.5024, val loss: 2.5242\n",
      "3659: lr: 0.0100, train loss: 2.4895, val loss: 2.4650\n",
      "3660: lr: 0.0100, train loss: 2.4710, val loss: 2.4482\n",
      "3661: lr: 0.0100, train loss: 2.4580, val loss: 2.4852\n",
      "3662: lr: 0.0100, train loss: 2.4436, val loss: 2.4854\n",
      "3663: lr: 0.0100, train loss: 2.4813, val loss: 2.4951\n",
      "3664: lr: 0.0100, train loss: 2.4398, val loss: 2.5010\n",
      "3665: lr: 0.0100, train loss: 2.4655, val loss: 2.4516\n",
      "3666: lr: 0.0100, train loss: 2.4217, val loss: 2.4405\n",
      "3667: lr: 0.0100, train loss: 2.4378, val loss: 2.4792\n",
      "3668: lr: 0.0100, train loss: 2.4974, val loss: 2.4092\n",
      "3669: lr: 0.0100, train loss: 2.4522, val loss: 2.4754\n",
      "3670: lr: 0.0100, train loss: 2.4608, val loss: 2.5400\n",
      "3671: lr: 0.0100, train loss: 2.4871, val loss: 2.5119\n",
      "3672: lr: 0.0100, train loss: 2.4885, val loss: 2.5200\n",
      "3673: lr: 0.0100, train loss: 2.4435, val loss: 2.4830\n",
      "3674: lr: 0.0100, train loss: 2.4671, val loss: 2.4556\n",
      "3675: lr: 0.0100, train loss: 2.4515, val loss: 2.4527\n",
      "3676: lr: 0.0100, train loss: 2.4580, val loss: 2.5012\n",
      "3677: lr: 0.0100, train loss: 2.4209, val loss: 2.4816\n",
      "3678: lr: 0.0100, train loss: 2.5301, val loss: 2.4974\n",
      "3679: lr: 0.0100, train loss: 2.4368, val loss: 2.4889\n",
      "3680: lr: 0.0100, train loss: 2.4556, val loss: 2.4907\n",
      "3681: lr: 0.0100, train loss: 2.5022, val loss: 2.5054\n",
      "3682: lr: 0.0100, train loss: 2.4559, val loss: 2.4503\n",
      "3683: lr: 0.0100, train loss: 2.4401, val loss: 2.4001\n",
      "3684: lr: 0.0100, train loss: 2.4575, val loss: 2.5622\n",
      "3685: lr: 0.0100, train loss: 2.4621, val loss: 2.4924\n",
      "3686: lr: 0.0100, train loss: 2.4899, val loss: 2.4878\n",
      "3687: lr: 0.0100, train loss: 2.5008, val loss: 2.5037\n",
      "3688: lr: 0.0100, train loss: 2.4632, val loss: 2.4823\n",
      "3689: lr: 0.0100, train loss: 2.4549, val loss: 2.5187\n",
      "3690: lr: 0.0100, train loss: 2.4944, val loss: 2.4651\n",
      "3691: lr: 0.0100, train loss: 2.4597, val loss: 2.4714\n",
      "3692: lr: 0.0100, train loss: 2.4686, val loss: 2.4854\n",
      "3693: lr: 0.0100, train loss: 2.4603, val loss: 2.4312\n",
      "3694: lr: 0.0100, train loss: 2.4677, val loss: 2.4683\n",
      "3695: lr: 0.0100, train loss: 2.4555, val loss: 2.4757\n",
      "3696: lr: 0.0100, train loss: 2.4502, val loss: 2.5319\n",
      "3697: lr: 0.0100, train loss: 2.4716, val loss: 2.4707\n",
      "3698: lr: 0.0100, train loss: 2.4789, val loss: 2.4855\n",
      "3699: lr: 0.0100, train loss: 2.5184, val loss: 2.4832\n",
      "3700: lr: 0.0100, train loss: 2.4558, val loss: 2.4589\n",
      "3701: lr: 0.0100, train loss: 2.4531, val loss: 2.4873\n",
      "3702: lr: 0.0100, train loss: 2.4449, val loss: 2.5081\n",
      "3703: lr: 0.0100, train loss: 2.4590, val loss: 2.4945\n",
      "3704: lr: 0.0100, train loss: 2.4871, val loss: 2.5028\n",
      "3705: lr: 0.0100, train loss: 2.4406, val loss: 2.5286\n",
      "3706: lr: 0.0100, train loss: 2.4433, val loss: 2.4905\n",
      "3707: lr: 0.0100, train loss: 2.4649, val loss: 2.4629\n",
      "3708: lr: 0.0100, train loss: 2.4400, val loss: 2.4726\n",
      "3709: lr: 0.0100, train loss: 2.4419, val loss: 2.4655\n",
      "3710: lr: 0.0100, train loss: 2.4254, val loss: 2.5365\n",
      "3711: lr: 0.0100, train loss: 2.4466, val loss: 2.5345\n",
      "3712: lr: 0.0100, train loss: 2.5054, val loss: 2.5425\n",
      "3713: lr: 0.0100, train loss: 2.4621, val loss: 2.4569\n",
      "3714: lr: 0.0100, train loss: 2.4196, val loss: 2.4652\n",
      "3715: lr: 0.0100, train loss: 2.4316, val loss: 2.4348\n",
      "3716: lr: 0.0100, train loss: 2.4775, val loss: 2.4986\n",
      "3717: lr: 0.0100, train loss: 2.4385, val loss: 2.5071\n",
      "3718: lr: 0.0100, train loss: 2.4563, val loss: 2.4490\n",
      "3719: lr: 0.0100, train loss: 2.4497, val loss: 2.5209\n",
      "3720: lr: 0.0100, train loss: 2.4874, val loss: 2.5045\n",
      "3721: lr: 0.0100, train loss: 2.4607, val loss: 2.5008\n",
      "3722: lr: 0.0100, train loss: 2.4805, val loss: 2.4690\n",
      "3723: lr: 0.0100, train loss: 2.4709, val loss: 2.4261\n",
      "3724: lr: 0.0100, train loss: 2.5207, val loss: 2.5483\n",
      "3725: lr: 0.0100, train loss: 2.4476, val loss: 2.4744\n",
      "3726: lr: 0.0100, train loss: 2.4380, val loss: 2.5245\n",
      "3727: lr: 0.0100, train loss: 2.5511, val loss: 2.4920\n",
      "3728: lr: 0.0100, train loss: 2.4455, val loss: 2.5105\n",
      "3729: lr: 0.0100, train loss: 2.4779, val loss: 2.5107\n",
      "3730: lr: 0.0100, train loss: 2.3987, val loss: 2.5267\n",
      "3731: lr: 0.0100, train loss: 2.4592, val loss: 2.5208\n",
      "3732: lr: 0.0100, train loss: 2.4441, val loss: 2.4744\n",
      "3733: lr: 0.0100, train loss: 2.4464, val loss: 2.4917\n",
      "3734: lr: 0.0100, train loss: 2.4667, val loss: 2.4434\n",
      "3735: lr: 0.0100, train loss: 2.4708, val loss: 2.4395\n",
      "3736: lr: 0.0100, train loss: 2.4871, val loss: 2.5258\n",
      "3737: lr: 0.0100, train loss: 2.4751, val loss: 2.4807\n",
      "3738: lr: 0.0100, train loss: 2.4267, val loss: 2.4467\n",
      "3739: lr: 0.0100, train loss: 2.4470, val loss: 2.5063\n",
      "3740: lr: 0.0100, train loss: 2.4750, val loss: 2.4939\n",
      "3741: lr: 0.0100, train loss: 2.4633, val loss: 2.5247\n",
      "3742: lr: 0.0100, train loss: 2.4646, val loss: 2.5386\n",
      "3743: lr: 0.0100, train loss: 2.4737, val loss: 2.4773\n",
      "3744: lr: 0.0100, train loss: 2.4468, val loss: 2.4879\n",
      "3745: lr: 0.0100, train loss: 2.4510, val loss: 2.4949\n",
      "3746: lr: 0.0100, train loss: 2.4870, val loss: 2.4699\n",
      "3747: lr: 0.0100, train loss: 2.4815, val loss: 2.5267\n",
      "3748: lr: 0.0100, train loss: 2.4713, val loss: 2.4572\n",
      "3749: lr: 0.0100, train loss: 2.4342, val loss: 2.5177\n",
      "3750: lr: 0.0100, train loss: 2.4146, val loss: 2.5060\n",
      "3751: lr: 0.0100, train loss: 2.4306, val loss: 2.5117\n",
      "3752: lr: 0.0100, train loss: 2.5061, val loss: 2.4943\n",
      "3753: lr: 0.0100, train loss: 2.4318, val loss: 2.5001\n",
      "3754: lr: 0.0100, train loss: 2.5384, val loss: 2.5141\n",
      "3755: lr: 0.0100, train loss: 2.4826, val loss: 2.5514\n",
      "3756: lr: 0.0100, train loss: 2.4932, val loss: 2.4845\n",
      "3757: lr: 0.0100, train loss: 2.5083, val loss: 2.4470\n",
      "3758: lr: 0.0100, train loss: 2.4411, val loss: 2.4786\n",
      "3759: lr: 0.0100, train loss: 2.4655, val loss: 2.5342\n",
      "3760: lr: 0.0100, train loss: 2.4954, val loss: 2.4853\n",
      "3761: lr: 0.0100, train loss: 2.4723, val loss: 2.5255\n",
      "3762: lr: 0.0100, train loss: 2.4504, val loss: 2.4999\n",
      "3763: lr: 0.0100, train loss: 2.4422, val loss: 2.5080\n",
      "3764: lr: 0.0100, train loss: 2.4649, val loss: 2.4774\n",
      "3765: lr: 0.0100, train loss: 2.4621, val loss: 2.5417\n",
      "3766: lr: 0.0100, train loss: 2.4472, val loss: 2.4596\n",
      "3767: lr: 0.0100, train loss: 2.4710, val loss: 2.5023\n",
      "3768: lr: 0.0100, train loss: 2.4472, val loss: 2.5781\n",
      "3769: lr: 0.0100, train loss: 2.4540, val loss: 2.5171\n",
      "3770: lr: 0.0100, train loss: 2.5021, val loss: 2.4581\n",
      "3771: lr: 0.0100, train loss: 2.4870, val loss: 2.5337\n",
      "3772: lr: 0.0100, train loss: 2.4174, val loss: 2.4728\n",
      "3773: lr: 0.0100, train loss: 2.4617, val loss: 2.4882\n",
      "3774: lr: 0.0100, train loss: 2.4843, val loss: 2.4946\n",
      "3775: lr: 0.0100, train loss: 2.4459, val loss: 2.4183\n",
      "3776: lr: 0.0100, train loss: 2.4595, val loss: 2.5489\n",
      "3777: lr: 0.0100, train loss: 2.4737, val loss: 2.5035\n",
      "3778: lr: 0.0100, train loss: 2.4685, val loss: 2.4704\n",
      "3779: lr: 0.0100, train loss: 2.4541, val loss: 2.4940\n",
      "3780: lr: 0.0100, train loss: 2.4339, val loss: 2.4841\n",
      "3781: lr: 0.0100, train loss: 2.4713, val loss: 2.5010\n",
      "3782: lr: 0.0100, train loss: 2.4514, val loss: 2.5034\n",
      "3783: lr: 0.0100, train loss: 2.4898, val loss: 2.4795\n",
      "3784: lr: 0.0100, train loss: 2.4715, val loss: 2.4798\n",
      "3785: lr: 0.0100, train loss: 2.4391, val loss: 2.4948\n",
      "3786: lr: 0.0100, train loss: 2.4810, val loss: 2.4852\n",
      "3787: lr: 0.0100, train loss: 2.4733, val loss: 2.5197\n",
      "3788: lr: 0.0100, train loss: 2.4759, val loss: 2.4916\n",
      "3789: lr: 0.0100, train loss: 2.4663, val loss: 2.4845\n",
      "3790: lr: 0.0100, train loss: 2.4712, val loss: 2.5049\n",
      "3791: lr: 0.0100, train loss: 2.4441, val loss: 2.4988\n",
      "3792: lr: 0.0100, train loss: 2.4320, val loss: 2.5554\n",
      "3793: lr: 0.0100, train loss: 2.4274, val loss: 2.4871\n",
      "3794: lr: 0.0100, train loss: 2.4384, val loss: 2.4655\n",
      "3795: lr: 0.0100, train loss: 2.4572, val loss: 2.4598\n",
      "3796: lr: 0.0100, train loss: 2.4369, val loss: 2.4561\n",
      "3797: lr: 0.0100, train loss: 2.4319, val loss: 2.4819\n",
      "3798: lr: 0.0100, train loss: 2.4784, val loss: 2.4691\n",
      "3799: lr: 0.0100, train loss: 2.4773, val loss: 2.5000\n",
      "3800: lr: 0.0100, train loss: 2.5074, val loss: 2.5241\n",
      "3801: lr: 0.0100, train loss: 2.4481, val loss: 2.4660\n",
      "3802: lr: 0.0100, train loss: 2.5111, val loss: 2.4701\n",
      "3803: lr: 0.0100, train loss: 2.4657, val loss: 2.4782\n",
      "3804: lr: 0.0100, train loss: 2.4897, val loss: 2.4828\n",
      "3805: lr: 0.0100, train loss: 2.4232, val loss: 2.4958\n",
      "3806: lr: 0.0100, train loss: 2.4880, val loss: 2.5026\n",
      "3807: lr: 0.0100, train loss: 2.4363, val loss: 2.4913\n",
      "3808: lr: 0.0100, train loss: 2.4183, val loss: 2.4551\n",
      "3809: lr: 0.0100, train loss: 2.4197, val loss: 2.4847\n",
      "3810: lr: 0.0100, train loss: 2.4517, val loss: 2.4566\n",
      "3811: lr: 0.0100, train loss: 2.4563, val loss: 2.4564\n",
      "3812: lr: 0.0100, train loss: 2.4785, val loss: 2.5188\n",
      "3813: lr: 0.0100, train loss: 2.4463, val loss: 2.4852\n",
      "3814: lr: 0.0100, train loss: 2.4609, val loss: 2.4910\n",
      "3815: lr: 0.0100, train loss: 2.4952, val loss: 2.5134\n",
      "3816: lr: 0.0100, train loss: 2.4702, val loss: 2.4992\n",
      "3817: lr: 0.0100, train loss: 2.4668, val loss: 2.5015\n",
      "3818: lr: 0.0100, train loss: 2.4253, val loss: 2.4293\n",
      "3819: lr: 0.0100, train loss: 2.4943, val loss: 2.4791\n",
      "3820: lr: 0.0100, train loss: 2.4744, val loss: 2.5109\n",
      "3821: lr: 0.0100, train loss: 2.4923, val loss: 2.4989\n",
      "3822: lr: 0.0100, train loss: 2.4555, val loss: 2.5357\n",
      "3823: lr: 0.0100, train loss: 2.4370, val loss: 2.4584\n",
      "3824: lr: 0.0100, train loss: 2.4134, val loss: 2.4498\n",
      "3825: lr: 0.0100, train loss: 2.4771, val loss: 2.5639\n",
      "3826: lr: 0.0100, train loss: 2.4660, val loss: 2.4455\n",
      "3827: lr: 0.0100, train loss: 2.4476, val loss: 2.4318\n",
      "3828: lr: 0.0100, train loss: 2.4349, val loss: 2.4657\n",
      "3829: lr: 0.0100, train loss: 2.4925, val loss: 2.5285\n",
      "3830: lr: 0.0100, train loss: 2.4528, val loss: 2.4593\n",
      "3831: lr: 0.0100, train loss: 2.4194, val loss: 2.4577\n",
      "3832: lr: 0.0100, train loss: 2.4479, val loss: 2.4829\n",
      "3833: lr: 0.0100, train loss: 2.4736, val loss: 2.4823\n",
      "3834: lr: 0.0100, train loss: 2.4584, val loss: 2.4629\n",
      "3835: lr: 0.0100, train loss: 2.4514, val loss: 2.4379\n",
      "3836: lr: 0.0100, train loss: 2.4692, val loss: 2.4909\n",
      "3837: lr: 0.0100, train loss: 2.4274, val loss: 2.4772\n",
      "3838: lr: 0.0100, train loss: 2.4985, val loss: 2.5229\n",
      "3839: lr: 0.0100, train loss: 2.4903, val loss: 2.4849\n",
      "3840: lr: 0.0100, train loss: 2.4674, val loss: 2.5208\n",
      "3841: lr: 0.0100, train loss: 2.4693, val loss: 2.4782\n",
      "3842: lr: 0.0100, train loss: 2.4788, val loss: 2.4981\n",
      "3843: lr: 0.0100, train loss: 2.4310, val loss: 2.4628\n",
      "3844: lr: 0.0100, train loss: 2.4838, val loss: 2.4956\n",
      "3845: lr: 0.0100, train loss: 2.4507, val loss: 2.4756\n",
      "3846: lr: 0.0100, train loss: 2.4754, val loss: 2.4846\n",
      "3847: lr: 0.0100, train loss: 2.4673, val loss: 2.4929\n",
      "3848: lr: 0.0100, train loss: 2.4365, val loss: 2.4512\n",
      "3849: lr: 0.0100, train loss: 2.5086, val loss: 2.5002\n",
      "3850: lr: 0.0100, train loss: 2.4977, val loss: 2.4943\n",
      "3851: lr: 0.0100, train loss: 2.4556, val loss: 2.4578\n",
      "3852: lr: 0.0100, train loss: 2.4673, val loss: 2.5052\n",
      "3853: lr: 0.0100, train loss: 2.4989, val loss: 2.5360\n",
      "3854: lr: 0.0100, train loss: 2.4464, val loss: 2.4628\n",
      "3855: lr: 0.0100, train loss: 2.5119, val loss: 2.5005\n",
      "3856: lr: 0.0100, train loss: 2.4871, val loss: 2.5144\n",
      "3857: lr: 0.0100, train loss: 2.4688, val loss: 2.4403\n",
      "3858: lr: 0.0100, train loss: 2.4934, val loss: 2.5206\n",
      "3859: lr: 0.0100, train loss: 2.4555, val loss: 2.4424\n",
      "3860: lr: 0.0100, train loss: 2.4320, val loss: 2.4493\n",
      "3861: lr: 0.0100, train loss: 2.4790, val loss: 2.4804\n",
      "3862: lr: 0.0100, train loss: 2.4788, val loss: 2.5146\n",
      "3863: lr: 0.0100, train loss: 2.4254, val loss: 2.4582\n",
      "3864: lr: 0.0100, train loss: 2.4223, val loss: 2.4763\n",
      "3865: lr: 0.0100, train loss: 2.4705, val loss: 2.5031\n",
      "3866: lr: 0.0100, train loss: 2.4588, val loss: 2.4761\n",
      "3867: lr: 0.0100, train loss: 2.4211, val loss: 2.4494\n",
      "3868: lr: 0.0100, train loss: 2.4642, val loss: 2.4127\n",
      "3869: lr: 0.0100, train loss: 2.4253, val loss: 2.4635\n",
      "3870: lr: 0.0100, train loss: 2.4224, val loss: 2.4721\n",
      "3871: lr: 0.0100, train loss: 2.4638, val loss: 2.4084\n",
      "3872: lr: 0.0100, train loss: 2.4306, val loss: 2.5245\n",
      "3873: lr: 0.0100, train loss: 2.4605, val loss: 2.4209\n",
      "3874: lr: 0.0100, train loss: 2.4374, val loss: 2.4792\n",
      "3875: lr: 0.0100, train loss: 2.4821, val loss: 2.4739\n",
      "3876: lr: 0.0100, train loss: 2.4537, val loss: 2.4909\n",
      "3877: lr: 0.0100, train loss: 2.4434, val loss: 2.4545\n",
      "3878: lr: 0.0100, train loss: 2.4524, val loss: 2.5345\n",
      "3879: lr: 0.0100, train loss: 2.4066, val loss: 2.4992\n",
      "3880: lr: 0.0100, train loss: 2.5120, val loss: 2.4935\n",
      "3881: lr: 0.0100, train loss: 2.4317, val loss: 2.4325\n",
      "3882: lr: 0.0100, train loss: 2.4876, val loss: 2.5081\n",
      "3883: lr: 0.0100, train loss: 2.4672, val loss: 2.4756\n",
      "3884: lr: 0.0100, train loss: 2.4849, val loss: 2.5038\n",
      "3885: lr: 0.0100, train loss: 2.4958, val loss: 2.4594\n",
      "3886: lr: 0.0100, train loss: 2.4537, val loss: 2.5180\n",
      "3887: lr: 0.0100, train loss: 2.4400, val loss: 2.5350\n",
      "3888: lr: 0.0100, train loss: 2.4505, val loss: 2.4662\n",
      "3889: lr: 0.0100, train loss: 2.4459, val loss: 2.4259\n",
      "3890: lr: 0.0100, train loss: 2.4979, val loss: 2.5175\n",
      "3891: lr: 0.0100, train loss: 2.4652, val loss: 2.4837\n",
      "3892: lr: 0.0100, train loss: 2.4202, val loss: 2.4693\n",
      "3893: lr: 0.0100, train loss: 2.4665, val loss: 2.5092\n",
      "3894: lr: 0.0100, train loss: 2.4919, val loss: 2.4640\n",
      "3895: lr: 0.0100, train loss: 2.4434, val loss: 2.4986\n",
      "3896: lr: 0.0100, train loss: 2.4487, val loss: 2.4630\n",
      "3897: lr: 0.0100, train loss: 2.4376, val loss: 2.5157\n",
      "3898: lr: 0.0100, train loss: 2.4133, val loss: 2.5037\n",
      "3899: lr: 0.0100, train loss: 2.4681, val loss: 2.4986\n",
      "3900: lr: 0.0100, train loss: 2.4527, val loss: 2.5022\n",
      "3901: lr: 0.0100, train loss: 2.4842, val loss: 2.4844\n",
      "3902: lr: 0.0100, train loss: 2.4200, val loss: 2.4877\n",
      "3903: lr: 0.0100, train loss: 2.4492, val loss: 2.5103\n",
      "3904: lr: 0.0100, train loss: 2.4429, val loss: 2.4813\n",
      "3905: lr: 0.0100, train loss: 2.5018, val loss: 2.4620\n",
      "3906: lr: 0.0100, train loss: 2.4301, val loss: 2.5371\n",
      "3907: lr: 0.0100, train loss: 2.4912, val loss: 2.4878\n",
      "3908: lr: 0.0100, train loss: 2.4675, val loss: 2.4958\n",
      "3909: lr: 0.0100, train loss: 2.4815, val loss: 2.4664\n",
      "3910: lr: 0.0100, train loss: 2.5045, val loss: 2.4617\n",
      "3911: lr: 0.0100, train loss: 2.5111, val loss: 2.4813\n",
      "3912: lr: 0.0100, train loss: 2.4433, val loss: 2.4558\n",
      "3913: lr: 0.0100, train loss: 2.4606, val loss: 2.4891\n",
      "3914: lr: 0.0100, train loss: 2.5020, val loss: 2.4472\n",
      "3915: lr: 0.0100, train loss: 2.4515, val loss: 2.5414\n",
      "3916: lr: 0.0100, train loss: 2.4996, val loss: 2.4908\n",
      "3917: lr: 0.0100, train loss: 2.4108, val loss: 2.5010\n",
      "3918: lr: 0.0100, train loss: 2.4757, val loss: 2.4902\n",
      "3919: lr: 0.0100, train loss: 2.4501, val loss: 2.5618\n",
      "3920: lr: 0.0100, train loss: 2.4281, val loss: 2.4863\n",
      "3921: lr: 0.0100, train loss: 2.4581, val loss: 2.4818\n",
      "3922: lr: 0.0100, train loss: 2.4305, val loss: 2.5388\n",
      "3923: lr: 0.0100, train loss: 2.4468, val loss: 2.5067\n",
      "3924: lr: 0.0100, train loss: 2.4557, val loss: 2.4732\n",
      "3925: lr: 0.0100, train loss: 2.4558, val loss: 2.4900\n",
      "3926: lr: 0.0100, train loss: 2.5161, val loss: 2.4999\n",
      "3927: lr: 0.0100, train loss: 2.4926, val loss: 2.4942\n",
      "3928: lr: 0.0100, train loss: 2.4721, val loss: 2.4819\n",
      "3929: lr: 0.0100, train loss: 2.4936, val loss: 2.4928\n",
      "3930: lr: 0.0100, train loss: 2.4500, val loss: 2.4437\n",
      "3931: lr: 0.0100, train loss: 2.4617, val loss: 2.5061\n",
      "3932: lr: 0.0100, train loss: 2.4668, val loss: 2.4428\n",
      "3933: lr: 0.0100, train loss: 2.4973, val loss: 2.4844\n",
      "3934: lr: 0.0100, train loss: 2.4652, val loss: 2.5360\n",
      "3935: lr: 0.0100, train loss: 2.4706, val loss: 2.4898\n",
      "3936: lr: 0.0100, train loss: 2.4614, val loss: 2.5042\n",
      "3937: lr: 0.0100, train loss: 2.4727, val loss: 2.5064\n",
      "3938: lr: 0.0100, train loss: 2.5113, val loss: 2.4778\n",
      "3939: lr: 0.0100, train loss: 2.4631, val loss: 2.4540\n",
      "3940: lr: 0.0100, train loss: 2.4446, val loss: 2.5164\n",
      "3941: lr: 0.0100, train loss: 2.4790, val loss: 2.5653\n",
      "3942: lr: 0.0100, train loss: 2.4623, val loss: 2.4988\n",
      "3943: lr: 0.0100, train loss: 2.4414, val loss: 2.5337\n",
      "3944: lr: 0.0100, train loss: 2.4718, val loss: 2.4558\n",
      "3945: lr: 0.0100, train loss: 2.4951, val loss: 2.4089\n",
      "3946: lr: 0.0100, train loss: 2.4717, val loss: 2.5077\n",
      "3947: lr: 0.0100, train loss: 2.4335, val loss: 2.4623\n",
      "3948: lr: 0.0100, train loss: 2.4028, val loss: 2.5688\n",
      "3949: lr: 0.0100, train loss: 2.4410, val loss: 2.5400\n",
      "3950: lr: 0.0100, train loss: 2.4359, val loss: 2.5276\n",
      "3951: lr: 0.0100, train loss: 2.4603, val loss: 2.4966\n",
      "3952: lr: 0.0100, train loss: 2.4791, val loss: 2.5241\n",
      "3953: lr: 0.0100, train loss: 2.4769, val loss: 2.4203\n",
      "3954: lr: 0.0100, train loss: 2.4450, val loss: 2.4422\n",
      "3955: lr: 0.0100, train loss: 2.4599, val loss: 2.4959\n",
      "3956: lr: 0.0100, train loss: 2.4697, val loss: 2.4267\n",
      "3957: lr: 0.0100, train loss: 2.4911, val loss: 2.4769\n",
      "3958: lr: 0.0100, train loss: 2.4981, val loss: 2.4717\n",
      "3959: lr: 0.0100, train loss: 2.4839, val loss: 2.4790\n",
      "3960: lr: 0.0100, train loss: 2.4319, val loss: 2.4812\n",
      "3961: lr: 0.0100, train loss: 2.4606, val loss: 2.5061\n",
      "3962: lr: 0.0100, train loss: 2.4896, val loss: 2.4632\n",
      "3963: lr: 0.0100, train loss: 2.4413, val loss: 2.5347\n",
      "3964: lr: 0.0100, train loss: 2.4343, val loss: 2.5072\n",
      "3965: lr: 0.0100, train loss: 2.4524, val loss: 2.5165\n",
      "3966: lr: 0.0100, train loss: 2.4813, val loss: 2.4536\n",
      "3967: lr: 0.0100, train loss: 2.4762, val loss: 2.4573\n",
      "3968: lr: 0.0100, train loss: 2.4665, val loss: 2.5321\n",
      "3969: lr: 0.0100, train loss: 2.4142, val loss: 2.4625\n",
      "3970: lr: 0.0100, train loss: 2.4598, val loss: 2.4478\n",
      "3971: lr: 0.0100, train loss: 2.4521, val loss: 2.4837\n",
      "3972: lr: 0.0100, train loss: 2.4492, val loss: 2.5037\n",
      "3973: lr: 0.0100, train loss: 2.4533, val loss: 2.4871\n",
      "3974: lr: 0.0100, train loss: 2.4579, val loss: 2.5272\n",
      "3975: lr: 0.0100, train loss: 2.5019, val loss: 2.4900\n",
      "3976: lr: 0.0100, train loss: 2.4605, val loss: 2.4778\n",
      "3977: lr: 0.0100, train loss: 2.4317, val loss: 2.4644\n",
      "3978: lr: 0.0100, train loss: 2.4593, val loss: 2.5163\n",
      "3979: lr: 0.0100, train loss: 2.4984, val loss: 2.5034\n",
      "3980: lr: 0.0100, train loss: 2.4083, val loss: 2.4628\n",
      "3981: lr: 0.0100, train loss: 2.4936, val loss: 2.5329\n",
      "3982: lr: 0.0100, train loss: 2.4287, val loss: 2.4919\n",
      "3983: lr: 0.0100, train loss: 2.4500, val loss: 2.4956\n",
      "3984: lr: 0.0100, train loss: 2.4647, val loss: 2.4534\n",
      "3985: lr: 0.0100, train loss: 2.4591, val loss: 2.4861\n",
      "3986: lr: 0.0100, train loss: 2.4504, val loss: 2.4617\n",
      "3987: lr: 0.0100, train loss: 2.4641, val loss: 2.4325\n",
      "3988: lr: 0.0100, train loss: 2.4471, val loss: 2.4640\n",
      "3989: lr: 0.0100, train loss: 2.4546, val loss: 2.4603\n",
      "3990: lr: 0.0100, train loss: 2.4109, val loss: 2.4668\n",
      "3991: lr: 0.0100, train loss: 2.4381, val loss: 2.5036\n",
      "3992: lr: 0.0100, train loss: 2.4448, val loss: 2.4924\n",
      "3993: lr: 0.0100, train loss: 2.4675, val loss: 2.4984\n",
      "3994: lr: 0.0100, train loss: 2.4426, val loss: 2.4875\n",
      "3995: lr: 0.0100, train loss: 2.4629, val loss: 2.4740\n",
      "3996: lr: 0.0100, train loss: 2.4908, val loss: 2.4848\n",
      "3997: lr: 0.0100, train loss: 2.4853, val loss: 2.4820\n",
      "3998: lr: 0.0100, train loss: 2.4443, val loss: 2.4535\n",
      "3999: lr: 0.0100, train loss: 2.4647, val loss: 2.4759\n",
      "4000: lr: 0.0050, train loss: 2.4576, val loss: 2.4894\n",
      "4001: lr: 0.0050, train loss: 2.4382, val loss: 2.4471\n",
      "4002: lr: 0.0050, train loss: 2.4896, val loss: 2.5265\n",
      "4003: lr: 0.0050, train loss: 2.4691, val loss: 2.5179\n",
      "4004: lr: 0.0050, train loss: 2.4504, val loss: 2.4921\n",
      "4005: lr: 0.0050, train loss: 2.3980, val loss: 2.4974\n",
      "4006: lr: 0.0050, train loss: 2.4704, val loss: 2.4982\n",
      "4007: lr: 0.0050, train loss: 2.4590, val loss: 2.5080\n",
      "4008: lr: 0.0050, train loss: 2.4352, val loss: 2.5764\n",
      "4009: lr: 0.0050, train loss: 2.4488, val loss: 2.4600\n",
      "4010: lr: 0.0050, train loss: 2.4945, val loss: 2.4649\n",
      "4011: lr: 0.0050, train loss: 2.4685, val loss: 2.5196\n",
      "4012: lr: 0.0050, train loss: 2.4495, val loss: 2.5108\n",
      "4013: lr: 0.0050, train loss: 2.4258, val loss: 2.5108\n",
      "4014: lr: 0.0050, train loss: 2.4497, val loss: 2.5567\n",
      "4015: lr: 0.0050, train loss: 2.4338, val loss: 2.5232\n",
      "4016: lr: 0.0050, train loss: 2.5140, val loss: 2.5046\n",
      "4017: lr: 0.0050, train loss: 2.4721, val loss: 2.5425\n",
      "4018: lr: 0.0050, train loss: 2.4441, val loss: 2.4244\n",
      "4019: lr: 0.0050, train loss: 2.4360, val loss: 2.4760\n",
      "4020: lr: 0.0050, train loss: 2.4615, val loss: 2.4897\n",
      "4021: lr: 0.0050, train loss: 2.5030, val loss: 2.4467\n",
      "4022: lr: 0.0050, train loss: 2.4385, val loss: 2.5017\n",
      "4023: lr: 0.0050, train loss: 2.4197, val loss: 2.4585\n",
      "4024: lr: 0.0050, train loss: 2.4537, val loss: 2.4665\n",
      "4025: lr: 0.0050, train loss: 2.4496, val loss: 2.4938\n",
      "4026: lr: 0.0050, train loss: 2.4170, val loss: 2.4854\n",
      "4027: lr: 0.0050, train loss: 2.4241, val loss: 2.4495\n",
      "4028: lr: 0.0050, train loss: 2.4499, val loss: 2.4976\n",
      "4029: lr: 0.0050, train loss: 2.4046, val loss: 2.4927\n",
      "4030: lr: 0.0050, train loss: 2.4270, val loss: 2.4721\n",
      "4031: lr: 0.0050, train loss: 2.5076, val loss: 2.4455\n",
      "4032: lr: 0.0050, train loss: 2.4279, val loss: 2.4251\n",
      "4033: lr: 0.0050, train loss: 2.4267, val loss: 2.5435\n",
      "4034: lr: 0.0050, train loss: 2.4726, val loss: 2.4550\n",
      "4035: lr: 0.0050, train loss: 2.4707, val loss: 2.4696\n",
      "4036: lr: 0.0050, train loss: 2.4810, val loss: 2.5105\n",
      "4037: lr: 0.0050, train loss: 2.4775, val loss: 2.4982\n",
      "4038: lr: 0.0050, train loss: 2.4504, val loss: 2.4521\n",
      "4039: lr: 0.0050, train loss: 2.4613, val loss: 2.4997\n",
      "4040: lr: 0.0050, train loss: 2.4731, val loss: 2.4965\n",
      "4041: lr: 0.0050, train loss: 2.5101, val loss: 2.4613\n",
      "4042: lr: 0.0050, train loss: 2.3913, val loss: 2.4861\n",
      "4043: lr: 0.0050, train loss: 2.4461, val loss: 2.5199\n",
      "4044: lr: 0.0050, train loss: 2.4714, val loss: 2.5120\n",
      "4045: lr: 0.0050, train loss: 2.4503, val loss: 2.4944\n",
      "4046: lr: 0.0050, train loss: 2.4521, val loss: 2.4683\n",
      "4047: lr: 0.0050, train loss: 2.4247, val loss: 2.4937\n",
      "4048: lr: 0.0050, train loss: 2.4265, val loss: 2.5024\n",
      "4049: lr: 0.0050, train loss: 2.4355, val loss: 2.4646\n",
      "4050: lr: 0.0050, train loss: 2.4384, val loss: 2.4818\n",
      "4051: lr: 0.0050, train loss: 2.4789, val loss: 2.5427\n",
      "4052: lr: 0.0050, train loss: 2.4766, val loss: 2.5196\n",
      "4053: lr: 0.0050, train loss: 2.4654, val loss: 2.5129\n",
      "4054: lr: 0.0050, train loss: 2.4887, val loss: 2.4733\n",
      "4055: lr: 0.0050, train loss: 2.4930, val loss: 2.4255\n",
      "4056: lr: 0.0050, train loss: 2.4539, val loss: 2.4778\n",
      "4057: lr: 0.0050, train loss: 2.4261, val loss: 2.4806\n",
      "4058: lr: 0.0050, train loss: 2.4639, val loss: 2.4719\n",
      "4059: lr: 0.0050, train loss: 2.4618, val loss: 2.5136\n",
      "4060: lr: 0.0050, train loss: 2.4688, val loss: 2.4522\n",
      "4061: lr: 0.0050, train loss: 2.4263, val loss: 2.4626\n",
      "4062: lr: 0.0050, train loss: 2.4293, val loss: 2.5049\n",
      "4063: lr: 0.0050, train loss: 2.4708, val loss: 2.4777\n",
      "4064: lr: 0.0050, train loss: 2.4301, val loss: 2.5426\n",
      "4065: lr: 0.0050, train loss: 2.4698, val loss: 2.5254\n",
      "4066: lr: 0.0050, train loss: 2.4297, val loss: 2.4862\n",
      "4067: lr: 0.0050, train loss: 2.4845, val loss: 2.5241\n",
      "4068: lr: 0.0050, train loss: 2.4728, val loss: 2.4630\n",
      "4069: lr: 0.0050, train loss: 2.4253, val loss: 2.4560\n",
      "4070: lr: 0.0050, train loss: 2.4628, val loss: 2.4878\n",
      "4071: lr: 0.0050, train loss: 2.4587, val loss: 2.4644\n",
      "4072: lr: 0.0050, train loss: 2.4578, val loss: 2.4994\n",
      "4073: lr: 0.0050, train loss: 2.4351, val loss: 2.5172\n",
      "4074: lr: 0.0050, train loss: 2.4585, val loss: 2.5054\n",
      "4075: lr: 0.0050, train loss: 2.4448, val loss: 2.4878\n",
      "4076: lr: 0.0050, train loss: 2.3850, val loss: 2.4778\n",
      "4077: lr: 0.0050, train loss: 2.4904, val loss: 2.4682\n",
      "4078: lr: 0.0050, train loss: 2.4802, val loss: 2.5089\n",
      "4079: lr: 0.0050, train loss: 2.4686, val loss: 2.4663\n",
      "4080: lr: 0.0050, train loss: 2.4557, val loss: 2.5132\n",
      "4081: lr: 0.0050, train loss: 2.4158, val loss: 2.4813\n",
      "4082: lr: 0.0050, train loss: 2.4640, val loss: 2.4947\n",
      "4083: lr: 0.0050, train loss: 2.4787, val loss: 2.4884\n",
      "4084: lr: 0.0050, train loss: 2.4783, val loss: 2.4806\n",
      "4085: lr: 0.0050, train loss: 2.4830, val loss: 2.5289\n",
      "4086: lr: 0.0050, train loss: 2.4615, val loss: 2.4902\n",
      "4087: lr: 0.0050, train loss: 2.4177, val loss: 2.5408\n",
      "4088: lr: 0.0050, train loss: 2.4214, val loss: 2.5119\n",
      "4089: lr: 0.0050, train loss: 2.4615, val loss: 2.4162\n",
      "4090: lr: 0.0050, train loss: 2.4359, val loss: 2.4618\n",
      "4091: lr: 0.0050, train loss: 2.4328, val loss: 2.4684\n",
      "4092: lr: 0.0050, train loss: 2.4259, val loss: 2.4985\n",
      "4093: lr: 0.0050, train loss: 2.4638, val loss: 2.5084\n",
      "4094: lr: 0.0050, train loss: 2.4419, val loss: 2.4516\n",
      "4095: lr: 0.0050, train loss: 2.4366, val loss: 2.4466\n",
      "4096: lr: 0.0050, train loss: 2.4355, val loss: 2.4811\n",
      "4097: lr: 0.0050, train loss: 2.4119, val loss: 2.5285\n",
      "4098: lr: 0.0050, train loss: 2.4588, val loss: 2.4950\n",
      "4099: lr: 0.0050, train loss: 2.4636, val loss: 2.4520\n",
      "4100: lr: 0.0050, train loss: 2.4551, val loss: 2.4880\n",
      "4101: lr: 0.0050, train loss: 2.4581, val loss: 2.5703\n",
      "4102: lr: 0.0050, train loss: 2.4504, val loss: 2.5036\n",
      "4103: lr: 0.0050, train loss: 2.4707, val loss: 2.4857\n",
      "4104: lr: 0.0050, train loss: 2.4685, val loss: 2.4772\n",
      "4105: lr: 0.0050, train loss: 2.4469, val loss: 2.4465\n",
      "4106: lr: 0.0050, train loss: 2.4345, val loss: 2.4709\n",
      "4107: lr: 0.0050, train loss: 2.4167, val loss: 2.4856\n",
      "4108: lr: 0.0050, train loss: 2.4882, val loss: 2.5497\n",
      "4109: lr: 0.0050, train loss: 2.4687, val loss: 2.4893\n",
      "4110: lr: 0.0050, train loss: 2.4183, val loss: 2.4796\n",
      "4111: lr: 0.0050, train loss: 2.4257, val loss: 2.4404\n",
      "4112: lr: 0.0050, train loss: 2.4553, val loss: 2.5198\n",
      "4113: lr: 0.0050, train loss: 2.5151, val loss: 2.5039\n",
      "4114: lr: 0.0050, train loss: 2.5053, val loss: 2.5245\n",
      "4115: lr: 0.0050, train loss: 2.4742, val loss: 2.4447\n",
      "4116: lr: 0.0050, train loss: 2.4510, val loss: 2.4598\n",
      "4117: lr: 0.0050, train loss: 2.4572, val loss: 2.4999\n",
      "4118: lr: 0.0050, train loss: 2.4736, val loss: 2.4868\n",
      "4119: lr: 0.0050, train loss: 2.4529, val loss: 2.5050\n",
      "4120: lr: 0.0050, train loss: 2.4526, val loss: 2.4692\n",
      "4121: lr: 0.0050, train loss: 2.4671, val loss: 2.4544\n",
      "4122: lr: 0.0050, train loss: 2.4770, val loss: 2.4358\n",
      "4123: lr: 0.0050, train loss: 2.4781, val loss: 2.4700\n",
      "4124: lr: 0.0050, train loss: 2.4503, val loss: 2.4990\n",
      "4125: lr: 0.0050, train loss: 2.4654, val loss: 2.4835\n",
      "4126: lr: 0.0050, train loss: 2.4891, val loss: 2.4613\n",
      "4127: lr: 0.0050, train loss: 2.4602, val loss: 2.4531\n",
      "4128: lr: 0.0050, train loss: 2.4522, val loss: 2.5229\n",
      "4129: lr: 0.0050, train loss: 2.4908, val loss: 2.5075\n",
      "4130: lr: 0.0050, train loss: 2.4747, val loss: 2.4380\n",
      "4131: lr: 0.0050, train loss: 2.4427, val loss: 2.5523\n",
      "4132: lr: 0.0050, train loss: 2.4494, val loss: 2.4883\n",
      "4133: lr: 0.0050, train loss: 2.4570, val loss: 2.4861\n",
      "4134: lr: 0.0050, train loss: 2.4383, val loss: 2.5258\n",
      "4135: lr: 0.0050, train loss: 2.4589, val loss: 2.5290\n",
      "4136: lr: 0.0050, train loss: 2.4645, val loss: 2.4778\n",
      "4137: lr: 0.0050, train loss: 2.4919, val loss: 2.4551\n",
      "4138: lr: 0.0050, train loss: 2.4401, val loss: 2.5129\n",
      "4139: lr: 0.0050, train loss: 2.5147, val loss: 2.5288\n",
      "4140: lr: 0.0050, train loss: 2.4440, val loss: 2.4419\n",
      "4141: lr: 0.0050, train loss: 2.4558, val loss: 2.5099\n",
      "4142: lr: 0.0050, train loss: 2.4622, val loss: 2.4832\n",
      "4143: lr: 0.0050, train loss: 2.4594, val loss: 2.4738\n",
      "4144: lr: 0.0050, train loss: 2.4705, val loss: 2.5005\n",
      "4145: lr: 0.0050, train loss: 2.4589, val loss: 2.4669\n",
      "4146: lr: 0.0050, train loss: 2.4566, val loss: 2.5001\n",
      "4147: lr: 0.0050, train loss: 2.4390, val loss: 2.4516\n",
      "4148: lr: 0.0050, train loss: 2.4457, val loss: 2.4873\n",
      "4149: lr: 0.0050, train loss: 2.4186, val loss: 2.5336\n",
      "4150: lr: 0.0050, train loss: 2.4226, val loss: 2.5006\n",
      "4151: lr: 0.0050, train loss: 2.4473, val loss: 2.5045\n",
      "4152: lr: 0.0050, train loss: 2.4653, val loss: 2.5251\n",
      "4153: lr: 0.0050, train loss: 2.4702, val loss: 2.4725\n",
      "4154: lr: 0.0050, train loss: 2.4796, val loss: 2.5128\n",
      "4155: lr: 0.0050, train loss: 2.4613, val loss: 2.4908\n",
      "4156: lr: 0.0050, train loss: 2.4714, val loss: 2.4564\n",
      "4157: lr: 0.0050, train loss: 2.4474, val loss: 2.4690\n",
      "4158: lr: 0.0050, train loss: 2.4333, val loss: 2.5575\n",
      "4159: lr: 0.0050, train loss: 2.4565, val loss: 2.5054\n",
      "4160: lr: 0.0050, train loss: 2.4467, val loss: 2.5558\n",
      "4161: lr: 0.0050, train loss: 2.4670, val loss: 2.4947\n",
      "4162: lr: 0.0050, train loss: 2.4556, val loss: 2.4435\n",
      "4163: lr: 0.0050, train loss: 2.4386, val loss: 2.5087\n",
      "4164: lr: 0.0050, train loss: 2.4475, val loss: 2.4717\n",
      "4165: lr: 0.0050, train loss: 2.3970, val loss: 2.5089\n",
      "4166: lr: 0.0050, train loss: 2.4317, val loss: 2.4636\n",
      "4167: lr: 0.0050, train loss: 2.4549, val loss: 2.5410\n",
      "4168: lr: 0.0050, train loss: 2.4687, val loss: 2.4836\n",
      "4169: lr: 0.0050, train loss: 2.4517, val loss: 2.5091\n",
      "4170: lr: 0.0050, train loss: 2.4824, val loss: 2.4329\n",
      "4171: lr: 0.0050, train loss: 2.4638, val loss: 2.4277\n",
      "4172: lr: 0.0050, train loss: 2.4794, val loss: 2.4698\n",
      "4173: lr: 0.0050, train loss: 2.4114, val loss: 2.5049\n",
      "4174: lr: 0.0050, train loss: 2.4247, val loss: 2.4786\n",
      "4175: lr: 0.0050, train loss: 2.3891, val loss: 2.5085\n",
      "4176: lr: 0.0050, train loss: 2.4776, val loss: 2.4767\n",
      "4177: lr: 0.0050, train loss: 2.4179, val loss: 2.4472\n",
      "4178: lr: 0.0050, train loss: 2.4910, val loss: 2.4622\n",
      "4179: lr: 0.0050, train loss: 2.4745, val loss: 2.5104\n",
      "4180: lr: 0.0050, train loss: 2.4698, val loss: 2.4914\n",
      "4181: lr: 0.0050, train loss: 2.4770, val loss: 2.4936\n",
      "4182: lr: 0.0050, train loss: 2.4511, val loss: 2.5269\n",
      "4183: lr: 0.0050, train loss: 2.4827, val loss: 2.4213\n",
      "4184: lr: 0.0050, train loss: 2.4467, val loss: 2.4628\n",
      "4185: lr: 0.0050, train loss: 2.4991, val loss: 2.4473\n",
      "4186: lr: 0.0050, train loss: 2.4488, val loss: 2.4550\n",
      "4187: lr: 0.0050, train loss: 2.4679, val loss: 2.4939\n",
      "4188: lr: 0.0050, train loss: 2.4482, val loss: 2.4652\n",
      "4189: lr: 0.0050, train loss: 2.4788, val loss: 2.4556\n",
      "4190: lr: 0.0050, train loss: 2.4815, val loss: 2.5157\n",
      "4191: lr: 0.0050, train loss: 2.4539, val loss: 2.4974\n",
      "4192: lr: 0.0050, train loss: 2.4524, val loss: 2.5246\n",
      "4193: lr: 0.0050, train loss: 2.4411, val loss: 2.5001\n",
      "4194: lr: 0.0050, train loss: 2.4274, val loss: 2.4485\n",
      "4195: lr: 0.0050, train loss: 2.5073, val loss: 2.5279\n",
      "4196: lr: 0.0050, train loss: 2.5107, val loss: 2.5181\n",
      "4197: lr: 0.0050, train loss: 2.4270, val loss: 2.4743\n",
      "4198: lr: 0.0050, train loss: 2.4179, val loss: 2.4935\n",
      "4199: lr: 0.0050, train loss: 2.4137, val loss: 2.4636\n",
      "4200: lr: 0.0050, train loss: 2.4440, val loss: 2.5320\n",
      "4201: lr: 0.0050, train loss: 2.4299, val loss: 2.4415\n",
      "4202: lr: 0.0050, train loss: 2.4395, val loss: 2.4756\n",
      "4203: lr: 0.0050, train loss: 2.4761, val loss: 2.5059\n",
      "4204: lr: 0.0050, train loss: 2.4515, val loss: 2.4686\n",
      "4205: lr: 0.0050, train loss: 2.4657, val loss: 2.5648\n",
      "4206: lr: 0.0050, train loss: 2.5153, val loss: 2.5079\n",
      "4207: lr: 0.0050, train loss: 2.4659, val loss: 2.4734\n",
      "4208: lr: 0.0050, train loss: 2.4490, val loss: 2.4526\n",
      "4209: lr: 0.0050, train loss: 2.4556, val loss: 2.4626\n",
      "4210: lr: 0.0050, train loss: 2.4920, val loss: 2.4936\n",
      "4211: lr: 0.0050, train loss: 2.4624, val loss: 2.5534\n",
      "4212: lr: 0.0050, train loss: 2.5006, val loss: 2.4952\n",
      "4213: lr: 0.0050, train loss: 2.4573, val loss: 2.4799\n",
      "4214: lr: 0.0050, train loss: 2.4460, val loss: 2.5347\n",
      "4215: lr: 0.0050, train loss: 2.4862, val loss: 2.4764\n",
      "4216: lr: 0.0050, train loss: 2.4861, val loss: 2.4684\n",
      "4217: lr: 0.0050, train loss: 2.4684, val loss: 2.4521\n",
      "4218: lr: 0.0050, train loss: 2.4864, val loss: 2.4517\n",
      "4219: lr: 0.0050, train loss: 2.5021, val loss: 2.5112\n",
      "4220: lr: 0.0050, train loss: 2.4783, val loss: 2.5068\n",
      "4221: lr: 0.0050, train loss: 2.4170, val loss: 2.5297\n",
      "4222: lr: 0.0050, train loss: 2.4459, val loss: 2.4954\n",
      "4223: lr: 0.0050, train loss: 2.4231, val loss: 2.4865\n",
      "4224: lr: 0.0050, train loss: 2.4502, val loss: 2.4639\n",
      "4225: lr: 0.0050, train loss: 2.4062, val loss: 2.5272\n",
      "4226: lr: 0.0050, train loss: 2.4799, val loss: 2.4869\n",
      "4227: lr: 0.0050, train loss: 2.4316, val loss: 2.5061\n",
      "4228: lr: 0.0050, train loss: 2.4545, val loss: 2.5043\n",
      "4229: lr: 0.0050, train loss: 2.4607, val loss: 2.3982\n",
      "4230: lr: 0.0050, train loss: 2.4377, val loss: 2.4567\n",
      "4231: lr: 0.0050, train loss: 2.4826, val loss: 2.4871\n",
      "4232: lr: 0.0050, train loss: 2.4529, val loss: 2.4567\n",
      "4233: lr: 0.0050, train loss: 2.4036, val loss: 2.5213\n",
      "4234: lr: 0.0050, train loss: 2.4781, val loss: 2.4780\n",
      "4235: lr: 0.0050, train loss: 2.4663, val loss: 2.5023\n",
      "4236: lr: 0.0050, train loss: 2.4435, val loss: 2.4798\n",
      "4237: lr: 0.0050, train loss: 2.4707, val loss: 2.4677\n",
      "4238: lr: 0.0050, train loss: 2.4953, val loss: 2.4665\n",
      "4239: lr: 0.0050, train loss: 2.4439, val loss: 2.4689\n",
      "4240: lr: 0.0050, train loss: 2.4496, val loss: 2.4377\n",
      "4241: lr: 0.0050, train loss: 2.4749, val loss: 2.4962\n",
      "4242: lr: 0.0050, train loss: 2.4071, val loss: 2.5190\n",
      "4243: lr: 0.0050, train loss: 2.4914, val loss: 2.5317\n",
      "4244: lr: 0.0050, train loss: 2.4644, val loss: 2.4567\n",
      "4245: lr: 0.0050, train loss: 2.4859, val loss: 2.4878\n",
      "4246: lr: 0.0050, train loss: 2.4673, val loss: 2.4781\n",
      "4247: lr: 0.0050, train loss: 2.5339, val loss: 2.4746\n",
      "4248: lr: 0.0050, train loss: 2.4532, val loss: 2.4928\n",
      "4249: lr: 0.0050, train loss: 2.4578, val loss: 2.4906\n",
      "4250: lr: 0.0050, train loss: 2.4711, val loss: 2.5085\n",
      "4251: lr: 0.0050, train loss: 2.4741, val loss: 2.4590\n",
      "4252: lr: 0.0050, train loss: 2.4613, val loss: 2.4846\n",
      "4253: lr: 0.0050, train loss: 2.4649, val loss: 2.5421\n",
      "4254: lr: 0.0050, train loss: 2.4560, val loss: 2.4842\n",
      "4255: lr: 0.0050, train loss: 2.4834, val loss: 2.5370\n",
      "4256: lr: 0.0050, train loss: 2.4152, val loss: 2.5222\n",
      "4257: lr: 0.0050, train loss: 2.4583, val loss: 2.4824\n",
      "4258: lr: 0.0050, train loss: 2.4246, val loss: 2.4568\n",
      "4259: lr: 0.0050, train loss: 2.4967, val loss: 2.4621\n",
      "4260: lr: 0.0050, train loss: 2.5212, val loss: 2.5386\n",
      "4261: lr: 0.0050, train loss: 2.4645, val loss: 2.4560\n",
      "4262: lr: 0.0050, train loss: 2.5159, val loss: 2.4735\n",
      "4263: lr: 0.0050, train loss: 2.4459, val loss: 2.4946\n",
      "4264: lr: 0.0050, train loss: 2.4089, val loss: 2.5146\n",
      "4265: lr: 0.0050, train loss: 2.4544, val loss: 2.4518\n",
      "4266: lr: 0.0050, train loss: 2.4864, val loss: 2.5180\n",
      "4267: lr: 0.0050, train loss: 2.4334, val loss: 2.4856\n",
      "4268: lr: 0.0050, train loss: 2.4350, val loss: 2.5066\n",
      "4269: lr: 0.0050, train loss: 2.4138, val loss: 2.4766\n",
      "4270: lr: 0.0050, train loss: 2.4796, val loss: 2.4620\n",
      "4271: lr: 0.0050, train loss: 2.4768, val loss: 2.4772\n",
      "4272: lr: 0.0050, train loss: 2.4466, val loss: 2.4956\n",
      "4273: lr: 0.0050, train loss: 2.4927, val loss: 2.5163\n",
      "4274: lr: 0.0050, train loss: 2.4879, val loss: 2.4776\n",
      "4275: lr: 0.0050, train loss: 2.4468, val loss: 2.5009\n",
      "4276: lr: 0.0050, train loss: 2.4769, val loss: 2.4800\n",
      "4277: lr: 0.0050, train loss: 2.4982, val loss: 2.4663\n",
      "4278: lr: 0.0050, train loss: 2.3907, val loss: 2.4579\n",
      "4279: lr: 0.0050, train loss: 2.4818, val loss: 2.4827\n",
      "4280: lr: 0.0050, train loss: 2.4373, val loss: 2.4790\n",
      "4281: lr: 0.0050, train loss: 2.4340, val loss: 2.4908\n",
      "4282: lr: 0.0050, train loss: 2.4272, val loss: 2.4657\n",
      "4283: lr: 0.0050, train loss: 2.4669, val loss: 2.4206\n",
      "4284: lr: 0.0050, train loss: 2.4484, val loss: 2.4643\n",
      "4285: lr: 0.0050, train loss: 2.4740, val loss: 2.5244\n",
      "4286: lr: 0.0050, train loss: 2.4377, val loss: 2.5088\n",
      "4287: lr: 0.0050, train loss: 2.4777, val loss: 2.4965\n",
      "4288: lr: 0.0050, train loss: 2.4451, val loss: 2.4766\n",
      "4289: lr: 0.0050, train loss: 2.4512, val loss: 2.5022\n",
      "4290: lr: 0.0050, train loss: 2.4492, val loss: 2.4615\n",
      "4291: lr: 0.0050, train loss: 2.4601, val loss: 2.4707\n",
      "4292: lr: 0.0050, train loss: 2.4093, val loss: 2.4695\n",
      "4293: lr: 0.0050, train loss: 2.4471, val loss: 2.5027\n",
      "4294: lr: 0.0050, train loss: 2.4565, val loss: 2.4683\n",
      "4295: lr: 0.0050, train loss: 2.4050, val loss: 2.4777\n",
      "4296: lr: 0.0050, train loss: 2.4515, val loss: 2.4615\n",
      "4297: lr: 0.0050, train loss: 2.4454, val loss: 2.5065\n",
      "4298: lr: 0.0050, train loss: 2.4607, val loss: 2.4773\n",
      "4299: lr: 0.0050, train loss: 2.4577, val loss: 2.4899\n",
      "4300: lr: 0.0050, train loss: 2.4628, val loss: 2.4878\n",
      "4301: lr: 0.0050, train loss: 2.4703, val loss: 2.5258\n",
      "4302: lr: 0.0050, train loss: 2.4760, val loss: 2.5122\n",
      "4303: lr: 0.0050, train loss: 2.4082, val loss: 2.5179\n",
      "4304: lr: 0.0050, train loss: 2.4184, val loss: 2.4493\n",
      "4305: lr: 0.0050, train loss: 2.4271, val loss: 2.4865\n",
      "4306: lr: 0.0050, train loss: 2.4561, val loss: 2.4606\n",
      "4307: lr: 0.0050, train loss: 2.4616, val loss: 2.4848\n",
      "4308: lr: 0.0050, train loss: 2.4513, val loss: 2.4961\n",
      "4309: lr: 0.0050, train loss: 2.4748, val loss: 2.4565\n",
      "4310: lr: 0.0050, train loss: 2.4458, val loss: 2.5329\n",
      "4311: lr: 0.0050, train loss: 2.4890, val loss: 2.4225\n",
      "4312: lr: 0.0050, train loss: 2.4153, val loss: 2.5165\n",
      "4313: lr: 0.0050, train loss: 2.4531, val loss: 2.5381\n",
      "4314: lr: 0.0050, train loss: 2.4744, val loss: 2.4962\n",
      "4315: lr: 0.0050, train loss: 2.4326, val loss: 2.4847\n",
      "4316: lr: 0.0050, train loss: 2.4088, val loss: 2.5285\n",
      "4317: lr: 0.0050, train loss: 2.4508, val loss: 2.5313\n",
      "4318: lr: 0.0050, train loss: 2.4449, val loss: 2.5329\n",
      "4319: lr: 0.0050, train loss: 2.4917, val loss: 2.4954\n",
      "4320: lr: 0.0050, train loss: 2.4564, val loss: 2.4397\n",
      "4321: lr: 0.0050, train loss: 2.4418, val loss: 2.4317\n",
      "4322: lr: 0.0050, train loss: 2.4807, val loss: 2.4712\n",
      "4323: lr: 0.0050, train loss: 2.5102, val loss: 2.4614\n",
      "4324: lr: 0.0050, train loss: 2.4840, val loss: 2.4960\n",
      "4325: lr: 0.0050, train loss: 2.4750, val loss: 2.4669\n",
      "4326: lr: 0.0050, train loss: 2.4274, val loss: 2.5060\n",
      "4327: lr: 0.0050, train loss: 2.4507, val loss: 2.5156\n",
      "4328: lr: 0.0050, train loss: 2.4556, val loss: 2.5013\n",
      "4329: lr: 0.0050, train loss: 2.4695, val loss: 2.5206\n",
      "4330: lr: 0.0050, train loss: 2.4409, val loss: 2.4826\n",
      "4331: lr: 0.0050, train loss: 2.4551, val loss: 2.4615\n",
      "4332: lr: 0.0050, train loss: 2.4598, val loss: 2.5002\n",
      "4333: lr: 0.0050, train loss: 2.4723, val loss: 2.4477\n",
      "4334: lr: 0.0050, train loss: 2.4448, val loss: 2.4680\n",
      "4335: lr: 0.0050, train loss: 2.4633, val loss: 2.4536\n",
      "4336: lr: 0.0050, train loss: 2.4645, val loss: 2.4531\n",
      "4337: lr: 0.0050, train loss: 2.4612, val loss: 2.5036\n",
      "4338: lr: 0.0050, train loss: 2.4402, val loss: 2.5088\n",
      "4339: lr: 0.0050, train loss: 2.4549, val loss: 2.4476\n",
      "4340: lr: 0.0050, train loss: 2.4799, val loss: 2.4909\n",
      "4341: lr: 0.0050, train loss: 2.4870, val loss: 2.5587\n",
      "4342: lr: 0.0050, train loss: 2.4436, val loss: 2.5125\n",
      "4343: lr: 0.0050, train loss: 2.5047, val loss: 2.5143\n",
      "4344: lr: 0.0050, train loss: 2.4555, val loss: 2.4752\n",
      "4345: lr: 0.0050, train loss: 2.4821, val loss: 2.4921\n",
      "4346: lr: 0.0050, train loss: 2.4514, val loss: 2.4760\n",
      "4347: lr: 0.0050, train loss: 2.4241, val loss: 2.4818\n",
      "4348: lr: 0.0050, train loss: 2.4341, val loss: 2.4665\n",
      "4349: lr: 0.0050, train loss: 2.5047, val loss: 2.4737\n",
      "4350: lr: 0.0050, train loss: 2.4623, val loss: 2.4665\n",
      "4351: lr: 0.0050, train loss: 2.4787, val loss: 2.4655\n",
      "4352: lr: 0.0050, train loss: 2.4814, val loss: 2.4686\n",
      "4353: lr: 0.0050, train loss: 2.4606, val loss: 2.4932\n",
      "4354: lr: 0.0050, train loss: 2.4437, val loss: 2.4955\n",
      "4355: lr: 0.0050, train loss: 2.4371, val loss: 2.4808\n",
      "4356: lr: 0.0050, train loss: 2.5092, val loss: 2.4646\n",
      "4357: lr: 0.0050, train loss: 2.4608, val loss: 2.4736\n",
      "4358: lr: 0.0050, train loss: 2.4127, val loss: 2.4776\n",
      "4359: lr: 0.0050, train loss: 2.4755, val loss: 2.4632\n",
      "4360: lr: 0.0050, train loss: 2.4574, val loss: 2.4669\n",
      "4361: lr: 0.0050, train loss: 2.3917, val loss: 2.4989\n",
      "4362: lr: 0.0050, train loss: 2.4483, val loss: 2.4815\n",
      "4363: lr: 0.0050, train loss: 2.4375, val loss: 2.5099\n",
      "4364: lr: 0.0050, train loss: 2.4458, val loss: 2.4897\n",
      "4365: lr: 0.0050, train loss: 2.4268, val loss: 2.4958\n",
      "4366: lr: 0.0050, train loss: 2.4867, val loss: 2.4917\n",
      "4367: lr: 0.0050, train loss: 2.4488, val loss: 2.4974\n",
      "4368: lr: 0.0050, train loss: 2.4587, val loss: 2.5040\n",
      "4369: lr: 0.0050, train loss: 2.4459, val loss: 2.4593\n",
      "4370: lr: 0.0050, train loss: 2.4508, val loss: 2.4543\n",
      "4371: lr: 0.0050, train loss: 2.4512, val loss: 2.5081\n",
      "4372: lr: 0.0050, train loss: 2.4680, val loss: 2.4803\n",
      "4373: lr: 0.0050, train loss: 2.4563, val loss: 2.4817\n",
      "4374: lr: 0.0050, train loss: 2.4172, val loss: 2.5053\n",
      "4375: lr: 0.0050, train loss: 2.4298, val loss: 2.4443\n",
      "4376: lr: 0.0050, train loss: 2.4735, val loss: 2.4632\n",
      "4377: lr: 0.0050, train loss: 2.4745, val loss: 2.4823\n",
      "4378: lr: 0.0050, train loss: 2.4514, val loss: 2.4912\n",
      "4379: lr: 0.0050, train loss: 2.4909, val loss: 2.4316\n",
      "4380: lr: 0.0050, train loss: 2.4775, val loss: 2.4829\n",
      "4381: lr: 0.0050, train loss: 2.4605, val loss: 2.4879\n",
      "4382: lr: 0.0050, train loss: 2.4772, val loss: 2.4769\n",
      "4383: lr: 0.0050, train loss: 2.4907, val loss: 2.4826\n",
      "4384: lr: 0.0050, train loss: 2.4455, val loss: 2.4651\n",
      "4385: lr: 0.0050, train loss: 2.4918, val loss: 2.4712\n",
      "4386: lr: 0.0050, train loss: 2.4607, val loss: 2.4170\n",
      "4387: lr: 0.0050, train loss: 2.4451, val loss: 2.5819\n",
      "4388: lr: 0.0050, train loss: 2.4424, val loss: 2.4964\n",
      "4389: lr: 0.0050, train loss: 2.4389, val loss: 2.4344\n",
      "4390: lr: 0.0050, train loss: 2.4970, val loss: 2.5314\n",
      "4391: lr: 0.0050, train loss: 2.4514, val loss: 2.4346\n",
      "4392: lr: 0.0050, train loss: 2.4491, val loss: 2.5378\n",
      "4393: lr: 0.0050, train loss: 2.4498, val loss: 2.4634\n",
      "4394: lr: 0.0050, train loss: 2.4340, val loss: 2.4512\n",
      "4395: lr: 0.0050, train loss: 2.4594, val loss: 2.4637\n",
      "4396: lr: 0.0050, train loss: 2.4622, val loss: 2.4804\n",
      "4397: lr: 0.0050, train loss: 2.4170, val loss: 2.4423\n",
      "4398: lr: 0.0050, train loss: 2.4558, val loss: 2.5364\n",
      "4399: lr: 0.0050, train loss: 2.4544, val loss: 2.4707\n",
      "4400: lr: 0.0050, train loss: 2.4328, val loss: 2.4206\n",
      "4401: lr: 0.0050, train loss: 2.4845, val loss: 2.4794\n",
      "4402: lr: 0.0050, train loss: 2.4224, val loss: 2.5693\n",
      "4403: lr: 0.0050, train loss: 2.4419, val loss: 2.4904\n",
      "4404: lr: 0.0050, train loss: 2.5018, val loss: 2.4846\n",
      "4405: lr: 0.0050, train loss: 2.4707, val loss: 2.4759\n",
      "4406: lr: 0.0050, train loss: 2.5051, val loss: 2.5049\n",
      "4407: lr: 0.0050, train loss: 2.4386, val loss: 2.4731\n",
      "4408: lr: 0.0050, train loss: 2.4821, val loss: 2.5141\n",
      "4409: lr: 0.0050, train loss: 2.4848, val loss: 2.5111\n",
      "4410: lr: 0.0050, train loss: 2.4544, val loss: 2.5426\n",
      "4411: lr: 0.0050, train loss: 2.5008, val loss: 2.4517\n",
      "4412: lr: 0.0050, train loss: 2.4868, val loss: 2.4666\n",
      "4413: lr: 0.0050, train loss: 2.4254, val loss: 2.4705\n",
      "4414: lr: 0.0050, train loss: 2.4827, val loss: 2.5077\n",
      "4415: lr: 0.0050, train loss: 2.4752, val loss: 2.4888\n",
      "4416: lr: 0.0050, train loss: 2.4950, val loss: 2.5092\n",
      "4417: lr: 0.0050, train loss: 2.4103, val loss: 2.4968\n",
      "4418: lr: 0.0050, train loss: 2.4226, val loss: 2.5123\n",
      "4419: lr: 0.0050, train loss: 2.4301, val loss: 2.5183\n",
      "4420: lr: 0.0050, train loss: 2.4802, val loss: 2.5070\n",
      "4421: lr: 0.0050, train loss: 2.3866, val loss: 2.4901\n",
      "4422: lr: 0.0050, train loss: 2.4663, val loss: 2.4568\n",
      "4423: lr: 0.0050, train loss: 2.4766, val loss: 2.4891\n",
      "4424: lr: 0.0050, train loss: 2.4974, val loss: 2.4993\n",
      "4425: lr: 0.0050, train loss: 2.4455, val loss: 2.5469\n",
      "4426: lr: 0.0050, train loss: 2.4643, val loss: 2.4375\n",
      "4427: lr: 0.0050, train loss: 2.4593, val loss: 2.4927\n",
      "4428: lr: 0.0050, train loss: 2.4767, val loss: 2.4691\n",
      "4429: lr: 0.0050, train loss: 2.4661, val loss: 2.4747\n",
      "4430: lr: 0.0050, train loss: 2.4796, val loss: 2.5360\n",
      "4431: lr: 0.0050, train loss: 2.4890, val loss: 2.5105\n",
      "4432: lr: 0.0050, train loss: 2.4510, val loss: 2.4752\n",
      "4433: lr: 0.0050, train loss: 2.4461, val loss: 2.5170\n",
      "4434: lr: 0.0050, train loss: 2.4632, val loss: 2.4515\n",
      "4435: lr: 0.0050, train loss: 2.4475, val loss: 2.4515\n",
      "4436: lr: 0.0050, train loss: 2.5212, val loss: 2.4792\n",
      "4437: lr: 0.0050, train loss: 2.4572, val loss: 2.4470\n",
      "4438: lr: 0.0050, train loss: 2.4624, val loss: 2.4913\n",
      "4439: lr: 0.0050, train loss: 2.4787, val loss: 2.4779\n",
      "4440: lr: 0.0050, train loss: 2.4408, val loss: 2.5034\n",
      "4441: lr: 0.0050, train loss: 2.4121, val loss: 2.5026\n",
      "4442: lr: 0.0050, train loss: 2.4369, val loss: 2.4642\n",
      "4443: lr: 0.0050, train loss: 2.4903, val loss: 2.4702\n",
      "4444: lr: 0.0050, train loss: 2.4669, val loss: 2.4788\n",
      "4445: lr: 0.0050, train loss: 2.4950, val loss: 2.4496\n",
      "4446: lr: 0.0050, train loss: 2.4922, val loss: 2.4955\n",
      "4447: lr: 0.0050, train loss: 2.4824, val loss: 2.4895\n",
      "4448: lr: 0.0050, train loss: 2.4661, val loss: 2.4983\n",
      "4449: lr: 0.0050, train loss: 2.4355, val loss: 2.4914\n",
      "4450: lr: 0.0050, train loss: 2.4560, val loss: 2.4933\n",
      "4451: lr: 0.0050, train loss: 2.3919, val loss: 2.4909\n",
      "4452: lr: 0.0050, train loss: 2.4661, val loss: 2.4846\n",
      "4453: lr: 0.0050, train loss: 2.4209, val loss: 2.4691\n",
      "4454: lr: 0.0050, train loss: 2.4119, val loss: 2.4452\n",
      "4455: lr: 0.0050, train loss: 2.4424, val loss: 2.4726\n",
      "4456: lr: 0.0050, train loss: 2.4673, val loss: 2.5020\n",
      "4457: lr: 0.0050, train loss: 2.4411, val loss: 2.4681\n",
      "4458: lr: 0.0050, train loss: 2.4568, val loss: 2.4658\n",
      "4459: lr: 0.0050, train loss: 2.4607, val loss: 2.4491\n",
      "4460: lr: 0.0050, train loss: 2.4374, val loss: 2.4621\n",
      "4461: lr: 0.0050, train loss: 2.4539, val loss: 2.5738\n",
      "4462: lr: 0.0050, train loss: 2.4487, val loss: 2.5268\n",
      "4463: lr: 0.0050, train loss: 2.4555, val loss: 2.4932\n",
      "4464: lr: 0.0050, train loss: 2.4437, val loss: 2.4585\n",
      "4465: lr: 0.0050, train loss: 2.4552, val loss: 2.4959\n",
      "4466: lr: 0.0050, train loss: 2.4996, val loss: 2.5680\n",
      "4467: lr: 0.0050, train loss: 2.4952, val loss: 2.4903\n",
      "4468: lr: 0.0050, train loss: 2.4879, val loss: 2.4342\n",
      "4469: lr: 0.0050, train loss: 2.4570, val loss: 2.4790\n",
      "4470: lr: 0.0050, train loss: 2.4327, val loss: 2.4471\n",
      "4471: lr: 0.0050, train loss: 2.4433, val loss: 2.5063\n",
      "4472: lr: 0.0050, train loss: 2.4818, val loss: 2.4427\n",
      "4473: lr: 0.0050, train loss: 2.4817, val loss: 2.5065\n",
      "4474: lr: 0.0050, train loss: 2.5170, val loss: 2.5052\n",
      "4475: lr: 0.0050, train loss: 2.4991, val loss: 2.4883\n",
      "4476: lr: 0.0050, train loss: 2.5080, val loss: 2.4680\n",
      "4477: lr: 0.0050, train loss: 2.4692, val loss: 2.5146\n",
      "4478: lr: 0.0050, train loss: 2.4645, val loss: 2.4432\n",
      "4479: lr: 0.0050, train loss: 2.4947, val loss: 2.4996\n",
      "4480: lr: 0.0050, train loss: 2.4518, val loss: 2.4887\n",
      "4481: lr: 0.0050, train loss: 2.4322, val loss: 2.4538\n",
      "4482: lr: 0.0050, train loss: 2.4499, val loss: 2.4460\n",
      "4483: lr: 0.0050, train loss: 2.4776, val loss: 2.4422\n",
      "4484: lr: 0.0050, train loss: 2.4702, val loss: 2.4729\n",
      "4485: lr: 0.0050, train loss: 2.4980, val loss: 2.4863\n",
      "4486: lr: 0.0050, train loss: 2.5101, val loss: 2.4599\n",
      "4487: lr: 0.0050, train loss: 2.4385, val loss: 2.5388\n",
      "4488: lr: 0.0050, train loss: 2.4821, val loss: 2.4977\n",
      "4489: lr: 0.0050, train loss: 2.4274, val loss: 2.4708\n",
      "4490: lr: 0.0050, train loss: 2.4587, val loss: 2.4798\n",
      "4491: lr: 0.0050, train loss: 2.4326, val loss: 2.5109\n",
      "4492: lr: 0.0050, train loss: 2.4529, val loss: 2.4673\n",
      "4493: lr: 0.0050, train loss: 2.4513, val loss: 2.5341\n",
      "4494: lr: 0.0050, train loss: 2.4764, val loss: 2.4161\n",
      "4495: lr: 0.0050, train loss: 2.4554, val loss: 2.5110\n",
      "4496: lr: 0.0050, train loss: 2.4488, val loss: 2.4551\n",
      "4497: lr: 0.0050, train loss: 2.4033, val loss: 2.4728\n",
      "4498: lr: 0.0050, train loss: 2.4214, val loss: 2.4852\n",
      "4499: lr: 0.0050, train loss: 2.4527, val loss: 2.4788\n",
      "4500: lr: 0.0050, train loss: 2.4834, val loss: 2.4924\n",
      "4501: lr: 0.0050, train loss: 2.4500, val loss: 2.4727\n",
      "4502: lr: 0.0050, train loss: 2.4686, val loss: 2.4540\n",
      "4503: lr: 0.0050, train loss: 2.4593, val loss: 2.5167\n",
      "4504: lr: 0.0050, train loss: 2.4232, val loss: 2.5029\n",
      "4505: lr: 0.0050, train loss: 2.4741, val loss: 2.4757\n",
      "4506: lr: 0.0050, train loss: 2.4579, val loss: 2.5704\n",
      "4507: lr: 0.0050, train loss: 2.4608, val loss: 2.5015\n",
      "4508: lr: 0.0050, train loss: 2.4317, val loss: 2.4622\n",
      "4509: lr: 0.0050, train loss: 2.4929, val loss: 2.5035\n",
      "4510: lr: 0.0050, train loss: 2.4735, val loss: 2.4576\n",
      "4511: lr: 0.0050, train loss: 2.4398, val loss: 2.4654\n",
      "4512: lr: 0.0050, train loss: 2.4740, val loss: 2.4925\n",
      "4513: lr: 0.0050, train loss: 2.4305, val loss: 2.4439\n",
      "4514: lr: 0.0050, train loss: 2.4378, val loss: 2.4626\n",
      "4515: lr: 0.0050, train loss: 2.4785, val loss: 2.4733\n",
      "4516: lr: 0.0050, train loss: 2.4460, val loss: 2.4871\n",
      "4517: lr: 0.0050, train loss: 2.4483, val loss: 2.4746\n",
      "4518: lr: 0.0050, train loss: 2.4982, val loss: 2.4571\n",
      "4519: lr: 0.0050, train loss: 2.4308, val loss: 2.5002\n",
      "4520: lr: 0.0050, train loss: 2.4636, val loss: 2.4698\n",
      "4521: lr: 0.0050, train loss: 2.4677, val loss: 2.4686\n",
      "4522: lr: 0.0050, train loss: 2.4214, val loss: 2.4731\n",
      "4523: lr: 0.0050, train loss: 2.4282, val loss: 2.5214\n",
      "4524: lr: 0.0050, train loss: 2.4037, val loss: 2.4364\n",
      "4525: lr: 0.0050, train loss: 2.4471, val loss: 2.4478\n",
      "4526: lr: 0.0050, train loss: 2.4731, val loss: 2.5533\n",
      "4527: lr: 0.0050, train loss: 2.4112, val loss: 2.4689\n",
      "4528: lr: 0.0050, train loss: 2.4165, val loss: 2.4734\n",
      "4529: lr: 0.0050, train loss: 2.4884, val loss: 2.5374\n",
      "4530: lr: 0.0050, train loss: 2.4610, val loss: 2.4300\n",
      "4531: lr: 0.0050, train loss: 2.4765, val loss: 2.4733\n",
      "4532: lr: 0.0050, train loss: 2.4484, val loss: 2.5293\n",
      "4533: lr: 0.0050, train loss: 2.4607, val loss: 2.4708\n",
      "4534: lr: 0.0050, train loss: 2.4354, val loss: 2.4996\n",
      "4535: lr: 0.0050, train loss: 2.4615, val loss: 2.4244\n",
      "4536: lr: 0.0050, train loss: 2.4400, val loss: 2.4947\n",
      "4537: lr: 0.0050, train loss: 2.4723, val loss: 2.4614\n",
      "4538: lr: 0.0050, train loss: 2.4885, val loss: 2.4658\n",
      "4539: lr: 0.0050, train loss: 2.4767, val loss: 2.5177\n",
      "4540: lr: 0.0050, train loss: 2.4203, val loss: 2.4820\n",
      "4541: lr: 0.0050, train loss: 2.4942, val loss: 2.5503\n",
      "4542: lr: 0.0050, train loss: 2.4264, val loss: 2.5069\n",
      "4543: lr: 0.0050, train loss: 2.4849, val loss: 2.4642\n",
      "4544: lr: 0.0050, train loss: 2.4644, val loss: 2.4918\n",
      "4545: lr: 0.0050, train loss: 2.4248, val loss: 2.4809\n",
      "4546: lr: 0.0050, train loss: 2.4277, val loss: 2.5227\n",
      "4547: lr: 0.0050, train loss: 2.4664, val loss: 2.5269\n",
      "4548: lr: 0.0050, train loss: 2.4901, val loss: 2.4671\n",
      "4549: lr: 0.0050, train loss: 2.4572, val loss: 2.5113\n",
      "4550: lr: 0.0050, train loss: 2.4434, val loss: 2.5113\n",
      "4551: lr: 0.0050, train loss: 2.4443, val loss: 2.4982\n",
      "4552: lr: 0.0050, train loss: 2.4594, val loss: 2.4883\n",
      "4553: lr: 0.0050, train loss: 2.4906, val loss: 2.4784\n",
      "4554: lr: 0.0050, train loss: 2.4767, val loss: 2.4393\n",
      "4555: lr: 0.0050, train loss: 2.4279, val loss: 2.5036\n",
      "4556: lr: 0.0050, train loss: 2.4749, val loss: 2.4893\n",
      "4557: lr: 0.0050, train loss: 2.4739, val loss: 2.5203\n",
      "4558: lr: 0.0050, train loss: 2.4386, val loss: 2.4531\n",
      "4559: lr: 0.0050, train loss: 2.4548, val loss: 2.5197\n",
      "4560: lr: 0.0050, train loss: 2.4527, val loss: 2.4646\n",
      "4561: lr: 0.0050, train loss: 2.4451, val loss: 2.5023\n",
      "4562: lr: 0.0050, train loss: 2.4155, val loss: 2.4810\n",
      "4563: lr: 0.0050, train loss: 2.4571, val loss: 2.4968\n",
      "4564: lr: 0.0050, train loss: 2.4740, val loss: 2.4635\n",
      "4565: lr: 0.0050, train loss: 2.4479, val loss: 2.4945\n",
      "4566: lr: 0.0050, train loss: 2.4492, val loss: 2.4529\n",
      "4567: lr: 0.0050, train loss: 2.4446, val loss: 2.4702\n",
      "4568: lr: 0.0050, train loss: 2.4867, val loss: 2.5488\n",
      "4569: lr: 0.0050, train loss: 2.4075, val loss: 2.5459\n",
      "4570: lr: 0.0050, train loss: 2.4322, val loss: 2.5167\n",
      "4571: lr: 0.0050, train loss: 2.4580, val loss: 2.5004\n",
      "4572: lr: 0.0050, train loss: 2.5065, val loss: 2.4966\n",
      "4573: lr: 0.0050, train loss: 2.4112, val loss: 2.4737\n",
      "4574: lr: 0.0050, train loss: 2.4452, val loss: 2.4807\n",
      "4575: lr: 0.0050, train loss: 2.4583, val loss: 2.4555\n",
      "4576: lr: 0.0050, train loss: 2.3944, val loss: 2.5343\n",
      "4577: lr: 0.0050, train loss: 2.4591, val loss: 2.5577\n",
      "4578: lr: 0.0050, train loss: 2.3732, val loss: 2.5048\n",
      "4579: lr: 0.0050, train loss: 2.4139, val loss: 2.4932\n",
      "4580: lr: 0.0050, train loss: 2.4594, val loss: 2.4833\n",
      "4581: lr: 0.0050, train loss: 2.4503, val loss: 2.5044\n",
      "4582: lr: 0.0050, train loss: 2.4298, val loss: 2.4772\n",
      "4583: lr: 0.0050, train loss: 2.4981, val loss: 2.4427\n",
      "4584: lr: 0.0050, train loss: 2.4361, val loss: 2.4889\n",
      "4585: lr: 0.0050, train loss: 2.4944, val loss: 2.5190\n",
      "4586: lr: 0.0050, train loss: 2.4635, val loss: 2.5406\n",
      "4587: lr: 0.0050, train loss: 2.4697, val loss: 2.5011\n",
      "4588: lr: 0.0050, train loss: 2.4665, val loss: 2.4584\n",
      "4589: lr: 0.0050, train loss: 2.4529, val loss: 2.5114\n",
      "4590: lr: 0.0050, train loss: 2.4433, val loss: 2.5024\n",
      "4591: lr: 0.0050, train loss: 2.4490, val loss: 2.5663\n",
      "4592: lr: 0.0050, train loss: 2.4623, val loss: 2.5082\n",
      "4593: lr: 0.0050, train loss: 2.4542, val loss: 2.5203\n",
      "4594: lr: 0.0050, train loss: 2.4563, val loss: 2.4737\n",
      "4595: lr: 0.0050, train loss: 2.4743, val loss: 2.4781\n",
      "4596: lr: 0.0050, train loss: 2.4541, val loss: 2.5130\n",
      "4597: lr: 0.0050, train loss: 2.4533, val loss: 2.5201\n",
      "4598: lr: 0.0050, train loss: 2.4222, val loss: 2.4889\n",
      "4599: lr: 0.0050, train loss: 2.4076, val loss: 2.4964\n",
      "4600: lr: 0.0050, train loss: 2.4494, val loss: 2.5401\n",
      "4601: lr: 0.0050, train loss: 2.3980, val loss: 2.4483\n",
      "4602: lr: 0.0050, train loss: 2.4237, val loss: 2.5087\n",
      "4603: lr: 0.0050, train loss: 2.4377, val loss: 2.4846\n",
      "4604: lr: 0.0050, train loss: 2.4301, val loss: 2.4782\n",
      "4605: lr: 0.0050, train loss: 2.4741, val loss: 2.5169\n",
      "4606: lr: 0.0050, train loss: 2.4073, val loss: 2.4840\n",
      "4607: lr: 0.0050, train loss: 2.4374, val loss: 2.4702\n",
      "4608: lr: 0.0050, train loss: 2.4752, val loss: 2.4987\n",
      "4609: lr: 0.0050, train loss: 2.4570, val loss: 2.5031\n",
      "4610: lr: 0.0050, train loss: 2.4507, val loss: 2.4749\n",
      "4611: lr: 0.0050, train loss: 2.4497, val loss: 2.4746\n",
      "4612: lr: 0.0050, train loss: 2.4603, val loss: 2.5118\n",
      "4613: lr: 0.0050, train loss: 2.4928, val loss: 2.4799\n",
      "4614: lr: 0.0050, train loss: 2.4876, val loss: 2.4742\n",
      "4615: lr: 0.0050, train loss: 2.4504, val loss: 2.4863\n",
      "4616: lr: 0.0050, train loss: 2.4583, val loss: 2.5007\n",
      "4617: lr: 0.0050, train loss: 2.4343, val loss: 2.5530\n",
      "4618: lr: 0.0050, train loss: 2.4453, val loss: 2.4295\n",
      "4619: lr: 0.0050, train loss: 2.4601, val loss: 2.4800\n",
      "4620: lr: 0.0050, train loss: 2.4419, val loss: 2.5028\n",
      "4621: lr: 0.0050, train loss: 2.4597, val loss: 2.4806\n",
      "4622: lr: 0.0050, train loss: 2.4679, val loss: 2.4926\n",
      "4623: lr: 0.0050, train loss: 2.4645, val loss: 2.5185\n",
      "4624: lr: 0.0050, train loss: 2.4716, val loss: 2.4705\n",
      "4625: lr: 0.0050, train loss: 2.4688, val loss: 2.5060\n",
      "4626: lr: 0.0050, train loss: 2.4751, val loss: 2.4921\n",
      "4627: lr: 0.0050, train loss: 2.4249, val loss: 2.4582\n",
      "4628: lr: 0.0050, train loss: 2.4594, val loss: 2.4445\n",
      "4629: lr: 0.0050, train loss: 2.4616, val loss: 2.4610\n",
      "4630: lr: 0.0050, train loss: 2.4187, val loss: 2.4734\n",
      "4631: lr: 0.0050, train loss: 2.4773, val loss: 2.4478\n",
      "4632: lr: 0.0050, train loss: 2.4552, val loss: 2.4859\n",
      "4633: lr: 0.0050, train loss: 2.4668, val loss: 2.4560\n",
      "4634: lr: 0.0050, train loss: 2.4661, val loss: 2.4732\n",
      "4635: lr: 0.0050, train loss: 2.4630, val loss: 2.4988\n",
      "4636: lr: 0.0050, train loss: 2.5117, val loss: 2.4954\n",
      "4637: lr: 0.0050, train loss: 2.4605, val loss: 2.5178\n",
      "4638: lr: 0.0050, train loss: 2.4284, val loss: 2.4700\n",
      "4639: lr: 0.0050, train loss: 2.4442, val loss: 2.5177\n",
      "4640: lr: 0.0050, train loss: 2.4140, val loss: 2.5402\n",
      "4641: lr: 0.0050, train loss: 2.4854, val loss: 2.4764\n",
      "4642: lr: 0.0050, train loss: 2.4508, val loss: 2.4794\n",
      "4643: lr: 0.0050, train loss: 2.4433, val loss: 2.5134\n",
      "4644: lr: 0.0050, train loss: 2.4770, val loss: 2.4762\n",
      "4645: lr: 0.0050, train loss: 2.4412, val loss: 2.5018\n",
      "4646: lr: 0.0050, train loss: 2.4052, val loss: 2.5150\n",
      "4647: lr: 0.0050, train loss: 2.4413, val loss: 2.5328\n",
      "4648: lr: 0.0050, train loss: 2.4194, val loss: 2.4862\n",
      "4649: lr: 0.0050, train loss: 2.4712, val loss: 2.5098\n",
      "4650: lr: 0.0050, train loss: 2.4647, val loss: 2.4207\n",
      "4651: lr: 0.0050, train loss: 2.4857, val loss: 2.4510\n",
      "4652: lr: 0.0050, train loss: 2.4415, val loss: 2.4675\n",
      "4653: lr: 0.0050, train loss: 2.4460, val loss: 2.4876\n",
      "4654: lr: 0.0050, train loss: 2.4641, val loss: 2.4910\n",
      "4655: lr: 0.0050, train loss: 2.4745, val loss: 2.4667\n",
      "4656: lr: 0.0050, train loss: 2.4649, val loss: 2.4737\n",
      "4657: lr: 0.0050, train loss: 2.4232, val loss: 2.5307\n",
      "4658: lr: 0.0050, train loss: 2.4106, val loss: 2.5163\n",
      "4659: lr: 0.0050, train loss: 2.4966, val loss: 2.4651\n",
      "4660: lr: 0.0050, train loss: 2.4406, val loss: 2.5192\n",
      "4661: lr: 0.0050, train loss: 2.4504, val loss: 2.5089\n",
      "4662: lr: 0.0050, train loss: 2.4642, val loss: 2.5582\n",
      "4663: lr: 0.0050, train loss: 2.4471, val loss: 2.5067\n",
      "4664: lr: 0.0050, train loss: 2.5038, val loss: 2.4642\n",
      "4665: lr: 0.0050, train loss: 2.4486, val loss: 2.5266\n",
      "4666: lr: 0.0050, train loss: 2.4239, val loss: 2.4448\n",
      "4667: lr: 0.0050, train loss: 2.4979, val loss: 2.4794\n",
      "4668: lr: 0.0050, train loss: 2.4278, val loss: 2.4490\n",
      "4669: lr: 0.0050, train loss: 2.4512, val loss: 2.5287\n",
      "4670: lr: 0.0050, train loss: 2.4783, val loss: 2.4380\n",
      "4671: lr: 0.0050, train loss: 2.4498, val loss: 2.4924\n",
      "4672: lr: 0.0050, train loss: 2.4512, val loss: 2.4886\n",
      "4673: lr: 0.0050, train loss: 2.4406, val loss: 2.4421\n",
      "4674: lr: 0.0050, train loss: 2.4696, val loss: 2.4792\n",
      "4675: lr: 0.0050, train loss: 2.4267, val loss: 2.4881\n",
      "4676: lr: 0.0050, train loss: 2.4230, val loss: 2.4946\n",
      "4677: lr: 0.0050, train loss: 2.4636, val loss: 2.4999\n",
      "4678: lr: 0.0050, train loss: 2.4518, val loss: 2.5403\n",
      "4679: lr: 0.0050, train loss: 2.4188, val loss: 2.5181\n",
      "4680: lr: 0.0050, train loss: 2.4301, val loss: 2.4374\n",
      "4681: lr: 0.0050, train loss: 2.4185, val loss: 2.4846\n",
      "4682: lr: 0.0050, train loss: 2.4888, val loss: 2.5133\n",
      "4683: lr: 0.0050, train loss: 2.4288, val loss: 2.4741\n",
      "4684: lr: 0.0050, train loss: 2.4297, val loss: 2.5046\n",
      "4685: lr: 0.0050, train loss: 2.3956, val loss: 2.5004\n",
      "4686: lr: 0.0050, train loss: 2.4465, val loss: 2.5363\n",
      "4687: lr: 0.0050, train loss: 2.4168, val loss: 2.4550\n",
      "4688: lr: 0.0050, train loss: 2.4240, val loss: 2.4825\n",
      "4689: lr: 0.0050, train loss: 2.4088, val loss: 2.5129\n",
      "4690: lr: 0.0050, train loss: 2.4521, val loss: 2.4750\n",
      "4691: lr: 0.0050, train loss: 2.4012, val loss: 2.4577\n",
      "4692: lr: 0.0050, train loss: 2.4762, val loss: 2.5135\n",
      "4693: lr: 0.0050, train loss: 2.4084, val loss: 2.4837\n",
      "4694: lr: 0.0050, train loss: 2.4265, val loss: 2.4468\n",
      "4695: lr: 0.0050, train loss: 2.4358, val loss: 2.5024\n",
      "4696: lr: 0.0050, train loss: 2.4264, val loss: 2.4709\n",
      "4697: lr: 0.0050, train loss: 2.4122, val loss: 2.4700\n",
      "4698: lr: 0.0050, train loss: 2.4171, val loss: 2.5013\n",
      "4699: lr: 0.0050, train loss: 2.4591, val loss: 2.4852\n",
      "4700: lr: 0.0050, train loss: 2.4687, val loss: 2.5137\n",
      "4701: lr: 0.0050, train loss: 2.4229, val loss: 2.5047\n",
      "4702: lr: 0.0050, train loss: 2.4229, val loss: 2.4859\n",
      "4703: lr: 0.0050, train loss: 2.4397, val loss: 2.4829\n",
      "4704: lr: 0.0050, train loss: 2.4927, val loss: 2.5189\n",
      "4705: lr: 0.0050, train loss: 2.4523, val loss: 2.4800\n",
      "4706: lr: 0.0050, train loss: 2.4219, val loss: 2.5282\n",
      "4707: lr: 0.0050, train loss: 2.3993, val loss: 2.5139\n",
      "4708: lr: 0.0050, train loss: 2.4700, val loss: 2.5316\n",
      "4709: lr: 0.0050, train loss: 2.4278, val loss: 2.5077\n",
      "4710: lr: 0.0050, train loss: 2.4345, val loss: 2.4561\n",
      "4711: lr: 0.0050, train loss: 2.4941, val loss: 2.5381\n",
      "4712: lr: 0.0050, train loss: 2.4314, val loss: 2.5129\n",
      "4713: lr: 0.0050, train loss: 2.4283, val loss: 2.4878\n",
      "4714: lr: 0.0050, train loss: 2.4542, val loss: 2.4846\n",
      "4715: lr: 0.0050, train loss: 2.4580, val loss: 2.4898\n",
      "4716: lr: 0.0050, train loss: 2.4247, val loss: 2.5456\n",
      "4717: lr: 0.0050, train loss: 2.4148, val loss: 2.4901\n",
      "4718: lr: 0.0050, train loss: 2.4407, val loss: 2.4915\n",
      "4719: lr: 0.0050, train loss: 2.4480, val loss: 2.5090\n",
      "4720: lr: 0.0050, train loss: 2.4344, val loss: 2.4514\n",
      "4721: lr: 0.0050, train loss: 2.4528, val loss: 2.4696\n",
      "4722: lr: 0.0050, train loss: 2.4341, val loss: 2.5202\n",
      "4723: lr: 0.0050, train loss: 2.4823, val loss: 2.4919\n",
      "4724: lr: 0.0050, train loss: 2.4279, val loss: 2.4728\n",
      "4725: lr: 0.0050, train loss: 2.4532, val loss: 2.4659\n",
      "4726: lr: 0.0050, train loss: 2.4693, val loss: 2.5379\n",
      "4727: lr: 0.0050, train loss: 2.4748, val loss: 2.4754\n",
      "4728: lr: 0.0050, train loss: 2.4118, val loss: 2.5310\n",
      "4729: lr: 0.0050, train loss: 2.4393, val loss: 2.5056\n",
      "4730: lr: 0.0050, train loss: 2.4841, val loss: 2.4892\n",
      "4731: lr: 0.0050, train loss: 2.4761, val loss: 2.4366\n",
      "4732: lr: 0.0050, train loss: 2.4393, val loss: 2.4628\n",
      "4733: lr: 0.0050, train loss: 2.4411, val loss: 2.5044\n",
      "4734: lr: 0.0050, train loss: 2.4161, val loss: 2.4484\n",
      "4735: lr: 0.0050, train loss: 2.5096, val loss: 2.4425\n",
      "4736: lr: 0.0050, train loss: 2.4611, val loss: 2.4864\n",
      "4737: lr: 0.0050, train loss: 2.4354, val loss: 2.5276\n",
      "4738: lr: 0.0050, train loss: 2.4557, val loss: 2.5186\n",
      "4739: lr: 0.0050, train loss: 2.4712, val loss: 2.4726\n",
      "4740: lr: 0.0050, train loss: 2.4414, val loss: 2.4990\n",
      "4741: lr: 0.0050, train loss: 2.4739, val loss: 2.4721\n",
      "4742: lr: 0.0050, train loss: 2.4514, val loss: 2.4946\n",
      "4743: lr: 0.0050, train loss: 2.4228, val loss: 2.5069\n",
      "4744: lr: 0.0050, train loss: 2.3976, val loss: 2.4964\n",
      "4745: lr: 0.0050, train loss: 2.4867, val loss: 2.4943\n",
      "4746: lr: 0.0050, train loss: 2.4466, val loss: 2.4483\n",
      "4747: lr: 0.0050, train loss: 2.4389, val loss: 2.4861\n",
      "4748: lr: 0.0050, train loss: 2.4523, val loss: 2.4935\n",
      "4749: lr: 0.0050, train loss: 2.4565, val loss: 2.4653\n",
      "4750: lr: 0.0050, train loss: 2.5114, val loss: 2.5070\n",
      "4751: lr: 0.0050, train loss: 2.4354, val loss: 2.5201\n",
      "4752: lr: 0.0050, train loss: 2.4143, val loss: 2.5124\n",
      "4753: lr: 0.0050, train loss: 2.4251, val loss: 2.4710\n",
      "4754: lr: 0.0050, train loss: 2.4754, val loss: 2.5055\n",
      "4755: lr: 0.0050, train loss: 2.4309, val loss: 2.4738\n",
      "4756: lr: 0.0050, train loss: 2.4447, val loss: 2.4526\n",
      "4757: lr: 0.0050, train loss: 2.4285, val loss: 2.5159\n",
      "4758: lr: 0.0050, train loss: 2.4189, val loss: 2.5365\n",
      "4759: lr: 0.0050, train loss: 2.4638, val loss: 2.4632\n",
      "4760: lr: 0.0050, train loss: 2.5342, val loss: 2.4922\n",
      "4761: lr: 0.0050, train loss: 2.4511, val loss: 2.4982\n",
      "4762: lr: 0.0050, train loss: 2.5184, val loss: 2.5306\n",
      "4763: lr: 0.0050, train loss: 2.4786, val loss: 2.5095\n",
      "4764: lr: 0.0050, train loss: 2.4596, val loss: 2.4541\n",
      "4765: lr: 0.0050, train loss: 2.4633, val loss: 2.4343\n",
      "4766: lr: 0.0050, train loss: 2.4599, val loss: 2.4714\n",
      "4767: lr: 0.0050, train loss: 2.4414, val loss: 2.4563\n",
      "4768: lr: 0.0050, train loss: 2.5137, val loss: 2.4586\n",
      "4769: lr: 0.0050, train loss: 2.4223, val loss: 2.4454\n",
      "4770: lr: 0.0050, train loss: 2.4469, val loss: 2.4494\n",
      "4771: lr: 0.0050, train loss: 2.4828, val loss: 2.4494\n",
      "4772: lr: 0.0050, train loss: 2.4600, val loss: 2.4834\n",
      "4773: lr: 0.0050, train loss: 2.4822, val loss: 2.4482\n",
      "4774: lr: 0.0050, train loss: 2.4263, val loss: 2.4706\n",
      "4775: lr: 0.0050, train loss: 2.4613, val loss: 2.5493\n",
      "4776: lr: 0.0050, train loss: 2.4505, val loss: 2.4649\n",
      "4777: lr: 0.0050, train loss: 2.4810, val loss: 2.4783\n",
      "4778: lr: 0.0050, train loss: 2.4388, val loss: 2.4896\n",
      "4779: lr: 0.0050, train loss: 2.4501, val loss: 2.4832\n",
      "4780: lr: 0.0050, train loss: 2.4848, val loss: 2.5623\n",
      "4781: lr: 0.0050, train loss: 2.4548, val loss: 2.4631\n",
      "4782: lr: 0.0050, train loss: 2.4704, val loss: 2.5149\n",
      "4783: lr: 0.0050, train loss: 2.4557, val loss: 2.4940\n",
      "4784: lr: 0.0050, train loss: 2.4275, val loss: 2.4660\n",
      "4785: lr: 0.0050, train loss: 2.4834, val loss: 2.4332\n",
      "4786: lr: 0.0050, train loss: 2.4698, val loss: 2.5175\n",
      "4787: lr: 0.0050, train loss: 2.4723, val loss: 2.4664\n",
      "4788: lr: 0.0050, train loss: 2.4169, val loss: 2.4421\n",
      "4789: lr: 0.0050, train loss: 2.4711, val loss: 2.4529\n",
      "4790: lr: 0.0050, train loss: 2.4380, val loss: 2.4677\n",
      "4791: lr: 0.0050, train loss: 2.4731, val loss: 2.5188\n",
      "4792: lr: 0.0050, train loss: 2.4587, val loss: 2.5123\n",
      "4793: lr: 0.0050, train loss: 2.4534, val loss: 2.4485\n",
      "4794: lr: 0.0050, train loss: 2.4328, val loss: 2.4790\n",
      "4795: lr: 0.0050, train loss: 2.4254, val loss: 2.5303\n",
      "4796: lr: 0.0050, train loss: 2.4669, val loss: 2.4899\n",
      "4797: lr: 0.0050, train loss: 2.4911, val loss: 2.5202\n",
      "4798: lr: 0.0050, train loss: 2.4774, val loss: 2.4827\n",
      "4799: lr: 0.0050, train loss: 2.4634, val loss: 2.4643\n",
      "4800: lr: 0.0050, train loss: 2.4292, val loss: 2.4901\n",
      "4801: lr: 0.0050, train loss: 2.4166, val loss: 2.4907\n",
      "4802: lr: 0.0050, train loss: 2.4641, val loss: 2.4596\n",
      "4803: lr: 0.0050, train loss: 2.4725, val loss: 2.4709\n",
      "4804: lr: 0.0050, train loss: 2.4113, val loss: 2.4737\n",
      "4805: lr: 0.0050, train loss: 2.4634, val loss: 2.4171\n",
      "4806: lr: 0.0050, train loss: 2.4571, val loss: 2.5157\n",
      "4807: lr: 0.0050, train loss: 2.4185, val loss: 2.4982\n",
      "4808: lr: 0.0050, train loss: 2.4314, val loss: 2.5090\n",
      "4809: lr: 0.0050, train loss: 2.4687, val loss: 2.5220\n",
      "4810: lr: 0.0050, train loss: 2.4770, val loss: 2.4901\n",
      "4811: lr: 0.0050, train loss: 2.4342, val loss: 2.5273\n",
      "4812: lr: 0.0050, train loss: 2.4574, val loss: 2.4562\n",
      "4813: lr: 0.0050, train loss: 2.3984, val loss: 2.4538\n",
      "4814: lr: 0.0050, train loss: 2.4262, val loss: 2.4688\n",
      "4815: lr: 0.0050, train loss: 2.4294, val loss: 2.5061\n",
      "4816: lr: 0.0050, train loss: 2.4213, val loss: 2.4944\n",
      "4817: lr: 0.0050, train loss: 2.4493, val loss: 2.5005\n",
      "4818: lr: 0.0050, train loss: 2.4333, val loss: 2.4911\n",
      "4819: lr: 0.0050, train loss: 2.4610, val loss: 2.4700\n",
      "4820: lr: 0.0050, train loss: 2.4852, val loss: 2.5415\n",
      "4821: lr: 0.0050, train loss: 2.4427, val loss: 2.4743\n",
      "4822: lr: 0.0050, train loss: 2.4715, val loss: 2.4769\n",
      "4823: lr: 0.0050, train loss: 2.4421, val loss: 2.4375\n",
      "4824: lr: 0.0050, train loss: 2.4182, val loss: 2.5016\n",
      "4825: lr: 0.0050, train loss: 2.4476, val loss: 2.4332\n",
      "4826: lr: 0.0050, train loss: 2.4520, val loss: 2.5035\n",
      "4827: lr: 0.0050, train loss: 2.4370, val loss: 2.4994\n",
      "4828: lr: 0.0050, train loss: 2.4700, val loss: 2.5142\n",
      "4829: lr: 0.0050, train loss: 2.4431, val loss: 2.5192\n",
      "4830: lr: 0.0050, train loss: 2.4657, val loss: 2.4743\n",
      "4831: lr: 0.0050, train loss: 2.4843, val loss: 2.4912\n",
      "4832: lr: 0.0050, train loss: 2.4644, val loss: 2.4896\n",
      "4833: lr: 0.0050, train loss: 2.4543, val loss: 2.4470\n",
      "4834: lr: 0.0050, train loss: 2.4727, val loss: 2.4750\n",
      "4835: lr: 0.0050, train loss: 2.4485, val loss: 2.4637\n",
      "4836: lr: 0.0050, train loss: 2.4376, val loss: 2.4676\n",
      "4837: lr: 0.0050, train loss: 2.4920, val loss: 2.5027\n",
      "4838: lr: 0.0050, train loss: 2.4726, val loss: 2.5101\n",
      "4839: lr: 0.0050, train loss: 2.4562, val loss: 2.4918\n",
      "4840: lr: 0.0050, train loss: 2.4125, val loss: 2.4798\n",
      "4841: lr: 0.0050, train loss: 2.4927, val loss: 2.4820\n",
      "4842: lr: 0.0050, train loss: 2.4675, val loss: 2.4413\n",
      "4843: lr: 0.0050, train loss: 2.4006, val loss: 2.4361\n",
      "4844: lr: 0.0050, train loss: 2.4529, val loss: 2.4719\n",
      "4845: lr: 0.0050, train loss: 2.4806, val loss: 2.5183\n",
      "4846: lr: 0.0050, train loss: 2.4454, val loss: 2.4679\n",
      "4847: lr: 0.0050, train loss: 2.4302, val loss: 2.5065\n",
      "4848: lr: 0.0050, train loss: 2.4566, val loss: 2.4745\n",
      "4849: lr: 0.0050, train loss: 2.4209, val loss: 2.4825\n",
      "4850: lr: 0.0050, train loss: 2.4535, val loss: 2.4630\n",
      "4851: lr: 0.0050, train loss: 2.4529, val loss: 2.5099\n",
      "4852: lr: 0.0050, train loss: 2.4396, val loss: 2.5160\n",
      "4853: lr: 0.0050, train loss: 2.4342, val loss: 2.4950\n",
      "4854: lr: 0.0050, train loss: 2.4833, val loss: 2.4490\n",
      "4855: lr: 0.0050, train loss: 2.4874, val loss: 2.4537\n",
      "4856: lr: 0.0050, train loss: 2.4846, val loss: 2.4887\n",
      "4857: lr: 0.0050, train loss: 2.4437, val loss: 2.4424\n",
      "4858: lr: 0.0050, train loss: 2.4604, val loss: 2.5075\n",
      "4859: lr: 0.0050, train loss: 2.5021, val loss: 2.5013\n",
      "4860: lr: 0.0050, train loss: 2.4603, val loss: 2.5416\n",
      "4861: lr: 0.0050, train loss: 2.4958, val loss: 2.5285\n",
      "4862: lr: 0.0050, train loss: 2.4217, val loss: 2.5232\n",
      "4863: lr: 0.0050, train loss: 2.4590, val loss: 2.5065\n",
      "4864: lr: 0.0050, train loss: 2.4764, val loss: 2.4648\n",
      "4865: lr: 0.0050, train loss: 2.4769, val loss: 2.4971\n",
      "4866: lr: 0.0050, train loss: 2.4821, val loss: 2.4795\n",
      "4867: lr: 0.0050, train loss: 2.4950, val loss: 2.5024\n",
      "4868: lr: 0.0050, train loss: 2.4271, val loss: 2.5121\n",
      "4869: lr: 0.0050, train loss: 2.4618, val loss: 2.5103\n",
      "4870: lr: 0.0050, train loss: 2.4338, val loss: 2.5061\n",
      "4871: lr: 0.0050, train loss: 2.4079, val loss: 2.4969\n",
      "4872: lr: 0.0050, train loss: 2.4354, val loss: 2.4859\n",
      "4873: lr: 0.0050, train loss: 2.4695, val loss: 2.5058\n",
      "4874: lr: 0.0050, train loss: 2.4641, val loss: 2.4886\n",
      "4875: lr: 0.0050, train loss: 2.4426, val loss: 2.5085\n",
      "4876: lr: 0.0050, train loss: 2.4490, val loss: 2.5114\n",
      "4877: lr: 0.0050, train loss: 2.4514, val loss: 2.5205\n",
      "4878: lr: 0.0050, train loss: 2.4894, val loss: 2.4649\n",
      "4879: lr: 0.0050, train loss: 2.4562, val loss: 2.5223\n",
      "4880: lr: 0.0050, train loss: 2.4638, val loss: 2.4361\n",
      "4881: lr: 0.0050, train loss: 2.4787, val loss: 2.5024\n",
      "4882: lr: 0.0050, train loss: 2.4606, val loss: 2.4851\n",
      "4883: lr: 0.0050, train loss: 2.3777, val loss: 2.4711\n",
      "4884: lr: 0.0050, train loss: 2.4727, val loss: 2.5362\n",
      "4885: lr: 0.0050, train loss: 2.4306, val loss: 2.4969\n",
      "4886: lr: 0.0050, train loss: 2.4283, val loss: 2.4647\n",
      "4887: lr: 0.0050, train loss: 2.4350, val loss: 2.4998\n",
      "4888: lr: 0.0050, train loss: 2.4386, val loss: 2.4672\n",
      "4889: lr: 0.0050, train loss: 2.4720, val loss: 2.4284\n",
      "4890: lr: 0.0050, train loss: 2.4461, val loss: 2.5553\n",
      "4891: lr: 0.0050, train loss: 2.4157, val loss: 2.4724\n",
      "4892: lr: 0.0050, train loss: 2.4268, val loss: 2.5016\n",
      "4893: lr: 0.0050, train loss: 2.4892, val loss: 2.5290\n",
      "4894: lr: 0.0050, train loss: 2.4199, val loss: 2.5393\n",
      "4895: lr: 0.0050, train loss: 2.4458, val loss: 2.5086\n",
      "4896: lr: 0.0050, train loss: 2.4335, val loss: 2.4213\n",
      "4897: lr: 0.0050, train loss: 2.4735, val loss: 2.4573\n",
      "4898: lr: 0.0050, train loss: 2.4805, val loss: 2.5022\n",
      "4899: lr: 0.0050, train loss: 2.4552, val loss: 2.4810\n",
      "4900: lr: 0.0050, train loss: 2.4621, val loss: 2.5140\n",
      "4901: lr: 0.0050, train loss: 2.4867, val loss: 2.4830\n",
      "4902: lr: 0.0050, train loss: 2.4261, val loss: 2.5445\n",
      "4903: lr: 0.0050, train loss: 2.4174, val loss: 2.4910\n",
      "4904: lr: 0.0050, train loss: 2.4553, val loss: 2.4809\n",
      "4905: lr: 0.0050, train loss: 2.4357, val loss: 2.4464\n",
      "4906: lr: 0.0050, train loss: 2.4874, val loss: 2.4454\n",
      "4907: lr: 0.0050, train loss: 2.4773, val loss: 2.4545\n",
      "4908: lr: 0.0050, train loss: 2.4409, val loss: 2.5089\n",
      "4909: lr: 0.0050, train loss: 2.4580, val loss: 2.4237\n",
      "4910: lr: 0.0050, train loss: 2.4665, val loss: 2.4471\n",
      "4911: lr: 0.0050, train loss: 2.4778, val loss: 2.4818\n",
      "4912: lr: 0.0050, train loss: 2.4509, val loss: 2.4578\n",
      "4913: lr: 0.0050, train loss: 2.4531, val loss: 2.4628\n",
      "4914: lr: 0.0050, train loss: 2.4672, val loss: 2.4973\n",
      "4915: lr: 0.0050, train loss: 2.4814, val loss: 2.4233\n",
      "4916: lr: 0.0050, train loss: 2.4614, val loss: 2.4815\n",
      "4917: lr: 0.0050, train loss: 2.4993, val loss: 2.4894\n",
      "4918: lr: 0.0050, train loss: 2.4346, val loss: 2.4461\n",
      "4919: lr: 0.0050, train loss: 2.4444, val loss: 2.5336\n",
      "4920: lr: 0.0050, train loss: 2.4246, val loss: 2.5271\n",
      "4921: lr: 0.0050, train loss: 2.4720, val loss: 2.4739\n",
      "4922: lr: 0.0050, train loss: 2.4859, val loss: 2.4869\n",
      "4923: lr: 0.0050, train loss: 2.4753, val loss: 2.4580\n",
      "4924: lr: 0.0050, train loss: 2.4857, val loss: 2.5512\n",
      "4925: lr: 0.0050, train loss: 2.4275, val loss: 2.4895\n",
      "4926: lr: 0.0050, train loss: 2.4375, val loss: 2.4955\n",
      "4927: lr: 0.0050, train loss: 2.4611, val loss: 2.4065\n",
      "4928: lr: 0.0050, train loss: 2.4507, val loss: 2.4990\n",
      "4929: lr: 0.0050, train loss: 2.4224, val loss: 2.4650\n",
      "4930: lr: 0.0050, train loss: 2.4498, val loss: 2.4966\n",
      "4931: lr: 0.0050, train loss: 2.4702, val loss: 2.5013\n",
      "4932: lr: 0.0050, train loss: 2.4427, val loss: 2.5295\n",
      "4933: lr: 0.0050, train loss: 2.4406, val loss: 2.4618\n",
      "4934: lr: 0.0050, train loss: 2.4380, val loss: 2.4946\n",
      "4935: lr: 0.0050, train loss: 2.4523, val loss: 2.5131\n",
      "4936: lr: 0.0050, train loss: 2.4400, val loss: 2.4786\n",
      "4937: lr: 0.0050, train loss: 2.4419, val loss: 2.4681\n",
      "4938: lr: 0.0050, train loss: 2.4362, val loss: 2.4839\n",
      "4939: lr: 0.0050, train loss: 2.4375, val loss: 2.5167\n",
      "4940: lr: 0.0050, train loss: 2.4966, val loss: 2.4920\n",
      "4941: lr: 0.0050, train loss: 2.4858, val loss: 2.4933\n",
      "4942: lr: 0.0050, train loss: 2.4221, val loss: 2.5180\n",
      "4943: lr: 0.0050, train loss: 2.4802, val loss: 2.4787\n",
      "4944: lr: 0.0050, train loss: 2.3929, val loss: 2.5087\n",
      "4945: lr: 0.0050, train loss: 2.4796, val loss: 2.4720\n",
      "4946: lr: 0.0050, train loss: 2.4675, val loss: 2.4582\n",
      "4947: lr: 0.0050, train loss: 2.4579, val loss: 2.4801\n",
      "4948: lr: 0.0050, train loss: 2.4417, val loss: 2.4414\n",
      "4949: lr: 0.0050, train loss: 2.4554, val loss: 2.4596\n",
      "4950: lr: 0.0050, train loss: 2.4782, val loss: 2.4863\n",
      "4951: lr: 0.0050, train loss: 2.4375, val loss: 2.5574\n",
      "4952: lr: 0.0050, train loss: 2.4586, val loss: 2.5134\n",
      "4953: lr: 0.0050, train loss: 2.4433, val loss: 2.4694\n",
      "4954: lr: 0.0050, train loss: 2.4508, val loss: 2.5537\n",
      "4955: lr: 0.0050, train loss: 2.4200, val loss: 2.5267\n",
      "4956: lr: 0.0050, train loss: 2.4725, val loss: 2.5240\n",
      "4957: lr: 0.0050, train loss: 2.4798, val loss: 2.4694\n",
      "4958: lr: 0.0050, train loss: 2.4518, val loss: 2.5022\n",
      "4959: lr: 0.0050, train loss: 2.4695, val loss: 2.4907\n",
      "4960: lr: 0.0050, train loss: 2.4249, val loss: 2.5156\n",
      "4961: lr: 0.0050, train loss: 2.5004, val loss: 2.4936\n",
      "4962: lr: 0.0050, train loss: 2.5090, val loss: 2.4821\n",
      "4963: lr: 0.0050, train loss: 2.4108, val loss: 2.5227\n",
      "4964: lr: 0.0050, train loss: 2.4338, val loss: 2.4765\n",
      "4965: lr: 0.0050, train loss: 2.4425, val loss: 2.4861\n",
      "4966: lr: 0.0050, train loss: 2.4441, val loss: 2.5054\n",
      "4967: lr: 0.0050, train loss: 2.5087, val loss: 2.4837\n",
      "4968: lr: 0.0050, train loss: 2.4313, val loss: 2.4839\n",
      "4969: lr: 0.0050, train loss: 2.4311, val loss: 2.4887\n",
      "4970: lr: 0.0050, train loss: 2.4575, val loss: 2.4800\n",
      "4971: lr: 0.0050, train loss: 2.4313, val loss: 2.4626\n",
      "4972: lr: 0.0050, train loss: 2.4099, val loss: 2.4967\n",
      "4973: lr: 0.0050, train loss: 2.5120, val loss: 2.5102\n",
      "4974: lr: 0.0050, train loss: 2.4274, val loss: 2.5093\n",
      "4975: lr: 0.0050, train loss: 2.4543, val loss: 2.4772\n",
      "4976: lr: 0.0050, train loss: 2.4181, val loss: 2.4848\n",
      "4977: lr: 0.0050, train loss: 2.4499, val loss: 2.4954\n",
      "4978: lr: 0.0050, train loss: 2.4775, val loss: 2.4874\n",
      "4979: lr: 0.0050, train loss: 2.4488, val loss: 2.4896\n",
      "4980: lr: 0.0050, train loss: 2.4362, val loss: 2.5054\n",
      "4981: lr: 0.0050, train loss: 2.4657, val loss: 2.5024\n",
      "4982: lr: 0.0050, train loss: 2.4840, val loss: 2.5244\n",
      "4983: lr: 0.0050, train loss: 2.4244, val loss: 2.4917\n",
      "4984: lr: 0.0050, train loss: 2.4784, val loss: 2.5003\n",
      "4985: lr: 0.0050, train loss: 2.4236, val loss: 2.4886\n",
      "4986: lr: 0.0050, train loss: 2.4747, val loss: 2.5432\n",
      "4987: lr: 0.0050, train loss: 2.4324, val loss: 2.5162\n",
      "4988: lr: 0.0050, train loss: 2.4838, val loss: 2.5255\n",
      "4989: lr: 0.0050, train loss: 2.4549, val loss: 2.4771\n",
      "4990: lr: 0.0050, train loss: 2.4822, val loss: 2.4982\n",
      "4991: lr: 0.0050, train loss: 2.5168, val loss: 2.5007\n",
      "4992: lr: 0.0050, train loss: 2.4455, val loss: 2.4855\n",
      "4993: lr: 0.0050, train loss: 2.4748, val loss: 2.4605\n",
      "4994: lr: 0.0050, train loss: 2.4733, val loss: 2.4672\n",
      "4995: lr: 0.0050, train loss: 2.4160, val loss: 2.4740\n",
      "4996: lr: 0.0050, train loss: 2.4433, val loss: 2.4471\n",
      "4997: lr: 0.0050, train loss: 2.4626, val loss: 2.4216\n",
      "4998: lr: 0.0050, train loss: 2.4692, val loss: 2.5160\n",
      "4999: lr: 0.0050, train loss: 2.4798, val loss: 2.4926\n",
      "5000: lr: 0.0050, train loss: 2.4489, val loss: 2.4624\n",
      "5001: lr: 0.0050, train loss: 2.4825, val loss: 2.5145\n",
      "5002: lr: 0.0050, train loss: 2.4640, val loss: 2.4666\n",
      "5003: lr: 0.0050, train loss: 2.4591, val loss: 2.4364\n",
      "5004: lr: 0.0050, train loss: 2.4798, val loss: 2.5018\n",
      "5005: lr: 0.0050, train loss: 2.4649, val loss: 2.4645\n",
      "5006: lr: 0.0050, train loss: 2.4102, val loss: 2.4548\n",
      "5007: lr: 0.0050, train loss: 2.4768, val loss: 2.4822\n",
      "5008: lr: 0.0050, train loss: 2.4817, val loss: 2.4960\n",
      "5009: lr: 0.0050, train loss: 2.4927, val loss: 2.4881\n",
      "5010: lr: 0.0050, train loss: 2.4101, val loss: 2.4643\n",
      "5011: lr: 0.0050, train loss: 2.4778, val loss: 2.4588\n",
      "5012: lr: 0.0050, train loss: 2.4674, val loss: 2.4517\n",
      "5013: lr: 0.0050, train loss: 2.4551, val loss: 2.4574\n",
      "5014: lr: 0.0050, train loss: 2.4383, val loss: 2.5068\n",
      "5015: lr: 0.0050, train loss: 2.4668, val loss: 2.4832\n",
      "5016: lr: 0.0050, train loss: 2.4651, val loss: 2.4812\n",
      "5017: lr: 0.0050, train loss: 2.4611, val loss: 2.4794\n",
      "5018: lr: 0.0050, train loss: 2.4273, val loss: 2.5144\n",
      "5019: lr: 0.0050, train loss: 2.4958, val loss: 2.4779\n",
      "5020: lr: 0.0050, train loss: 2.4087, val loss: 2.5213\n",
      "5021: lr: 0.0050, train loss: 2.4495, val loss: 2.4764\n",
      "5022: lr: 0.0050, train loss: 2.4629, val loss: 2.4963\n",
      "5023: lr: 0.0050, train loss: 2.4299, val loss: 2.5462\n",
      "5024: lr: 0.0050, train loss: 2.4553, val loss: 2.5127\n",
      "5025: lr: 0.0050, train loss: 2.4673, val loss: 2.4825\n",
      "5026: lr: 0.0050, train loss: 2.4939, val loss: 2.5016\n",
      "5027: lr: 0.0050, train loss: 2.4805, val loss: 2.4760\n",
      "5028: lr: 0.0050, train loss: 2.4921, val loss: 2.4296\n",
      "5029: lr: 0.0050, train loss: 2.4846, val loss: 2.5100\n",
      "5030: lr: 0.0050, train loss: 2.4699, val loss: 2.5135\n",
      "5031: lr: 0.0050, train loss: 2.4554, val loss: 2.5181\n",
      "5032: lr: 0.0050, train loss: 2.4237, val loss: 2.4186\n",
      "5033: lr: 0.0050, train loss: 2.4485, val loss: 2.4940\n",
      "5034: lr: 0.0050, train loss: 2.4190, val loss: 2.4609\n",
      "5035: lr: 0.0050, train loss: 2.4910, val loss: 2.5038\n",
      "5036: lr: 0.0050, train loss: 2.4240, val loss: 2.4595\n",
      "5037: lr: 0.0050, train loss: 2.4291, val loss: 2.5425\n",
      "5038: lr: 0.0050, train loss: 2.4964, val loss: 2.4895\n",
      "5039: lr: 0.0050, train loss: 2.4721, val loss: 2.5312\n",
      "5040: lr: 0.0050, train loss: 2.4431, val loss: 2.4765\n",
      "5041: lr: 0.0050, train loss: 2.4338, val loss: 2.4436\n",
      "5042: lr: 0.0050, train loss: 2.4519, val loss: 2.4645\n",
      "5043: lr: 0.0050, train loss: 2.4438, val loss: 2.4613\n",
      "5044: lr: 0.0050, train loss: 2.4513, val loss: 2.4833\n",
      "5045: lr: 0.0050, train loss: 2.4001, val loss: 2.5071\n",
      "5046: lr: 0.0050, train loss: 2.4586, val loss: 2.5168\n",
      "5047: lr: 0.0050, train loss: 2.4442, val loss: 2.4859\n",
      "5048: lr: 0.0050, train loss: 2.4387, val loss: 2.4900\n",
      "5049: lr: 0.0050, train loss: 2.4738, val loss: 2.5004\n",
      "5050: lr: 0.0050, train loss: 2.4603, val loss: 2.5075\n",
      "5051: lr: 0.0050, train loss: 2.4468, val loss: 2.4267\n",
      "5052: lr: 0.0050, train loss: 2.4306, val loss: 2.4849\n",
      "5053: lr: 0.0050, train loss: 2.4505, val loss: 2.4784\n",
      "5054: lr: 0.0050, train loss: 2.4468, val loss: 2.4718\n",
      "5055: lr: 0.0050, train loss: 2.4209, val loss: 2.4866\n",
      "5056: lr: 0.0050, train loss: 2.4479, val loss: 2.5539\n",
      "5057: lr: 0.0050, train loss: 2.4864, val loss: 2.4725\n",
      "5058: lr: 0.0050, train loss: 2.4643, val loss: 2.4949\n",
      "5059: lr: 0.0050, train loss: 2.4482, val loss: 2.4315\n",
      "5060: lr: 0.0050, train loss: 2.4274, val loss: 2.4684\n",
      "5061: lr: 0.0050, train loss: 2.4354, val loss: 2.4905\n",
      "5062: lr: 0.0050, train loss: 2.4094, val loss: 2.4815\n",
      "5063: lr: 0.0050, train loss: 2.5033, val loss: 2.4716\n",
      "5064: lr: 0.0050, train loss: 2.4698, val loss: 2.4870\n",
      "5065: lr: 0.0050, train loss: 2.4809, val loss: 2.5349\n",
      "5066: lr: 0.0050, train loss: 2.4805, val loss: 2.5115\n",
      "5067: lr: 0.0050, train loss: 2.4790, val loss: 2.4311\n",
      "5068: lr: 0.0050, train loss: 2.4456, val loss: 2.5308\n",
      "5069: lr: 0.0050, train loss: 2.4367, val loss: 2.4463\n",
      "5070: lr: 0.0050, train loss: 2.4832, val loss: 2.4511\n",
      "5071: lr: 0.0050, train loss: 2.4536, val loss: 2.4917\n",
      "5072: lr: 0.0050, train loss: 2.4503, val loss: 2.4916\n",
      "5073: lr: 0.0050, train loss: 2.4439, val loss: 2.5337\n",
      "5074: lr: 0.0050, train loss: 2.4511, val loss: 2.4788\n",
      "5075: lr: 0.0050, train loss: 2.4746, val loss: 2.4695\n",
      "5076: lr: 0.0050, train loss: 2.5017, val loss: 2.5281\n",
      "5077: lr: 0.0050, train loss: 2.4090, val loss: 2.4818\n",
      "5078: lr: 0.0050, train loss: 2.4998, val loss: 2.4997\n",
      "5079: lr: 0.0050, train loss: 2.4199, val loss: 2.5116\n",
      "5080: lr: 0.0050, train loss: 2.4233, val loss: 2.5043\n",
      "5081: lr: 0.0050, train loss: 2.4426, val loss: 2.4189\n",
      "5082: lr: 0.0050, train loss: 2.4798, val loss: 2.5429\n",
      "5083: lr: 0.0050, train loss: 2.4275, val loss: 2.4482\n",
      "5084: lr: 0.0050, train loss: 2.5196, val loss: 2.5278\n",
      "5085: lr: 0.0050, train loss: 2.4643, val loss: 2.4692\n",
      "5086: lr: 0.0050, train loss: 2.4460, val loss: 2.4320\n",
      "5087: lr: 0.0050, train loss: 2.4670, val loss: 2.5401\n",
      "5088: lr: 0.0050, train loss: 2.4742, val loss: 2.5033\n",
      "5089: lr: 0.0050, train loss: 2.4983, val loss: 2.5590\n",
      "5090: lr: 0.0050, train loss: 2.4527, val loss: 2.4675\n",
      "5091: lr: 0.0050, train loss: 2.4412, val loss: 2.4410\n",
      "5092: lr: 0.0050, train loss: 2.4462, val loss: 2.4869\n",
      "5093: lr: 0.0050, train loss: 2.4304, val loss: 2.4746\n",
      "5094: lr: 0.0050, train loss: 2.4270, val loss: 2.4607\n",
      "5095: lr: 0.0050, train loss: 2.4705, val loss: 2.5349\n",
      "5096: lr: 0.0050, train loss: 2.4618, val loss: 2.4888\n",
      "5097: lr: 0.0050, train loss: 2.4078, val loss: 2.5249\n",
      "5098: lr: 0.0050, train loss: 2.4118, val loss: 2.4836\n",
      "5099: lr: 0.0050, train loss: 2.4486, val loss: 2.4562\n",
      "5100: lr: 0.0050, train loss: 2.4669, val loss: 2.4574\n",
      "5101: lr: 0.0050, train loss: 2.4570, val loss: 2.4571\n",
      "5102: lr: 0.0050, train loss: 2.4868, val loss: 2.5097\n",
      "5103: lr: 0.0050, train loss: 2.4283, val loss: 2.4529\n",
      "5104: lr: 0.0050, train loss: 2.4628, val loss: 2.5023\n",
      "5105: lr: 0.0050, train loss: 2.4173, val loss: 2.4794\n",
      "5106: lr: 0.0050, train loss: 2.4485, val loss: 2.4617\n",
      "5107: lr: 0.0050, train loss: 2.4422, val loss: 2.4664\n",
      "5108: lr: 0.0050, train loss: 2.4767, val loss: 2.4556\n",
      "5109: lr: 0.0050, train loss: 2.4720, val loss: 2.4755\n",
      "5110: lr: 0.0050, train loss: 2.4422, val loss: 2.4361\n",
      "5111: lr: 0.0050, train loss: 2.4692, val loss: 2.4652\n",
      "5112: lr: 0.0050, train loss: 2.3942, val loss: 2.4874\n",
      "5113: lr: 0.0050, train loss: 2.4871, val loss: 2.4154\n",
      "5114: lr: 0.0050, train loss: 2.4458, val loss: 2.4883\n",
      "5115: lr: 0.0050, train loss: 2.4442, val loss: 2.4676\n",
      "5116: lr: 0.0050, train loss: 2.4665, val loss: 2.4758\n",
      "5117: lr: 0.0050, train loss: 2.4348, val loss: 2.4746\n",
      "5118: lr: 0.0050, train loss: 2.4530, val loss: 2.4191\n",
      "5119: lr: 0.0050, train loss: 2.4585, val loss: 2.5052\n",
      "5120: lr: 0.0050, train loss: 2.4740, val loss: 2.4441\n",
      "5121: lr: 0.0050, train loss: 2.4363, val loss: 2.4655\n",
      "5122: lr: 0.0050, train loss: 2.4402, val loss: 2.5330\n",
      "5123: lr: 0.0050, train loss: 2.4353, val loss: 2.4740\n",
      "5124: lr: 0.0050, train loss: 2.4177, val loss: 2.4674\n",
      "5125: lr: 0.0050, train loss: 2.3970, val loss: 2.4965\n",
      "5126: lr: 0.0050, train loss: 2.4622, val loss: 2.4911\n",
      "5127: lr: 0.0050, train loss: 2.4941, val loss: 2.4133\n",
      "5128: lr: 0.0050, train loss: 2.4628, val loss: 2.4600\n",
      "5129: lr: 0.0050, train loss: 2.4533, val loss: 2.5197\n",
      "5130: lr: 0.0050, train loss: 2.4385, val loss: 2.5243\n",
      "5131: lr: 0.0050, train loss: 2.4638, val loss: 2.4908\n",
      "5132: lr: 0.0050, train loss: 2.4448, val loss: 2.4999\n",
      "5133: lr: 0.0050, train loss: 2.4811, val loss: 2.5366\n",
      "5134: lr: 0.0050, train loss: 2.4283, val loss: 2.5285\n",
      "5135: lr: 0.0050, train loss: 2.4551, val loss: 2.4708\n",
      "5136: lr: 0.0050, train loss: 2.4679, val loss: 2.4378\n",
      "5137: lr: 0.0050, train loss: 2.4726, val loss: 2.4328\n",
      "5138: lr: 0.0050, train loss: 2.4649, val loss: 2.5021\n",
      "5139: lr: 0.0050, train loss: 2.4479, val loss: 2.5312\n",
      "5140: lr: 0.0050, train loss: 2.4436, val loss: 2.4425\n",
      "5141: lr: 0.0050, train loss: 2.4324, val loss: 2.4518\n",
      "5142: lr: 0.0050, train loss: 2.4441, val loss: 2.4887\n",
      "5143: lr: 0.0050, train loss: 2.4716, val loss: 2.4432\n",
      "5144: lr: 0.0050, train loss: 2.4812, val loss: 2.5253\n",
      "5145: lr: 0.0050, train loss: 2.4261, val loss: 2.4759\n",
      "5146: lr: 0.0050, train loss: 2.4026, val loss: 2.4900\n",
      "5147: lr: 0.0050, train loss: 2.4733, val loss: 2.4926\n",
      "5148: lr: 0.0050, train loss: 2.4470, val loss: 2.5504\n",
      "5149: lr: 0.0050, train loss: 2.4558, val loss: 2.4552\n",
      "5150: lr: 0.0050, train loss: 2.4488, val loss: 2.4829\n",
      "5151: lr: 0.0050, train loss: 2.4392, val loss: 2.4580\n",
      "5152: lr: 0.0050, train loss: 2.4377, val loss: 2.5019\n",
      "5153: lr: 0.0050, train loss: 2.4865, val loss: 2.4091\n",
      "5154: lr: 0.0050, train loss: 2.5001, val loss: 2.4951\n",
      "5155: lr: 0.0050, train loss: 2.4131, val loss: 2.4195\n",
      "5156: lr: 0.0050, train loss: 2.4388, val loss: 2.4210\n",
      "5157: lr: 0.0050, train loss: 2.4790, val loss: 2.4774\n",
      "5158: lr: 0.0050, train loss: 2.4395, val loss: 2.4830\n",
      "5159: lr: 0.0050, train loss: 2.4857, val loss: 2.4740\n",
      "5160: lr: 0.0050, train loss: 2.4203, val loss: 2.5071\n",
      "5161: lr: 0.0050, train loss: 2.4960, val loss: 2.4509\n",
      "5162: lr: 0.0050, train loss: 2.4343, val loss: 2.4608\n",
      "5163: lr: 0.0050, train loss: 2.4334, val loss: 2.4739\n",
      "5164: lr: 0.0050, train loss: 2.4549, val loss: 2.4661\n",
      "5165: lr: 0.0050, train loss: 2.4581, val loss: 2.4929\n",
      "5166: lr: 0.0050, train loss: 2.4866, val loss: 2.5533\n",
      "5167: lr: 0.0050, train loss: 2.4429, val loss: 2.4933\n",
      "5168: lr: 0.0050, train loss: 2.4175, val loss: 2.4766\n",
      "5169: lr: 0.0050, train loss: 2.4752, val loss: 2.5182\n",
      "5170: lr: 0.0050, train loss: 2.4305, val loss: 2.4541\n",
      "5171: lr: 0.0050, train loss: 2.4717, val loss: 2.5059\n",
      "5172: lr: 0.0050, train loss: 2.4367, val loss: 2.4720\n",
      "5173: lr: 0.0050, train loss: 2.4513, val loss: 2.4547\n",
      "5174: lr: 0.0050, train loss: 2.4617, val loss: 2.4697\n",
      "5175: lr: 0.0050, train loss: 2.4422, val loss: 2.4928\n",
      "5176: lr: 0.0050, train loss: 2.4920, val loss: 2.4390\n",
      "5177: lr: 0.0050, train loss: 2.5150, val loss: 2.4990\n",
      "5178: lr: 0.0050, train loss: 2.4676, val loss: 2.4469\n",
      "5179: lr: 0.0050, train loss: 2.4532, val loss: 2.4787\n",
      "5180: lr: 0.0050, train loss: 2.4317, val loss: 2.5358\n",
      "5181: lr: 0.0050, train loss: 2.4267, val loss: 2.4814\n",
      "5182: lr: 0.0050, train loss: 2.4515, val loss: 2.4661\n",
      "5183: lr: 0.0050, train loss: 2.4560, val loss: 2.4813\n",
      "5184: lr: 0.0050, train loss: 2.4357, val loss: 2.5064\n",
      "5185: lr: 0.0050, train loss: 2.4981, val loss: 2.4421\n",
      "5186: lr: 0.0050, train loss: 2.4946, val loss: 2.4531\n",
      "5187: lr: 0.0050, train loss: 2.4537, val loss: 2.4932\n",
      "5188: lr: 0.0050, train loss: 2.4524, val loss: 2.5396\n",
      "5189: lr: 0.0050, train loss: 2.5010, val loss: 2.5201\n",
      "5190: lr: 0.0050, train loss: 2.4462, val loss: 2.4991\n",
      "5191: lr: 0.0050, train loss: 2.5017, val loss: 2.4762\n",
      "5192: lr: 0.0050, train loss: 2.4892, val loss: 2.4859\n",
      "5193: lr: 0.0050, train loss: 2.4760, val loss: 2.5007\n",
      "5194: lr: 0.0050, train loss: 2.4526, val loss: 2.4776\n",
      "5195: lr: 0.0050, train loss: 2.5214, val loss: 2.5232\n",
      "5196: lr: 0.0050, train loss: 2.4321, val loss: 2.4889\n",
      "5197: lr: 0.0050, train loss: 2.4709, val loss: 2.4740\n",
      "5198: lr: 0.0050, train loss: 2.5043, val loss: 2.5025\n",
      "5199: lr: 0.0050, train loss: 2.4525, val loss: 2.4554\n",
      "5200: lr: 0.0050, train loss: 2.4553, val loss: 2.5301\n",
      "5201: lr: 0.0050, train loss: 2.4510, val loss: 2.4647\n",
      "5202: lr: 0.0050, train loss: 2.4677, val loss: 2.4819\n",
      "5203: lr: 0.0050, train loss: 2.4825, val loss: 2.5134\n",
      "5204: lr: 0.0050, train loss: 2.4304, val loss: 2.4199\n",
      "5205: lr: 0.0050, train loss: 2.4599, val loss: 2.4279\n",
      "5206: lr: 0.0050, train loss: 2.4648, val loss: 2.5018\n",
      "5207: lr: 0.0050, train loss: 2.4994, val loss: 2.5221\n",
      "5208: lr: 0.0050, train loss: 2.4445, val loss: 2.5053\n",
      "5209: lr: 0.0050, train loss: 2.4917, val loss: 2.4559\n",
      "5210: lr: 0.0050, train loss: 2.4467, val loss: 2.4905\n",
      "5211: lr: 0.0050, train loss: 2.4786, val loss: 2.4550\n",
      "5212: lr: 0.0050, train loss: 2.4469, val loss: 2.5057\n",
      "5213: lr: 0.0050, train loss: 2.4626, val loss: 2.4722\n",
      "5214: lr: 0.0050, train loss: 2.4629, val loss: 2.4785\n",
      "5215: lr: 0.0050, train loss: 2.4662, val loss: 2.5322\n",
      "5216: lr: 0.0050, train loss: 2.4780, val loss: 2.5088\n",
      "5217: lr: 0.0050, train loss: 2.4645, val loss: 2.4532\n",
      "5218: lr: 0.0050, train loss: 2.4502, val loss: 2.4453\n",
      "5219: lr: 0.0050, train loss: 2.4503, val loss: 2.4584\n",
      "5220: lr: 0.0050, train loss: 2.4700, val loss: 2.4347\n",
      "5221: lr: 0.0050, train loss: 2.4661, val loss: 2.4771\n",
      "5222: lr: 0.0050, train loss: 2.4299, val loss: 2.4757\n",
      "5223: lr: 0.0050, train loss: 2.4600, val loss: 2.4401\n",
      "5224: lr: 0.0050, train loss: 2.4907, val loss: 2.4483\n",
      "5225: lr: 0.0050, train loss: 2.4246, val loss: 2.4666\n",
      "5226: lr: 0.0050, train loss: 2.4545, val loss: 2.5230\n",
      "5227: lr: 0.0050, train loss: 2.4479, val loss: 2.4405\n",
      "5228: lr: 0.0050, train loss: 2.4882, val loss: 2.4723\n",
      "5229: lr: 0.0050, train loss: 2.4562, val loss: 2.4615\n",
      "5230: lr: 0.0050, train loss: 2.4941, val loss: 2.4855\n",
      "5231: lr: 0.0050, train loss: 2.5075, val loss: 2.4899\n",
      "5232: lr: 0.0050, train loss: 2.4490, val loss: 2.5028\n",
      "5233: lr: 0.0050, train loss: 2.4337, val loss: 2.5124\n",
      "5234: lr: 0.0050, train loss: 2.4890, val loss: 2.5092\n",
      "5235: lr: 0.0050, train loss: 2.4739, val loss: 2.4348\n",
      "5236: lr: 0.0050, train loss: 2.4640, val loss: 2.4541\n",
      "5237: lr: 0.0050, train loss: 2.4520, val loss: 2.4833\n",
      "5238: lr: 0.0050, train loss: 2.3981, val loss: 2.5437\n",
      "5239: lr: 0.0050, train loss: 2.4578, val loss: 2.5434\n",
      "5240: lr: 0.0050, train loss: 2.4477, val loss: 2.4321\n",
      "5241: lr: 0.0050, train loss: 2.4481, val loss: 2.4370\n",
      "5242: lr: 0.0050, train loss: 2.4346, val loss: 2.4758\n",
      "5243: lr: 0.0050, train loss: 2.4230, val loss: 2.4986\n",
      "5244: lr: 0.0050, train loss: 2.4431, val loss: 2.4996\n",
      "5245: lr: 0.0050, train loss: 2.4584, val loss: 2.4460\n",
      "5246: lr: 0.0050, train loss: 2.4414, val loss: 2.5318\n",
      "5247: lr: 0.0050, train loss: 2.4546, val loss: 2.5315\n",
      "5248: lr: 0.0050, train loss: 2.4913, val loss: 2.5004\n",
      "5249: lr: 0.0050, train loss: 2.4195, val loss: 2.5044\n",
      "5250: lr: 0.0050, train loss: 2.4392, val loss: 2.5000\n",
      "5251: lr: 0.0050, train loss: 2.4812, val loss: 2.4741\n",
      "5252: lr: 0.0050, train loss: 2.4656, val loss: 2.5456\n",
      "5253: lr: 0.0050, train loss: 2.4810, val loss: 2.4941\n",
      "5254: lr: 0.0050, train loss: 2.4313, val loss: 2.4860\n",
      "5255: lr: 0.0050, train loss: 2.4711, val loss: 2.4793\n",
      "5256: lr: 0.0050, train loss: 2.4514, val loss: 2.4988\n",
      "5257: lr: 0.0050, train loss: 2.4422, val loss: 2.5206\n",
      "5258: lr: 0.0050, train loss: 2.4609, val loss: 2.5313\n",
      "5259: lr: 0.0050, train loss: 2.4576, val loss: 2.4784\n",
      "5260: lr: 0.0050, train loss: 2.4653, val loss: 2.5087\n",
      "5261: lr: 0.0050, train loss: 2.4172, val loss: 2.5106\n",
      "5262: lr: 0.0050, train loss: 2.4362, val loss: 2.4726\n",
      "5263: lr: 0.0050, train loss: 2.4591, val loss: 2.4505\n",
      "5264: lr: 0.0050, train loss: 2.4825, val loss: 2.5208\n",
      "5265: lr: 0.0050, train loss: 2.4349, val loss: 2.4590\n",
      "5266: lr: 0.0050, train loss: 2.4404, val loss: 2.5354\n",
      "5267: lr: 0.0050, train loss: 2.4898, val loss: 2.4794\n",
      "5268: lr: 0.0050, train loss: 2.4687, val loss: 2.4952\n",
      "5269: lr: 0.0050, train loss: 2.4242, val loss: 2.4784\n",
      "5270: lr: 0.0050, train loss: 2.4278, val loss: 2.4619\n",
      "5271: lr: 0.0050, train loss: 2.4543, val loss: 2.5012\n",
      "5272: lr: 0.0050, train loss: 2.5212, val loss: 2.4900\n",
      "5273: lr: 0.0050, train loss: 2.4407, val loss: 2.4897\n",
      "5274: lr: 0.0050, train loss: 2.4812, val loss: 2.4719\n",
      "5275: lr: 0.0050, train loss: 2.4634, val loss: 2.5119\n",
      "5276: lr: 0.0050, train loss: 2.4350, val loss: 2.5226\n",
      "5277: lr: 0.0050, train loss: 2.4674, val loss: 2.4569\n",
      "5278: lr: 0.0050, train loss: 2.4744, val loss: 2.4258\n",
      "5279: lr: 0.0050, train loss: 2.4661, val loss: 2.4206\n",
      "5280: lr: 0.0050, train loss: 2.4367, val loss: 2.5266\n",
      "5281: lr: 0.0050, train loss: 2.5100, val loss: 2.4168\n",
      "5282: lr: 0.0050, train loss: 2.4840, val loss: 2.5262\n",
      "5283: lr: 0.0050, train loss: 2.4494, val loss: 2.5529\n",
      "5284: lr: 0.0050, train loss: 2.4369, val loss: 2.5011\n",
      "5285: lr: 0.0050, train loss: 2.4658, val loss: 2.4788\n",
      "5286: lr: 0.0050, train loss: 2.4507, val loss: 2.5124\n",
      "5287: lr: 0.0050, train loss: 2.4501, val loss: 2.5263\n",
      "5288: lr: 0.0050, train loss: 2.4909, val loss: 2.5100\n",
      "5289: lr: 0.0050, train loss: 2.4315, val loss: 2.4933\n",
      "5290: lr: 0.0050, train loss: 2.4264, val loss: 2.4758\n",
      "5291: lr: 0.0050, train loss: 2.4240, val loss: 2.4552\n",
      "5292: lr: 0.0050, train loss: 2.4764, val loss: 2.4752\n",
      "5293: lr: 0.0050, train loss: 2.4462, val loss: 2.4255\n",
      "5294: lr: 0.0050, train loss: 2.4738, val loss: 2.4612\n",
      "5295: lr: 0.0050, train loss: 2.4705, val loss: 2.4546\n",
      "5296: lr: 0.0050, train loss: 2.4711, val loss: 2.4897\n",
      "5297: lr: 0.0050, train loss: 2.4732, val loss: 2.4977\n",
      "5298: lr: 0.0050, train loss: 2.4230, val loss: 2.4702\n",
      "5299: lr: 0.0050, train loss: 2.4238, val loss: 2.4579\n",
      "5300: lr: 0.0050, train loss: 2.4546, val loss: 2.5111\n",
      "5301: lr: 0.0050, train loss: 2.4702, val loss: 2.5055\n",
      "5302: lr: 0.0050, train loss: 2.4681, val loss: 2.4692\n",
      "5303: lr: 0.0050, train loss: 2.4480, val loss: 2.4919\n",
      "5304: lr: 0.0050, train loss: 2.4236, val loss: 2.4836\n",
      "5305: lr: 0.0050, train loss: 2.4586, val loss: 2.5252\n",
      "5306: lr: 0.0050, train loss: 2.4252, val loss: 2.5437\n",
      "5307: lr: 0.0050, train loss: 2.4418, val loss: 2.4622\n",
      "5308: lr: 0.0050, train loss: 2.4503, val loss: 2.4924\n",
      "5309: lr: 0.0050, train loss: 2.4171, val loss: 2.4736\n",
      "5310: lr: 0.0050, train loss: 2.4317, val loss: 2.4275\n",
      "5311: lr: 0.0050, train loss: 2.4500, val loss: 2.4386\n",
      "5312: lr: 0.0050, train loss: 2.4891, val loss: 2.5137\n",
      "5313: lr: 0.0050, train loss: 2.4755, val loss: 2.4642\n",
      "5314: lr: 0.0050, train loss: 2.4737, val loss: 2.4754\n",
      "5315: lr: 0.0050, train loss: 2.4554, val loss: 2.4794\n",
      "5316: lr: 0.0050, train loss: 2.4432, val loss: 2.4124\n",
      "5317: lr: 0.0050, train loss: 2.4570, val loss: 2.4574\n",
      "5318: lr: 0.0050, train loss: 2.4532, val loss: 2.4674\n",
      "5319: lr: 0.0050, train loss: 2.5172, val loss: 2.5486\n",
      "5320: lr: 0.0050, train loss: 2.4450, val loss: 2.4562\n",
      "5321: lr: 0.0050, train loss: 2.4867, val loss: 2.4717\n",
      "5322: lr: 0.0050, train loss: 2.4575, val loss: 2.4976\n",
      "5323: lr: 0.0050, train loss: 2.4564, val loss: 2.4548\n",
      "5324: lr: 0.0050, train loss: 2.4864, val loss: 2.4966\n",
      "5325: lr: 0.0050, train loss: 2.4358, val loss: 2.4742\n",
      "5326: lr: 0.0050, train loss: 2.4339, val loss: 2.5177\n",
      "5327: lr: 0.0050, train loss: 2.4390, val loss: 2.5105\n",
      "5328: lr: 0.0050, train loss: 2.4537, val loss: 2.4790\n",
      "5329: lr: 0.0050, train loss: 2.4409, val loss: 2.5266\n",
      "5330: lr: 0.0050, train loss: 2.4868, val loss: 2.4582\n",
      "5331: lr: 0.0050, train loss: 2.4472, val loss: 2.4774\n",
      "5332: lr: 0.0050, train loss: 2.4838, val loss: 2.4433\n",
      "5333: lr: 0.0050, train loss: 2.4506, val loss: 2.4753\n",
      "5334: lr: 0.0050, train loss: 2.4953, val loss: 2.4126\n",
      "5335: lr: 0.0050, train loss: 2.5001, val loss: 2.4903\n",
      "5336: lr: 0.0050, train loss: 2.4340, val loss: 2.4317\n",
      "5337: lr: 0.0050, train loss: 2.4691, val loss: 2.4620\n",
      "5338: lr: 0.0050, train loss: 2.4676, val loss: 2.4809\n",
      "5339: lr: 0.0050, train loss: 2.4153, val loss: 2.5420\n",
      "5340: lr: 0.0050, train loss: 2.5131, val loss: 2.4684\n",
      "5341: lr: 0.0050, train loss: 2.4199, val loss: 2.4474\n",
      "5342: lr: 0.0050, train loss: 2.4790, val loss: 2.4638\n",
      "5343: lr: 0.0050, train loss: 2.4734, val loss: 2.5014\n",
      "5344: lr: 0.0050, train loss: 2.4270, val loss: 2.4873\n",
      "5345: lr: 0.0050, train loss: 2.4668, val loss: 2.5029\n",
      "5346: lr: 0.0050, train loss: 2.4381, val loss: 2.5036\n",
      "5347: lr: 0.0050, train loss: 2.4950, val loss: 2.4926\n",
      "5348: lr: 0.0050, train loss: 2.4671, val loss: 2.4746\n",
      "5349: lr: 0.0050, train loss: 2.4255, val loss: 2.4759\n",
      "5350: lr: 0.0050, train loss: 2.4362, val loss: 2.4687\n",
      "5351: lr: 0.0050, train loss: 2.4697, val loss: 2.5185\n",
      "5352: lr: 0.0050, train loss: 2.4792, val loss: 2.4614\n",
      "5353: lr: 0.0050, train loss: 2.4400, val loss: 2.4899\n",
      "5354: lr: 0.0050, train loss: 2.4411, val loss: 2.4528\n",
      "5355: lr: 0.0050, train loss: 2.4243, val loss: 2.4469\n",
      "5356: lr: 0.0050, train loss: 2.4603, val loss: 2.4588\n",
      "5357: lr: 0.0050, train loss: 2.4636, val loss: 2.4807\n",
      "5358: lr: 0.0050, train loss: 2.4288, val loss: 2.4442\n",
      "5359: lr: 0.0050, train loss: 2.4666, val loss: 2.5076\n",
      "5360: lr: 0.0050, train loss: 2.4694, val loss: 2.5135\n",
      "5361: lr: 0.0050, train loss: 2.4166, val loss: 2.4799\n",
      "5362: lr: 0.0050, train loss: 2.4490, val loss: 2.4868\n",
      "5363: lr: 0.0050, train loss: 2.4480, val loss: 2.5347\n",
      "5364: lr: 0.0050, train loss: 2.4478, val loss: 2.4669\n",
      "5365: lr: 0.0050, train loss: 2.4176, val loss: 2.4650\n",
      "5366: lr: 0.0050, train loss: 2.4706, val loss: 2.5009\n",
      "5367: lr: 0.0050, train loss: 2.4828, val loss: 2.5367\n",
      "5368: lr: 0.0050, train loss: 2.4599, val loss: 2.4452\n",
      "5369: lr: 0.0050, train loss: 2.4747, val loss: 2.4646\n",
      "5370: lr: 0.0050, train loss: 2.4623, val loss: 2.5102\n",
      "5371: lr: 0.0050, train loss: 2.4621, val loss: 2.4812\n",
      "5372: lr: 0.0050, train loss: 2.4386, val loss: 2.5069\n",
      "5373: lr: 0.0050, train loss: 2.4873, val loss: 2.5091\n",
      "5374: lr: 0.0050, train loss: 2.4640, val loss: 2.4926\n",
      "5375: lr: 0.0050, train loss: 2.4421, val loss: 2.4341\n",
      "5376: lr: 0.0050, train loss: 2.4213, val loss: 2.5064\n",
      "5377: lr: 0.0050, train loss: 2.4803, val loss: 2.4916\n",
      "5378: lr: 0.0050, train loss: 2.4413, val loss: 2.5145\n",
      "5379: lr: 0.0050, train loss: 2.4773, val loss: 2.4989\n",
      "5380: lr: 0.0050, train loss: 2.4244, val loss: 2.4635\n",
      "5381: lr: 0.0050, train loss: 2.4807, val loss: 2.4738\n",
      "5382: lr: 0.0050, train loss: 2.4444, val loss: 2.4606\n",
      "5383: lr: 0.0050, train loss: 2.4687, val loss: 2.4733\n",
      "5384: lr: 0.0050, train loss: 2.4391, val loss: 2.4445\n",
      "5385: lr: 0.0050, train loss: 2.5029, val loss: 2.4739\n",
      "5386: lr: 0.0050, train loss: 2.4613, val loss: 2.4738\n",
      "5387: lr: 0.0050, train loss: 2.4524, val loss: 2.5424\n",
      "5388: lr: 0.0050, train loss: 2.4774, val loss: 2.4356\n",
      "5389: lr: 0.0050, train loss: 2.4399, val loss: 2.5098\n",
      "5390: lr: 0.0050, train loss: 2.4050, val loss: 2.4261\n",
      "5391: lr: 0.0050, train loss: 2.4625, val loss: 2.4852\n",
      "5392: lr: 0.0050, train loss: 2.4770, val loss: 2.4502\n",
      "5393: lr: 0.0050, train loss: 2.5047, val loss: 2.4926\n",
      "5394: lr: 0.0050, train loss: 2.4386, val loss: 2.4330\n",
      "5395: lr: 0.0050, train loss: 2.3974, val loss: 2.4633\n",
      "5396: lr: 0.0050, train loss: 2.4675, val loss: 2.4612\n",
      "5397: lr: 0.0050, train loss: 2.4571, val loss: 2.4990\n",
      "5398: lr: 0.0050, train loss: 2.4456, val loss: 2.5149\n",
      "5399: lr: 0.0050, train loss: 2.4690, val loss: 2.4772\n",
      "5400: lr: 0.0050, train loss: 2.4439, val loss: 2.4949\n",
      "5401: lr: 0.0050, train loss: 2.4543, val loss: 2.5212\n",
      "5402: lr: 0.0050, train loss: 2.4609, val loss: 2.5121\n",
      "5403: lr: 0.0050, train loss: 2.4435, val loss: 2.4934\n",
      "5404: lr: 0.0050, train loss: 2.4828, val loss: 2.5005\n",
      "5405: lr: 0.0050, train loss: 2.4804, val loss: 2.4724\n",
      "5406: lr: 0.0050, train loss: 2.4022, val loss: 2.5183\n",
      "5407: lr: 0.0050, train loss: 2.4417, val loss: 2.5151\n",
      "5408: lr: 0.0050, train loss: 2.4547, val loss: 2.5179\n",
      "5409: lr: 0.0050, train loss: 2.4337, val loss: 2.5023\n",
      "5410: lr: 0.0050, train loss: 2.4402, val loss: 2.4697\n",
      "5411: lr: 0.0050, train loss: 2.4498, val loss: 2.4510\n",
      "5412: lr: 0.0050, train loss: 2.4392, val loss: 2.4691\n",
      "5413: lr: 0.0050, train loss: 2.4158, val loss: 2.5249\n",
      "5414: lr: 0.0050, train loss: 2.4765, val loss: 2.4654\n",
      "5415: lr: 0.0050, train loss: 2.4628, val loss: 2.4994\n",
      "5416: lr: 0.0050, train loss: 2.4969, val loss: 2.4725\n",
      "5417: lr: 0.0050, train loss: 2.4251, val loss: 2.4720\n",
      "5418: lr: 0.0050, train loss: 2.4689, val loss: 2.5209\n",
      "5419: lr: 0.0050, train loss: 2.3971, val loss: 2.4616\n",
      "5420: lr: 0.0050, train loss: 2.4595, val loss: 2.4577\n",
      "5421: lr: 0.0050, train loss: 2.4690, val loss: 2.4855\n",
      "5422: lr: 0.0050, train loss: 2.4178, val loss: 2.5332\n",
      "5423: lr: 0.0050, train loss: 2.4309, val loss: 2.5014\n",
      "5424: lr: 0.0050, train loss: 2.4618, val loss: 2.4998\n",
      "5425: lr: 0.0050, train loss: 2.4701, val loss: 2.4710\n",
      "5426: lr: 0.0050, train loss: 2.4479, val loss: 2.4822\n",
      "5427: lr: 0.0050, train loss: 2.4808, val loss: 2.4742\n",
      "5428: lr: 0.0050, train loss: 2.4289, val loss: 2.4660\n",
      "5429: lr: 0.0050, train loss: 2.3940, val loss: 2.4850\n",
      "5430: lr: 0.0050, train loss: 2.4203, val loss: 2.5153\n",
      "5431: lr: 0.0050, train loss: 2.4352, val loss: 2.5135\n",
      "5432: lr: 0.0050, train loss: 2.4268, val loss: 2.5109\n",
      "5433: lr: 0.0050, train loss: 2.4692, val loss: 2.5141\n",
      "5434: lr: 0.0050, train loss: 2.4388, val loss: 2.5371\n",
      "5435: lr: 0.0050, train loss: 2.4057, val loss: 2.5093\n",
      "5436: lr: 0.0050, train loss: 2.4586, val loss: 2.4796\n",
      "5437: lr: 0.0050, train loss: 2.4473, val loss: 2.5346\n",
      "5438: lr: 0.0050, train loss: 2.4606, val loss: 2.4864\n",
      "5439: lr: 0.0050, train loss: 2.3990, val loss: 2.4736\n",
      "5440: lr: 0.0050, train loss: 2.4448, val loss: 2.4547\n",
      "5441: lr: 0.0050, train loss: 2.4856, val loss: 2.4550\n",
      "5442: lr: 0.0050, train loss: 2.4632, val loss: 2.5073\n",
      "5443: lr: 0.0050, train loss: 2.4961, val loss: 2.4767\n",
      "5444: lr: 0.0050, train loss: 2.4437, val loss: 2.4780\n",
      "5445: lr: 0.0050, train loss: 2.4170, val loss: 2.4563\n",
      "5446: lr: 0.0050, train loss: 2.4580, val loss: 2.4609\n",
      "5447: lr: 0.0050, train loss: 2.4764, val loss: 2.4484\n",
      "5448: lr: 0.0050, train loss: 2.4528, val loss: 2.5141\n",
      "5449: lr: 0.0050, train loss: 2.4586, val loss: 2.4807\n",
      "5450: lr: 0.0050, train loss: 2.4700, val loss: 2.4483\n",
      "5451: lr: 0.0050, train loss: 2.4632, val loss: 2.4824\n",
      "5452: lr: 0.0050, train loss: 2.4322, val loss: 2.4445\n",
      "5453: lr: 0.0050, train loss: 2.4713, val loss: 2.4771\n",
      "5454: lr: 0.0050, train loss: 2.5302, val loss: 2.4725\n",
      "5455: lr: 0.0050, train loss: 2.4038, val loss: 2.5135\n",
      "5456: lr: 0.0050, train loss: 2.4368, val loss: 2.4946\n",
      "5457: lr: 0.0050, train loss: 2.4329, val loss: 2.5024\n",
      "5458: lr: 0.0050, train loss: 2.5062, val loss: 2.4810\n",
      "5459: lr: 0.0050, train loss: 2.4474, val loss: 2.4802\n",
      "5460: lr: 0.0050, train loss: 2.4655, val loss: 2.4453\n",
      "5461: lr: 0.0050, train loss: 2.4762, val loss: 2.4936\n",
      "5462: lr: 0.0050, train loss: 2.4441, val loss: 2.4874\n",
      "5463: lr: 0.0050, train loss: 2.4537, val loss: 2.5129\n",
      "5464: lr: 0.0050, train loss: 2.4508, val loss: 2.4891\n",
      "5465: lr: 0.0050, train loss: 2.4181, val loss: 2.5019\n",
      "5466: lr: 0.0050, train loss: 2.4748, val loss: 2.4445\n",
      "5467: lr: 0.0050, train loss: 2.4607, val loss: 2.4961\n",
      "5468: lr: 0.0050, train loss: 2.4600, val loss: 2.4835\n",
      "5469: lr: 0.0050, train loss: 2.4395, val loss: 2.4594\n",
      "5470: lr: 0.0050, train loss: 2.4518, val loss: 2.4622\n",
      "5471: lr: 0.0050, train loss: 2.4250, val loss: 2.4436\n",
      "5472: lr: 0.0050, train loss: 2.4849, val loss: 2.4715\n",
      "5473: lr: 0.0050, train loss: 2.4297, val loss: 2.5428\n",
      "5474: lr: 0.0050, train loss: 2.4564, val loss: 2.4646\n",
      "5475: lr: 0.0050, train loss: 2.4318, val loss: 2.4633\n",
      "5476: lr: 0.0050, train loss: 2.4720, val loss: 2.4973\n",
      "5477: lr: 0.0050, train loss: 2.4735, val loss: 2.5108\n",
      "5478: lr: 0.0050, train loss: 2.4243, val loss: 2.5045\n",
      "5479: lr: 0.0050, train loss: 2.4767, val loss: 2.4918\n",
      "5480: lr: 0.0050, train loss: 2.4470, val loss: 2.4986\n",
      "5481: lr: 0.0050, train loss: 2.4266, val loss: 2.4687\n",
      "5482: lr: 0.0050, train loss: 2.4758, val loss: 2.4695\n",
      "5483: lr: 0.0050, train loss: 2.4400, val loss: 2.4825\n",
      "5484: lr: 0.0050, train loss: 2.4858, val loss: 2.5023\n",
      "5485: lr: 0.0050, train loss: 2.4438, val loss: 2.4621\n",
      "5486: lr: 0.0050, train loss: 2.4230, val loss: 2.4895\n",
      "5487: lr: 0.0050, train loss: 2.4354, val loss: 2.5149\n",
      "5488: lr: 0.0050, train loss: 2.5107, val loss: 2.5048\n",
      "5489: lr: 0.0050, train loss: 2.4806, val loss: 2.5346\n",
      "5490: lr: 0.0050, train loss: 2.3976, val loss: 2.4807\n",
      "5491: lr: 0.0050, train loss: 2.4319, val loss: 2.4593\n",
      "5492: lr: 0.0050, train loss: 2.4446, val loss: 2.4689\n",
      "5493: lr: 0.0050, train loss: 2.4756, val loss: 2.5112\n",
      "5494: lr: 0.0050, train loss: 2.4283, val loss: 2.5050\n",
      "5495: lr: 0.0050, train loss: 2.4263, val loss: 2.4834\n",
      "5496: lr: 0.0050, train loss: 2.4665, val loss: 2.4820\n",
      "5497: lr: 0.0050, train loss: 2.4869, val loss: 2.4398\n",
      "5498: lr: 0.0050, train loss: 2.4723, val loss: 2.4320\n",
      "5499: lr: 0.0050, train loss: 2.4347, val loss: 2.4772\n",
      "5500: lr: 0.0050, train loss: 2.4863, val loss: 2.5424\n",
      "5501: lr: 0.0050, train loss: 2.4569, val loss: 2.4635\n",
      "5502: lr: 0.0050, train loss: 2.4240, val loss: 2.4639\n",
      "5503: lr: 0.0050, train loss: 2.4983, val loss: 2.5033\n",
      "5504: lr: 0.0050, train loss: 2.4309, val loss: 2.4878\n",
      "5505: lr: 0.0050, train loss: 2.4728, val loss: 2.4819\n",
      "5506: lr: 0.0050, train loss: 2.4875, val loss: 2.4613\n",
      "5507: lr: 0.0050, train loss: 2.4918, val loss: 2.4736\n",
      "5508: lr: 0.0050, train loss: 2.5078, val loss: 2.4770\n",
      "5509: lr: 0.0050, train loss: 2.4471, val loss: 2.4922\n",
      "5510: lr: 0.0050, train loss: 2.4567, val loss: 2.4983\n",
      "5511: lr: 0.0050, train loss: 2.4438, val loss: 2.5341\n",
      "5512: lr: 0.0050, train loss: 2.4732, val loss: 2.4647\n",
      "5513: lr: 0.0050, train loss: 2.4174, val loss: 2.5026\n",
      "5514: lr: 0.0050, train loss: 2.4254, val loss: 2.4574\n",
      "5515: lr: 0.0050, train loss: 2.4455, val loss: 2.4997\n",
      "5516: lr: 0.0050, train loss: 2.4757, val loss: 2.4692\n",
      "5517: lr: 0.0050, train loss: 2.4238, val loss: 2.4831\n",
      "5518: lr: 0.0050, train loss: 2.4590, val loss: 2.4527\n",
      "5519: lr: 0.0050, train loss: 2.5106, val loss: 2.4813\n",
      "5520: lr: 0.0050, train loss: 2.4316, val loss: 2.4662\n",
      "5521: lr: 0.0050, train loss: 2.4507, val loss: 2.4354\n",
      "5522: lr: 0.0050, train loss: 2.4344, val loss: 2.4596\n",
      "5523: lr: 0.0050, train loss: 2.4574, val loss: 2.4586\n",
      "5524: lr: 0.0050, train loss: 2.4246, val loss: 2.4749\n",
      "5525: lr: 0.0050, train loss: 2.4382, val loss: 2.5158\n",
      "5526: lr: 0.0050, train loss: 2.4286, val loss: 2.4593\n",
      "5527: lr: 0.0050, train loss: 2.4583, val loss: 2.4835\n",
      "5528: lr: 0.0050, train loss: 2.4941, val loss: 2.4574\n",
      "5529: lr: 0.0050, train loss: 2.4548, val loss: 2.5094\n",
      "5530: lr: 0.0050, train loss: 2.4770, val loss: 2.4876\n",
      "5531: lr: 0.0050, train loss: 2.4882, val loss: 2.4792\n",
      "5532: lr: 0.0050, train loss: 2.4303, val loss: 2.5383\n",
      "5533: lr: 0.0050, train loss: 2.4899, val loss: 2.4847\n",
      "5534: lr: 0.0050, train loss: 2.4004, val loss: 2.4832\n",
      "5535: lr: 0.0050, train loss: 2.5072, val loss: 2.4789\n",
      "5536: lr: 0.0050, train loss: 2.4141, val loss: 2.5021\n",
      "5537: lr: 0.0050, train loss: 2.4169, val loss: 2.4742\n",
      "5538: lr: 0.0050, train loss: 2.4669, val loss: 2.4646\n",
      "5539: lr: 0.0050, train loss: 2.4304, val loss: 2.4831\n",
      "5540: lr: 0.0050, train loss: 2.4390, val loss: 2.5100\n",
      "5541: lr: 0.0050, train loss: 2.4703, val loss: 2.5394\n",
      "5542: lr: 0.0050, train loss: 2.4635, val loss: 2.5496\n",
      "5543: lr: 0.0050, train loss: 2.4646, val loss: 2.4896\n",
      "5544: lr: 0.0050, train loss: 2.4681, val loss: 2.4917\n",
      "5545: lr: 0.0050, train loss: 2.4557, val loss: 2.4704\n",
      "5546: lr: 0.0050, train loss: 2.4310, val loss: 2.5116\n",
      "5547: lr: 0.0050, train loss: 2.4373, val loss: 2.4346\n",
      "5548: lr: 0.0050, train loss: 2.4330, val loss: 2.4622\n",
      "5549: lr: 0.0050, train loss: 2.4958, val loss: 2.5212\n",
      "5550: lr: 0.0050, train loss: 2.4554, val loss: 2.4811\n",
      "5551: lr: 0.0050, train loss: 2.4367, val loss: 2.4529\n",
      "5552: lr: 0.0050, train loss: 2.4274, val loss: 2.4708\n",
      "5553: lr: 0.0050, train loss: 2.4553, val loss: 2.4714\n",
      "5554: lr: 0.0050, train loss: 2.4761, val loss: 2.4752\n",
      "5555: lr: 0.0050, train loss: 2.4364, val loss: 2.5058\n",
      "5556: lr: 0.0050, train loss: 2.4821, val loss: 2.4536\n",
      "5557: lr: 0.0050, train loss: 2.4276, val loss: 2.5186\n",
      "5558: lr: 0.0050, train loss: 2.4594, val loss: 2.4581\n",
      "5559: lr: 0.0050, train loss: 2.4432, val loss: 2.5407\n",
      "5560: lr: 0.0050, train loss: 2.4395, val loss: 2.4963\n",
      "5561: lr: 0.0050, train loss: 2.4987, val loss: 2.4552\n",
      "5562: lr: 0.0050, train loss: 2.5081, val loss: 2.5780\n",
      "5563: lr: 0.0050, train loss: 2.4517, val loss: 2.4686\n",
      "5564: lr: 0.0050, train loss: 2.4219, val loss: 2.5227\n",
      "5565: lr: 0.0050, train loss: 2.5046, val loss: 2.4425\n",
      "5566: lr: 0.0050, train loss: 2.5357, val loss: 2.4945\n",
      "5567: lr: 0.0050, train loss: 2.4779, val loss: 2.5364\n",
      "5568: lr: 0.0050, train loss: 2.4708, val loss: 2.4694\n",
      "5569: lr: 0.0050, train loss: 2.3780, val loss: 2.4994\n",
      "5570: lr: 0.0050, train loss: 2.4900, val loss: 2.4691\n",
      "5571: lr: 0.0050, train loss: 2.4423, val loss: 2.4997\n",
      "5572: lr: 0.0050, train loss: 2.4823, val loss: 2.5190\n",
      "5573: lr: 0.0050, train loss: 2.4332, val loss: 2.4774\n",
      "5574: lr: 0.0050, train loss: 2.4514, val loss: 2.4875\n",
      "5575: lr: 0.0050, train loss: 2.4237, val loss: 2.4836\n",
      "5576: lr: 0.0050, train loss: 2.4283, val loss: 2.4529\n",
      "5577: lr: 0.0050, train loss: 2.4787, val loss: 2.4762\n",
      "5578: lr: 0.0050, train loss: 2.4903, val loss: 2.4719\n",
      "5579: lr: 0.0050, train loss: 2.4664, val loss: 2.5180\n",
      "5580: lr: 0.0050, train loss: 2.3898, val loss: 2.4938\n",
      "5581: lr: 0.0050, train loss: 2.4666, val loss: 2.4668\n",
      "5582: lr: 0.0050, train loss: 2.4589, val loss: 2.4388\n",
      "5583: lr: 0.0050, train loss: 2.4622, val loss: 2.4370\n",
      "5584: lr: 0.0050, train loss: 2.4810, val loss: 2.5163\n",
      "5585: lr: 0.0050, train loss: 2.4125, val loss: 2.4656\n",
      "5586: lr: 0.0050, train loss: 2.4307, val loss: 2.4885\n",
      "5587: lr: 0.0050, train loss: 2.4371, val loss: 2.4943\n",
      "5588: lr: 0.0050, train loss: 2.5222, val loss: 2.4507\n",
      "5589: lr: 0.0050, train loss: 2.4048, val loss: 2.5010\n",
      "5590: lr: 0.0050, train loss: 2.4705, val loss: 2.5492\n",
      "5591: lr: 0.0050, train loss: 2.4468, val loss: 2.5093\n",
      "5592: lr: 0.0050, train loss: 2.4480, val loss: 2.5523\n",
      "5593: lr: 0.0050, train loss: 2.4720, val loss: 2.4688\n",
      "5594: lr: 0.0050, train loss: 2.4467, val loss: 2.4551\n",
      "5595: lr: 0.0050, train loss: 2.4408, val loss: 2.4828\n",
      "5596: lr: 0.0050, train loss: 2.4200, val loss: 2.4372\n",
      "5597: lr: 0.0050, train loss: 2.4693, val loss: 2.4327\n",
      "5598: lr: 0.0050, train loss: 2.4440, val loss: 2.5149\n",
      "5599: lr: 0.0050, train loss: 2.4272, val loss: 2.4891\n",
      "5600: lr: 0.0050, train loss: 2.4150, val loss: 2.4988\n",
      "5601: lr: 0.0050, train loss: 2.5029, val loss: 2.5499\n",
      "5602: lr: 0.0050, train loss: 2.4653, val loss: 2.4387\n",
      "5603: lr: 0.0050, train loss: 2.4583, val loss: 2.4983\n",
      "5604: lr: 0.0050, train loss: 2.4295, val loss: 2.4688\n",
      "5605: lr: 0.0050, train loss: 2.4488, val loss: 2.4880\n",
      "5606: lr: 0.0050, train loss: 2.4235, val loss: 2.4765\n",
      "5607: lr: 0.0050, train loss: 2.5235, val loss: 2.4615\n",
      "5608: lr: 0.0050, train loss: 2.4734, val loss: 2.4480\n",
      "5609: lr: 0.0050, train loss: 2.4413, val loss: 2.5184\n",
      "5610: lr: 0.0050, train loss: 2.4549, val loss: 2.5143\n",
      "5611: lr: 0.0050, train loss: 2.4730, val loss: 2.4679\n",
      "5612: lr: 0.0050, train loss: 2.5014, val loss: 2.4606\n",
      "5613: lr: 0.0050, train loss: 2.4497, val loss: 2.4883\n",
      "5614: lr: 0.0050, train loss: 2.4354, val loss: 2.5267\n",
      "5615: lr: 0.0050, train loss: 2.4440, val loss: 2.4777\n",
      "5616: lr: 0.0050, train loss: 2.4547, val loss: 2.4947\n",
      "5617: lr: 0.0050, train loss: 2.3799, val loss: 2.4878\n",
      "5618: lr: 0.0050, train loss: 2.4235, val loss: 2.5519\n",
      "5619: lr: 0.0050, train loss: 2.4257, val loss: 2.4888\n",
      "5620: lr: 0.0050, train loss: 2.4725, val loss: 2.4719\n",
      "5621: lr: 0.0050, train loss: 2.4200, val loss: 2.5013\n",
      "5622: lr: 0.0050, train loss: 2.4700, val loss: 2.4995\n",
      "5623: lr: 0.0050, train loss: 2.4865, val loss: 2.4997\n",
      "5624: lr: 0.0050, train loss: 2.4791, val loss: 2.4773\n",
      "5625: lr: 0.0050, train loss: 2.4396, val loss: 2.4968\n",
      "5626: lr: 0.0050, train loss: 2.4537, val loss: 2.4943\n",
      "5627: lr: 0.0050, train loss: 2.4758, val loss: 2.4488\n",
      "5628: lr: 0.0050, train loss: 2.4665, val loss: 2.4595\n",
      "5629: lr: 0.0050, train loss: 2.4646, val loss: 2.4676\n",
      "5630: lr: 0.0050, train loss: 2.4532, val loss: 2.5036\n",
      "5631: lr: 0.0050, train loss: 2.4741, val loss: 2.4139\n",
      "5632: lr: 0.0050, train loss: 2.4247, val loss: 2.5365\n",
      "5633: lr: 0.0050, train loss: 2.4223, val loss: 2.4786\n",
      "5634: lr: 0.0050, train loss: 2.4553, val loss: 2.4635\n",
      "5635: lr: 0.0050, train loss: 2.4516, val loss: 2.4584\n",
      "5636: lr: 0.0050, train loss: 2.4735, val loss: 2.5440\n",
      "5637: lr: 0.0050, train loss: 2.5030, val loss: 2.5023\n",
      "5638: lr: 0.0050, train loss: 2.4442, val loss: 2.5109\n",
      "5639: lr: 0.0050, train loss: 2.4802, val loss: 2.5071\n",
      "5640: lr: 0.0050, train loss: 2.4631, val loss: 2.5106\n",
      "5641: lr: 0.0050, train loss: 2.4148, val loss: 2.4833\n",
      "5642: lr: 0.0050, train loss: 2.4248, val loss: 2.5186\n",
      "5643: lr: 0.0050, train loss: 2.4331, val loss: 2.5191\n",
      "5644: lr: 0.0050, train loss: 2.4611, val loss: 2.4745\n",
      "5645: lr: 0.0050, train loss: 2.4506, val loss: 2.5091\n",
      "5646: lr: 0.0050, train loss: 2.4895, val loss: 2.4505\n",
      "5647: lr: 0.0050, train loss: 2.4761, val loss: 2.5216\n",
      "5648: lr: 0.0050, train loss: 2.4883, val loss: 2.4518\n",
      "5649: lr: 0.0050, train loss: 2.4812, val loss: 2.5213\n",
      "5650: lr: 0.0050, train loss: 2.4436, val loss: 2.4482\n",
      "5651: lr: 0.0050, train loss: 2.4560, val loss: 2.4739\n",
      "5652: lr: 0.0050, train loss: 2.4571, val loss: 2.5018\n",
      "5653: lr: 0.0050, train loss: 2.4187, val loss: 2.4622\n",
      "5654: lr: 0.0050, train loss: 2.4597, val loss: 2.4706\n",
      "5655: lr: 0.0050, train loss: 2.4569, val loss: 2.4317\n",
      "5656: lr: 0.0050, train loss: 2.4890, val loss: 2.5013\n",
      "5657: lr: 0.0050, train loss: 2.4824, val loss: 2.4769\n",
      "5658: lr: 0.0050, train loss: 2.4578, val loss: 2.4668\n",
      "5659: lr: 0.0050, train loss: 2.4297, val loss: 2.4526\n",
      "5660: lr: 0.0050, train loss: 2.4757, val loss: 2.4941\n",
      "5661: lr: 0.0050, train loss: 2.4386, val loss: 2.5101\n",
      "5662: lr: 0.0050, train loss: 2.4242, val loss: 2.4914\n",
      "5663: lr: 0.0050, train loss: 2.4906, val loss: 2.4761\n",
      "5664: lr: 0.0050, train loss: 2.4684, val loss: 2.4368\n",
      "5665: lr: 0.0050, train loss: 2.4191, val loss: 2.4744\n",
      "5666: lr: 0.0050, train loss: 2.4426, val loss: 2.5185\n",
      "5667: lr: 0.0050, train loss: 2.4677, val loss: 2.5006\n",
      "5668: lr: 0.0050, train loss: 2.4480, val loss: 2.4612\n",
      "5669: lr: 0.0050, train loss: 2.4678, val loss: 2.4665\n",
      "5670: lr: 0.0050, train loss: 2.4752, val loss: 2.4575\n",
      "5671: lr: 0.0050, train loss: 2.4686, val loss: 2.4852\n",
      "5672: lr: 0.0050, train loss: 2.3844, val loss: 2.4498\n",
      "5673: lr: 0.0050, train loss: 2.4498, val loss: 2.4604\n",
      "5674: lr: 0.0050, train loss: 2.4430, val loss: 2.5117\n",
      "5675: lr: 0.0050, train loss: 2.5316, val loss: 2.5254\n",
      "5676: lr: 0.0050, train loss: 2.4332, val loss: 2.4695\n",
      "5677: lr: 0.0050, train loss: 2.3846, val loss: 2.5297\n",
      "5678: lr: 0.0050, train loss: 2.4615, val loss: 2.5139\n",
      "5679: lr: 0.0050, train loss: 2.4910, val loss: 2.4576\n",
      "5680: lr: 0.0050, train loss: 2.4506, val loss: 2.4280\n",
      "5681: lr: 0.0050, train loss: 2.4664, val loss: 2.4594\n",
      "5682: lr: 0.0050, train loss: 2.4367, val loss: 2.4631\n",
      "5683: lr: 0.0050, train loss: 2.4793, val loss: 2.4633\n",
      "5684: lr: 0.0050, train loss: 2.4761, val loss: 2.5101\n",
      "5685: lr: 0.0050, train loss: 2.4571, val loss: 2.4292\n",
      "5686: lr: 0.0050, train loss: 2.4740, val loss: 2.4317\n",
      "5687: lr: 0.0050, train loss: 2.4759, val loss: 2.4932\n",
      "5688: lr: 0.0050, train loss: 2.4619, val loss: 2.5185\n",
      "5689: lr: 0.0050, train loss: 2.3963, val loss: 2.4542\n",
      "5690: lr: 0.0050, train loss: 2.4928, val loss: 2.4893\n",
      "5691: lr: 0.0050, train loss: 2.5040, val loss: 2.4807\n",
      "5692: lr: 0.0050, train loss: 2.4705, val loss: 2.5606\n",
      "5693: lr: 0.0050, train loss: 2.4602, val loss: 2.5382\n",
      "5694: lr: 0.0050, train loss: 2.4310, val loss: 2.4776\n",
      "5695: lr: 0.0050, train loss: 2.4731, val loss: 2.5132\n",
      "5696: lr: 0.0050, train loss: 2.4431, val loss: 2.5018\n",
      "5697: lr: 0.0050, train loss: 2.4052, val loss: 2.4603\n",
      "5698: lr: 0.0050, train loss: 2.4408, val loss: 2.4673\n",
      "5699: lr: 0.0050, train loss: 2.4605, val loss: 2.5058\n",
      "5700: lr: 0.0050, train loss: 2.4468, val loss: 2.4774\n",
      "5701: lr: 0.0050, train loss: 2.4684, val loss: 2.5228\n",
      "5702: lr: 0.0050, train loss: 2.4967, val loss: 2.5169\n",
      "5703: lr: 0.0050, train loss: 2.4302, val loss: 2.5010\n",
      "5704: lr: 0.0050, train loss: 2.4811, val loss: 2.4522\n",
      "5705: lr: 0.0050, train loss: 2.4624, val loss: 2.4639\n",
      "5706: lr: 0.0050, train loss: 2.4877, val loss: 2.4746\n",
      "5707: lr: 0.0050, train loss: 2.4073, val loss: 2.4987\n",
      "5708: lr: 0.0050, train loss: 2.4600, val loss: 2.4997\n",
      "5709: lr: 0.0050, train loss: 2.4525, val loss: 2.5421\n",
      "5710: lr: 0.0050, train loss: 2.4496, val loss: 2.5317\n",
      "5711: lr: 0.0050, train loss: 2.4788, val loss: 2.4526\n",
      "5712: lr: 0.0050, train loss: 2.4458, val loss: 2.4300\n",
      "5713: lr: 0.0050, train loss: 2.5002, val loss: 2.5204\n",
      "5714: lr: 0.0050, train loss: 2.4510, val loss: 2.5059\n",
      "5715: lr: 0.0050, train loss: 2.5022, val loss: 2.4891\n",
      "5716: lr: 0.0050, train loss: 2.4607, val loss: 2.4975\n",
      "5717: lr: 0.0050, train loss: 2.4373, val loss: 2.4611\n",
      "5718: lr: 0.0050, train loss: 2.4096, val loss: 2.4955\n",
      "5719: lr: 0.0050, train loss: 2.4221, val loss: 2.4770\n",
      "5720: lr: 0.0050, train loss: 2.4713, val loss: 2.5079\n",
      "5721: lr: 0.0050, train loss: 2.4556, val loss: 2.4423\n",
      "5722: lr: 0.0050, train loss: 2.4545, val loss: 2.5108\n",
      "5723: lr: 0.0050, train loss: 2.4233, val loss: 2.5320\n",
      "5724: lr: 0.0050, train loss: 2.4369, val loss: 2.4745\n",
      "5725: lr: 0.0050, train loss: 2.4432, val loss: 2.4989\n",
      "5726: lr: 0.0050, train loss: 2.4901, val loss: 2.4669\n",
      "5727: lr: 0.0050, train loss: 2.4823, val loss: 2.4809\n",
      "5728: lr: 0.0050, train loss: 2.4212, val loss: 2.5549\n",
      "5729: lr: 0.0050, train loss: 2.4519, val loss: 2.4811\n",
      "5730: lr: 0.0050, train loss: 2.4549, val loss: 2.4758\n",
      "5731: lr: 0.0050, train loss: 2.4801, val loss: 2.4957\n",
      "5732: lr: 0.0050, train loss: 2.4436, val loss: 2.4950\n",
      "5733: lr: 0.0050, train loss: 2.4608, val loss: 2.5198\n",
      "5734: lr: 0.0050, train loss: 2.4772, val loss: 2.4661\n",
      "5735: lr: 0.0050, train loss: 2.4470, val loss: 2.4412\n",
      "5736: lr: 0.0050, train loss: 2.4871, val loss: 2.4771\n",
      "5737: lr: 0.0050, train loss: 2.4927, val loss: 2.4828\n",
      "5738: lr: 0.0050, train loss: 2.4764, val loss: 2.4743\n",
      "5739: lr: 0.0050, train loss: 2.5035, val loss: 2.4706\n",
      "5740: lr: 0.0050, train loss: 2.4652, val loss: 2.4996\n",
      "5741: lr: 0.0050, train loss: 2.4831, val loss: 2.4542\n",
      "5742: lr: 0.0050, train loss: 2.4753, val loss: 2.4832\n",
      "5743: lr: 0.0050, train loss: 2.4758, val loss: 2.5110\n",
      "5744: lr: 0.0050, train loss: 2.5040, val loss: 2.4862\n",
      "5745: lr: 0.0050, train loss: 2.4113, val loss: 2.4691\n",
      "5746: lr: 0.0050, train loss: 2.4421, val loss: 2.4636\n",
      "5747: lr: 0.0050, train loss: 2.4302, val loss: 2.4778\n",
      "5748: lr: 0.0050, train loss: 2.4476, val loss: 2.4910\n",
      "5749: lr: 0.0050, train loss: 2.4821, val loss: 2.4664\n",
      "5750: lr: 0.0050, train loss: 2.4580, val loss: 2.4689\n",
      "5751: lr: 0.0050, train loss: 2.4262, val loss: 2.5001\n",
      "5752: lr: 0.0050, train loss: 2.5248, val loss: 2.4503\n",
      "5753: lr: 0.0050, train loss: 2.4220, val loss: 2.4661\n",
      "5754: lr: 0.0050, train loss: 2.4527, val loss: 2.4747\n",
      "5755: lr: 0.0050, train loss: 2.4704, val loss: 2.4809\n",
      "5756: lr: 0.0050, train loss: 2.4394, val loss: 2.5244\n",
      "5757: lr: 0.0050, train loss: 2.4566, val loss: 2.4930\n",
      "5758: lr: 0.0050, train loss: 2.4497, val loss: 2.5174\n",
      "5759: lr: 0.0050, train loss: 2.4568, val loss: 2.5122\n",
      "5760: lr: 0.0050, train loss: 2.4953, val loss: 2.4690\n",
      "5761: lr: 0.0050, train loss: 2.4153, val loss: 2.5512\n",
      "5762: lr: 0.0050, train loss: 2.4780, val loss: 2.4416\n",
      "5763: lr: 0.0050, train loss: 2.4683, val loss: 2.4720\n",
      "5764: lr: 0.0050, train loss: 2.4737, val loss: 2.5327\n",
      "5765: lr: 0.0050, train loss: 2.4803, val loss: 2.4234\n",
      "5766: lr: 0.0050, train loss: 2.4364, val loss: 2.4220\n",
      "5767: lr: 0.0050, train loss: 2.4367, val loss: 2.4512\n",
      "5768: lr: 0.0050, train loss: 2.4337, val loss: 2.4540\n",
      "5769: lr: 0.0050, train loss: 2.4463, val loss: 2.4430\n",
      "5770: lr: 0.0050, train loss: 2.4853, val loss: 2.4748\n",
      "5771: lr: 0.0050, train loss: 2.4404, val loss: 2.4969\n",
      "5772: lr: 0.0050, train loss: 2.4466, val loss: 2.4819\n",
      "5773: lr: 0.0050, train loss: 2.4832, val loss: 2.4329\n",
      "5774: lr: 0.0050, train loss: 2.4633, val loss: 2.4325\n",
      "5775: lr: 0.0050, train loss: 2.4591, val loss: 2.4711\n",
      "5776: lr: 0.0050, train loss: 2.4641, val loss: 2.5073\n",
      "5777: lr: 0.0050, train loss: 2.4896, val loss: 2.4988\n",
      "5778: lr: 0.0050, train loss: 2.4803, val loss: 2.4944\n",
      "5779: lr: 0.0050, train loss: 2.4622, val loss: 2.4935\n",
      "5780: lr: 0.0050, train loss: 2.4568, val loss: 2.4435\n",
      "5781: lr: 0.0050, train loss: 2.4474, val loss: 2.4537\n",
      "5782: lr: 0.0050, train loss: 2.4950, val loss: 2.5319\n",
      "5783: lr: 0.0050, train loss: 2.4519, val loss: 2.4915\n",
      "5784: lr: 0.0050, train loss: 2.4723, val loss: 2.4764\n",
      "5785: lr: 0.0050, train loss: 2.4420, val loss: 2.4797\n",
      "5786: lr: 0.0050, train loss: 2.4475, val loss: 2.5390\n",
      "5787: lr: 0.0050, train loss: 2.5050, val loss: 2.5110\n",
      "5788: lr: 0.0050, train loss: 2.4905, val loss: 2.4735\n",
      "5789: lr: 0.0050, train loss: 2.4488, val loss: 2.4890\n",
      "5790: lr: 0.0050, train loss: 2.4730, val loss: 2.4857\n",
      "5791: lr: 0.0050, train loss: 2.4243, val loss: 2.4859\n",
      "5792: lr: 0.0050, train loss: 2.4527, val loss: 2.4687\n",
      "5793: lr: 0.0050, train loss: 2.4409, val loss: 2.5547\n",
      "5794: lr: 0.0050, train loss: 2.4570, val loss: 2.4699\n",
      "5795: lr: 0.0050, train loss: 2.4287, val loss: 2.4688\n",
      "5796: lr: 0.0050, train loss: 2.4334, val loss: 2.4663\n",
      "5797: lr: 0.0050, train loss: 2.4865, val loss: 2.4718\n",
      "5798: lr: 0.0050, train loss: 2.4419, val loss: 2.4979\n",
      "5799: lr: 0.0050, train loss: 2.4712, val loss: 2.4481\n",
      "5800: lr: 0.0050, train loss: 2.4364, val loss: 2.4939\n",
      "5801: lr: 0.0050, train loss: 2.4789, val loss: 2.4530\n",
      "5802: lr: 0.0050, train loss: 2.4350, val loss: 2.5061\n",
      "5803: lr: 0.0050, train loss: 2.4509, val loss: 2.4771\n",
      "5804: lr: 0.0050, train loss: 2.4868, val loss: 2.5020\n",
      "5805: lr: 0.0050, train loss: 2.4735, val loss: 2.5020\n",
      "5806: lr: 0.0050, train loss: 2.4544, val loss: 2.4967\n",
      "5807: lr: 0.0050, train loss: 2.4533, val loss: 2.5323\n",
      "5808: lr: 0.0050, train loss: 2.4077, val loss: 2.4551\n",
      "5809: lr: 0.0050, train loss: 2.4550, val loss: 2.4536\n",
      "5810: lr: 0.0050, train loss: 2.4815, val loss: 2.4893\n",
      "5811: lr: 0.0050, train loss: 2.4150, val loss: 2.4879\n",
      "5812: lr: 0.0050, train loss: 2.4324, val loss: 2.4482\n",
      "5813: lr: 0.0050, train loss: 2.4257, val loss: 2.4658\n",
      "5814: lr: 0.0050, train loss: 2.4235, val loss: 2.4836\n",
      "5815: lr: 0.0050, train loss: 2.5012, val loss: 2.4965\n",
      "5816: lr: 0.0050, train loss: 2.4120, val loss: 2.4945\n",
      "5817: lr: 0.0050, train loss: 2.4774, val loss: 2.4775\n",
      "5818: lr: 0.0050, train loss: 2.4538, val loss: 2.4682\n",
      "5819: lr: 0.0050, train loss: 2.4820, val loss: 2.5042\n",
      "5820: lr: 0.0050, train loss: 2.4753, val loss: 2.4767\n",
      "5821: lr: 0.0050, train loss: 2.4237, val loss: 2.5139\n",
      "5822: lr: 0.0050, train loss: 2.4496, val loss: 2.5201\n",
      "5823: lr: 0.0050, train loss: 2.4801, val loss: 2.4869\n",
      "5824: lr: 0.0050, train loss: 2.4741, val loss: 2.4960\n",
      "5825: lr: 0.0050, train loss: 2.4776, val loss: 2.4199\n",
      "5826: lr: 0.0050, train loss: 2.4431, val loss: 2.5177\n",
      "5827: lr: 0.0050, train loss: 2.4674, val loss: 2.4407\n",
      "5828: lr: 0.0050, train loss: 2.4285, val loss: 2.4790\n",
      "5829: lr: 0.0050, train loss: 2.4447, val loss: 2.4612\n",
      "5830: lr: 0.0050, train loss: 2.4303, val loss: 2.4813\n",
      "5831: lr: 0.0050, train loss: 2.4495, val loss: 2.5255\n",
      "5832: lr: 0.0050, train loss: 2.4680, val loss: 2.5042\n",
      "5833: lr: 0.0050, train loss: 2.4309, val loss: 2.5085\n",
      "5834: lr: 0.0050, train loss: 2.4281, val loss: 2.4969\n",
      "5835: lr: 0.0050, train loss: 2.4709, val loss: 2.4768\n",
      "5836: lr: 0.0050, train loss: 2.4582, val loss: 2.4790\n",
      "5837: lr: 0.0050, train loss: 2.4425, val loss: 2.4475\n",
      "5838: lr: 0.0050, train loss: 2.4554, val loss: 2.5271\n",
      "5839: lr: 0.0050, train loss: 2.4589, val loss: 2.4568\n",
      "5840: lr: 0.0050, train loss: 2.4529, val loss: 2.4488\n",
      "5841: lr: 0.0050, train loss: 2.3764, val loss: 2.5463\n",
      "5842: lr: 0.0050, train loss: 2.4990, val loss: 2.4931\n",
      "5843: lr: 0.0050, train loss: 2.4436, val loss: 2.4426\n",
      "5844: lr: 0.0050, train loss: 2.4093, val loss: 2.4889\n",
      "5845: lr: 0.0050, train loss: 2.4244, val loss: 2.5435\n",
      "5846: lr: 0.0050, train loss: 2.4572, val loss: 2.5179\n",
      "5847: lr: 0.0050, train loss: 2.4384, val loss: 2.4764\n",
      "5848: lr: 0.0050, train loss: 2.4544, val loss: 2.4272\n",
      "5849: lr: 0.0050, train loss: 2.4730, val loss: 2.4374\n",
      "5850: lr: 0.0050, train loss: 2.4663, val loss: 2.4416\n",
      "5851: lr: 0.0050, train loss: 2.4005, val loss: 2.4931\n",
      "5852: lr: 0.0050, train loss: 2.4648, val loss: 2.5329\n",
      "5853: lr: 0.0050, train loss: 2.5089, val loss: 2.4851\n",
      "5854: lr: 0.0050, train loss: 2.4338, val loss: 2.4881\n",
      "5855: lr: 0.0050, train loss: 2.4520, val loss: 2.5094\n",
      "5856: lr: 0.0050, train loss: 2.4361, val loss: 2.4707\n",
      "5857: lr: 0.0050, train loss: 2.4719, val loss: 2.5338\n",
      "5858: lr: 0.0050, train loss: 2.4652, val loss: 2.4939\n",
      "5859: lr: 0.0050, train loss: 2.4817, val loss: 2.4418\n",
      "5860: lr: 0.0050, train loss: 2.4599, val loss: 2.4699\n",
      "5861: lr: 0.0050, train loss: 2.4452, val loss: 2.5168\n",
      "5862: lr: 0.0050, train loss: 2.4676, val loss: 2.5094\n",
      "5863: lr: 0.0050, train loss: 2.4491, val loss: 2.4638\n",
      "5864: lr: 0.0050, train loss: 2.4267, val loss: 2.5068\n",
      "5865: lr: 0.0050, train loss: 2.4766, val loss: 2.5303\n",
      "5866: lr: 0.0050, train loss: 2.5177, val loss: 2.4460\n",
      "5867: lr: 0.0050, train loss: 2.4917, val loss: 2.5022\n",
      "5868: lr: 0.0050, train loss: 2.4622, val loss: 2.5090\n",
      "5869: lr: 0.0050, train loss: 2.4076, val loss: 2.5100\n",
      "5870: lr: 0.0050, train loss: 2.4826, val loss: 2.4821\n",
      "5871: lr: 0.0050, train loss: 2.4107, val loss: 2.4758\n",
      "5872: lr: 0.0050, train loss: 2.4725, val loss: 2.4920\n",
      "5873: lr: 0.0050, train loss: 2.4548, val loss: 2.5268\n",
      "5874: lr: 0.0050, train loss: 2.4221, val loss: 2.4449\n",
      "5875: lr: 0.0050, train loss: 2.4355, val loss: 2.4185\n",
      "5876: lr: 0.0050, train loss: 2.4122, val loss: 2.5115\n",
      "5877: lr: 0.0050, train loss: 2.4633, val loss: 2.4864\n",
      "5878: lr: 0.0050, train loss: 2.4254, val loss: 2.4691\n",
      "5879: lr: 0.0050, train loss: 2.4801, val loss: 2.4725\n",
      "5880: lr: 0.0050, train loss: 2.4888, val loss: 2.4421\n",
      "5881: lr: 0.0050, train loss: 2.4427, val loss: 2.4683\n",
      "5882: lr: 0.0050, train loss: 2.4290, val loss: 2.4281\n",
      "5883: lr: 0.0050, train loss: 2.4595, val loss: 2.4656\n",
      "5884: lr: 0.0050, train loss: 2.4196, val loss: 2.4538\n",
      "5885: lr: 0.0050, train loss: 2.4704, val loss: 2.4873\n",
      "5886: lr: 0.0050, train loss: 2.4587, val loss: 2.4978\n",
      "5887: lr: 0.0050, train loss: 2.4912, val loss: 2.4710\n",
      "5888: lr: 0.0050, train loss: 2.4674, val loss: 2.4916\n",
      "5889: lr: 0.0050, train loss: 2.4604, val loss: 2.5098\n",
      "5890: lr: 0.0050, train loss: 2.4799, val loss: 2.4824\n",
      "5891: lr: 0.0050, train loss: 2.4354, val loss: 2.4824\n",
      "5892: lr: 0.0050, train loss: 2.4734, val loss: 2.5172\n",
      "5893: lr: 0.0050, train loss: 2.4445, val loss: 2.5364\n",
      "5894: lr: 0.0050, train loss: 2.4272, val loss: 2.4628\n",
      "5895: lr: 0.0050, train loss: 2.4602, val loss: 2.4839\n",
      "5896: lr: 0.0050, train loss: 2.4123, val loss: 2.4748\n",
      "5897: lr: 0.0050, train loss: 2.4474, val loss: 2.4591\n",
      "5898: lr: 0.0050, train loss: 2.4239, val loss: 2.4921\n",
      "5899: lr: 0.0050, train loss: 2.4897, val loss: 2.4525\n",
      "5900: lr: 0.0050, train loss: 2.4436, val loss: 2.4433\n",
      "5901: lr: 0.0050, train loss: 2.4196, val loss: 2.4581\n",
      "5902: lr: 0.0050, train loss: 2.4329, val loss: 2.4625\n",
      "5903: lr: 0.0050, train loss: 2.4318, val loss: 2.4803\n",
      "5904: lr: 0.0050, train loss: 2.4751, val loss: 2.4470\n",
      "5905: lr: 0.0050, train loss: 2.4508, val loss: 2.4966\n",
      "5906: lr: 0.0050, train loss: 2.4587, val loss: 2.4940\n",
      "5907: lr: 0.0050, train loss: 2.4834, val loss: 2.4825\n",
      "5908: lr: 0.0050, train loss: 2.4874, val loss: 2.4370\n",
      "5909: lr: 0.0050, train loss: 2.4658, val loss: 2.4985\n",
      "5910: lr: 0.0050, train loss: 2.4718, val loss: 2.4812\n",
      "5911: lr: 0.0050, train loss: 2.4821, val loss: 2.4972\n",
      "5912: lr: 0.0050, train loss: 2.4510, val loss: 2.5194\n",
      "5913: lr: 0.0050, train loss: 2.4747, val loss: 2.4282\n",
      "5914: lr: 0.0050, train loss: 2.5059, val loss: 2.4701\n",
      "5915: lr: 0.0050, train loss: 2.4894, val loss: 2.4867\n",
      "5916: lr: 0.0050, train loss: 2.4656, val loss: 2.4566\n",
      "5917: lr: 0.0050, train loss: 2.4619, val loss: 2.4592\n",
      "5918: lr: 0.0050, train loss: 2.4561, val loss: 2.4871\n",
      "5919: lr: 0.0050, train loss: 2.4378, val loss: 2.5027\n",
      "5920: lr: 0.0050, train loss: 2.4365, val loss: 2.4600\n",
      "5921: lr: 0.0050, train loss: 2.4791, val loss: 2.4874\n",
      "5922: lr: 0.0050, train loss: 2.4659, val loss: 2.4999\n",
      "5923: lr: 0.0050, train loss: 2.4195, val loss: 2.5515\n",
      "5924: lr: 0.0050, train loss: 2.4586, val loss: 2.4951\n",
      "5925: lr: 0.0050, train loss: 2.4348, val loss: 2.4819\n",
      "5926: lr: 0.0050, train loss: 2.4810, val loss: 2.5229\n",
      "5927: lr: 0.0050, train loss: 2.4441, val loss: 2.4994\n",
      "5928: lr: 0.0050, train loss: 2.4608, val loss: 2.4714\n",
      "5929: lr: 0.0050, train loss: 2.4771, val loss: 2.5056\n",
      "5930: lr: 0.0050, train loss: 2.4824, val loss: 2.4732\n",
      "5931: lr: 0.0050, train loss: 2.4607, val loss: 2.5026\n",
      "5932: lr: 0.0050, train loss: 2.4682, val loss: 2.5036\n",
      "5933: lr: 0.0050, train loss: 2.4762, val loss: 2.4781\n",
      "5934: lr: 0.0050, train loss: 2.4138, val loss: 2.4760\n",
      "5935: lr: 0.0050, train loss: 2.4326, val loss: 2.5063\n",
      "5936: lr: 0.0050, train loss: 2.4391, val loss: 2.4767\n",
      "5937: lr: 0.0050, train loss: 2.4299, val loss: 2.4362\n",
      "5938: lr: 0.0050, train loss: 2.4432, val loss: 2.5048\n",
      "5939: lr: 0.0050, train loss: 2.4940, val loss: 2.5013\n",
      "5940: lr: 0.0050, train loss: 2.4380, val loss: 2.4916\n",
      "5941: lr: 0.0050, train loss: 2.4585, val loss: 2.4983\n",
      "5942: lr: 0.0050, train loss: 2.4677, val loss: 2.4960\n",
      "5943: lr: 0.0050, train loss: 2.4792, val loss: 2.5009\n",
      "5944: lr: 0.0050, train loss: 2.4710, val loss: 2.4613\n",
      "5945: lr: 0.0050, train loss: 2.4608, val loss: 2.5338\n",
      "5946: lr: 0.0050, train loss: 2.4572, val loss: 2.5104\n",
      "5947: lr: 0.0050, train loss: 2.4513, val loss: 2.5329\n",
      "5948: lr: 0.0050, train loss: 2.4937, val loss: 2.4902\n",
      "5949: lr: 0.0050, train loss: 2.4119, val loss: 2.4452\n",
      "5950: lr: 0.0050, train loss: 2.4074, val loss: 2.4860\n",
      "5951: lr: 0.0050, train loss: 2.4785, val loss: 2.4560\n",
      "5952: lr: 0.0050, train loss: 2.4732, val loss: 2.5298\n",
      "5953: lr: 0.0050, train loss: 2.4656, val loss: 2.4820\n",
      "5954: lr: 0.0050, train loss: 2.4262, val loss: 2.5127\n",
      "5955: lr: 0.0050, train loss: 2.4649, val loss: 2.5412\n",
      "5956: lr: 0.0050, train loss: 2.4521, val loss: 2.4841\n",
      "5957: lr: 0.0050, train loss: 2.4951, val loss: 2.4444\n",
      "5958: lr: 0.0050, train loss: 2.4753, val loss: 2.4711\n",
      "5959: lr: 0.0050, train loss: 2.4754, val loss: 2.4670\n",
      "5960: lr: 0.0050, train loss: 2.4288, val loss: 2.4736\n",
      "5961: lr: 0.0050, train loss: 2.4619, val loss: 2.4573\n",
      "5962: lr: 0.0050, train loss: 2.4785, val loss: 2.4254\n",
      "5963: lr: 0.0050, train loss: 2.4536, val loss: 2.4827\n",
      "5964: lr: 0.0050, train loss: 2.4463, val loss: 2.5118\n",
      "5965: lr: 0.0050, train loss: 2.4225, val loss: 2.5544\n",
      "5966: lr: 0.0050, train loss: 2.4708, val loss: 2.4987\n",
      "5967: lr: 0.0050, train loss: 2.5034, val loss: 2.4533\n",
      "5968: lr: 0.0050, train loss: 2.4544, val loss: 2.4546\n",
      "5969: lr: 0.0050, train loss: 2.4263, val loss: 2.4868\n",
      "5970: lr: 0.0050, train loss: 2.4467, val loss: 2.5196\n",
      "5971: lr: 0.0050, train loss: 2.4441, val loss: 2.5027\n",
      "5972: lr: 0.0050, train loss: 2.4727, val loss: 2.5288\n",
      "5973: lr: 0.0050, train loss: 2.4410, val loss: 2.4914\n",
      "5974: lr: 0.0050, train loss: 2.4888, val loss: 2.5087\n",
      "5975: lr: 0.0050, train loss: 2.4429, val loss: 2.4699\n",
      "5976: lr: 0.0050, train loss: 2.4574, val loss: 2.4357\n",
      "5977: lr: 0.0050, train loss: 2.5051, val loss: 2.5719\n",
      "5978: lr: 0.0050, train loss: 2.4275, val loss: 2.4610\n",
      "5979: lr: 0.0050, train loss: 2.4794, val loss: 2.4766\n",
      "5980: lr: 0.0050, train loss: 2.4688, val loss: 2.5125\n",
      "5981: lr: 0.0050, train loss: 2.4741, val loss: 2.4765\n",
      "5982: lr: 0.0050, train loss: 2.4967, val loss: 2.5130\n",
      "5983: lr: 0.0050, train loss: 2.4977, val loss: 2.4889\n",
      "5984: lr: 0.0050, train loss: 2.4917, val loss: 2.5150\n",
      "5985: lr: 0.0050, train loss: 2.4465, val loss: 2.5005\n",
      "5986: lr: 0.0050, train loss: 2.4422, val loss: 2.5254\n",
      "5987: lr: 0.0050, train loss: 2.5092, val loss: 2.5002\n",
      "5988: lr: 0.0050, train loss: 2.4640, val loss: 2.4874\n",
      "5989: lr: 0.0050, train loss: 2.4649, val loss: 2.4992\n",
      "5990: lr: 0.0050, train loss: 2.4319, val loss: 2.4781\n",
      "5991: lr: 0.0050, train loss: 2.4234, val loss: 2.5260\n",
      "5992: lr: 0.0050, train loss: 2.4554, val loss: 2.5144\n",
      "5993: lr: 0.0050, train loss: 2.4697, val loss: 2.5082\n",
      "5994: lr: 0.0050, train loss: 2.4957, val loss: 2.4755\n",
      "5995: lr: 0.0050, train loss: 2.4747, val loss: 2.5103\n",
      "5996: lr: 0.0050, train loss: 2.4117, val loss: 2.4919\n",
      "5997: lr: 0.0050, train loss: 2.4600, val loss: 2.4823\n",
      "5998: lr: 0.0050, train loss: 2.4310, val loss: 2.4563\n",
      "5999: lr: 0.0050, train loss: 2.4769, val loss: 2.4741\n",
      "6000: lr: 0.0050, train loss: 2.4683, val loss: 2.4949\n",
      "6001: lr: 0.0050, train loss: 2.4624, val loss: 2.4250\n",
      "6002: lr: 0.0050, train loss: 2.4541, val loss: 2.4772\n",
      "6003: lr: 0.0050, train loss: 2.3913, val loss: 2.4132\n",
      "6004: lr: 0.0050, train loss: 2.4689, val loss: 2.4860\n",
      "6005: lr: 0.0050, train loss: 2.5158, val loss: 2.5609\n",
      "6006: lr: 0.0050, train loss: 2.4727, val loss: 2.4359\n",
      "6007: lr: 0.0050, train loss: 2.4462, val loss: 2.4999\n",
      "6008: lr: 0.0050, train loss: 2.4473, val loss: 2.4836\n",
      "6009: lr: 0.0050, train loss: 2.4379, val loss: 2.4783\n",
      "6010: lr: 0.0050, train loss: 2.4612, val loss: 2.5193\n",
      "6011: lr: 0.0050, train loss: 2.4517, val loss: 2.4618\n",
      "6012: lr: 0.0050, train loss: 2.4401, val loss: 2.4961\n",
      "6013: lr: 0.0050, train loss: 2.4731, val loss: 2.4857\n",
      "6014: lr: 0.0050, train loss: 2.4443, val loss: 2.4626\n",
      "6015: lr: 0.0050, train loss: 2.4557, val loss: 2.5280\n",
      "6016: lr: 0.0050, train loss: 2.4648, val loss: 2.5009\n",
      "6017: lr: 0.0050, train loss: 2.4367, val loss: 2.4890\n",
      "6018: lr: 0.0050, train loss: 2.4583, val loss: 2.4913\n",
      "6019: lr: 0.0050, train loss: 2.4540, val loss: 2.4606\n",
      "6020: lr: 0.0050, train loss: 2.4279, val loss: 2.5029\n",
      "6021: lr: 0.0050, train loss: 2.4491, val loss: 2.4465\n",
      "6022: lr: 0.0050, train loss: 2.4684, val loss: 2.4659\n",
      "6023: lr: 0.0050, train loss: 2.4375, val loss: 2.4743\n",
      "6024: lr: 0.0050, train loss: 2.4537, val loss: 2.5287\n",
      "6025: lr: 0.0050, train loss: 2.4819, val loss: 2.4989\n",
      "6026: lr: 0.0050, train loss: 2.4634, val loss: 2.5126\n",
      "6027: lr: 0.0050, train loss: 2.4570, val loss: 2.5010\n",
      "6028: lr: 0.0050, train loss: 2.4191, val loss: 2.4663\n",
      "6029: lr: 0.0050, train loss: 2.4689, val loss: 2.5293\n",
      "6030: lr: 0.0050, train loss: 2.4996, val loss: 2.4544\n",
      "6031: lr: 0.0050, train loss: 2.4883, val loss: 2.4506\n",
      "6032: lr: 0.0050, train loss: 2.4777, val loss: 2.4479\n",
      "6033: lr: 0.0050, train loss: 2.4578, val loss: 2.4997\n",
      "6034: lr: 0.0050, train loss: 2.5120, val loss: 2.4796\n",
      "6035: lr: 0.0050, train loss: 2.4608, val loss: 2.4975\n",
      "6036: lr: 0.0050, train loss: 2.4402, val loss: 2.4824\n",
      "6037: lr: 0.0050, train loss: 2.4642, val loss: 2.5154\n",
      "6038: lr: 0.0050, train loss: 2.5253, val loss: 2.4897\n",
      "6039: lr: 0.0050, train loss: 2.4168, val loss: 2.4819\n",
      "6040: lr: 0.0050, train loss: 2.4500, val loss: 2.5012\n",
      "6041: lr: 0.0050, train loss: 2.4632, val loss: 2.4796\n",
      "6042: lr: 0.0050, train loss: 2.4693, val loss: 2.4804\n",
      "6043: lr: 0.0050, train loss: 2.4625, val loss: 2.4839\n",
      "6044: lr: 0.0050, train loss: 2.4714, val loss: 2.4946\n",
      "6045: lr: 0.0050, train loss: 2.4704, val loss: 2.5226\n",
      "6046: lr: 0.0050, train loss: 2.4392, val loss: 2.5391\n",
      "6047: lr: 0.0050, train loss: 2.4470, val loss: 2.4911\n",
      "6048: lr: 0.0050, train loss: 2.4386, val loss: 2.5008\n",
      "6049: lr: 0.0050, train loss: 2.4857, val loss: 2.4966\n",
      "6050: lr: 0.0050, train loss: 2.4085, val loss: 2.5152\n",
      "6051: lr: 0.0050, train loss: 2.4505, val loss: 2.4438\n",
      "6052: lr: 0.0050, train loss: 2.4583, val loss: 2.4873\n",
      "6053: lr: 0.0050, train loss: 2.4621, val loss: 2.4918\n",
      "6054: lr: 0.0050, train loss: 2.4414, val loss: 2.4565\n",
      "6055: lr: 0.0050, train loss: 2.4446, val loss: 2.4861\n",
      "6056: lr: 0.0050, train loss: 2.4252, val loss: 2.4747\n",
      "6057: lr: 0.0050, train loss: 2.4783, val loss: 2.4624\n",
      "6058: lr: 0.0050, train loss: 2.4814, val loss: 2.4795\n",
      "6059: lr: 0.0050, train loss: 2.4757, val loss: 2.4999\n",
      "6060: lr: 0.0050, train loss: 2.4448, val loss: 2.4730\n",
      "6061: lr: 0.0050, train loss: 2.4890, val loss: 2.4985\n",
      "6062: lr: 0.0050, train loss: 2.4853, val loss: 2.4161\n",
      "6063: lr: 0.0050, train loss: 2.4280, val loss: 2.5345\n",
      "6064: lr: 0.0050, train loss: 2.4307, val loss: 2.5436\n",
      "6065: lr: 0.0050, train loss: 2.4566, val loss: 2.4161\n",
      "6066: lr: 0.0050, train loss: 2.4343, val loss: 2.4788\n",
      "6067: lr: 0.0050, train loss: 2.4446, val loss: 2.4545\n",
      "6068: lr: 0.0050, train loss: 2.5157, val loss: 2.4778\n",
      "6069: lr: 0.0050, train loss: 2.4653, val loss: 2.4926\n",
      "6070: lr: 0.0050, train loss: 2.4483, val loss: 2.5272\n",
      "6071: lr: 0.0050, train loss: 2.4939, val loss: 2.4745\n",
      "6072: lr: 0.0050, train loss: 2.4476, val loss: 2.4510\n",
      "6073: lr: 0.0050, train loss: 2.4295, val loss: 2.5297\n",
      "6074: lr: 0.0050, train loss: 2.4478, val loss: 2.5093\n",
      "6075: lr: 0.0050, train loss: 2.4330, val loss: 2.4505\n",
      "6076: lr: 0.0050, train loss: 2.4788, val loss: 2.5076\n",
      "6077: lr: 0.0050, train loss: 2.4698, val loss: 2.5020\n",
      "6078: lr: 0.0050, train loss: 2.4132, val loss: 2.5313\n",
      "6079: lr: 0.0050, train loss: 2.4423, val loss: 2.4794\n",
      "6080: lr: 0.0050, train loss: 2.4060, val loss: 2.4977\n",
      "6081: lr: 0.0050, train loss: 2.3900, val loss: 2.5132\n",
      "6082: lr: 0.0050, train loss: 2.4561, val loss: 2.4932\n",
      "6083: lr: 0.0050, train loss: 2.4532, val loss: 2.5113\n",
      "6084: lr: 0.0050, train loss: 2.4835, val loss: 2.4752\n",
      "6085: lr: 0.0050, train loss: 2.4801, val loss: 2.4169\n",
      "6086: lr: 0.0050, train loss: 2.4945, val loss: 2.5548\n",
      "6087: lr: 0.0050, train loss: 2.4569, val loss: 2.4926\n",
      "6088: lr: 0.0050, train loss: 2.5012, val loss: 2.5285\n",
      "6089: lr: 0.0050, train loss: 2.4827, val loss: 2.5081\n",
      "6090: lr: 0.0050, train loss: 2.4449, val loss: 2.4827\n",
      "6091: lr: 0.0050, train loss: 2.4169, val loss: 2.5066\n",
      "6092: lr: 0.0050, train loss: 2.4463, val loss: 2.4809\n",
      "6093: lr: 0.0050, train loss: 2.4829, val loss: 2.4498\n",
      "6094: lr: 0.0050, train loss: 2.4762, val loss: 2.5108\n",
      "6095: lr: 0.0050, train loss: 2.5186, val loss: 2.4752\n",
      "6096: lr: 0.0050, train loss: 2.4866, val loss: 2.5539\n",
      "6097: lr: 0.0050, train loss: 2.4437, val loss: 2.4508\n",
      "6098: lr: 0.0050, train loss: 2.4832, val loss: 2.4469\n",
      "6099: lr: 0.0050, train loss: 2.4611, val loss: 2.4651\n",
      "6100: lr: 0.0050, train loss: 2.4457, val loss: 2.4717\n",
      "6101: lr: 0.0050, train loss: 2.4253, val loss: 2.5249\n",
      "6102: lr: 0.0050, train loss: 2.4489, val loss: 2.5063\n",
      "6103: lr: 0.0050, train loss: 2.3921, val loss: 2.4250\n",
      "6104: lr: 0.0050, train loss: 2.4688, val loss: 2.5390\n",
      "6105: lr: 0.0050, train loss: 2.4589, val loss: 2.4955\n",
      "6106: lr: 0.0050, train loss: 2.4386, val loss: 2.4585\n",
      "6107: lr: 0.0050, train loss: 2.4636, val loss: 2.4835\n",
      "6108: lr: 0.0050, train loss: 2.4792, val loss: 2.4781\n",
      "6109: lr: 0.0050, train loss: 2.4618, val loss: 2.5218\n",
      "6110: lr: 0.0050, train loss: 2.4777, val loss: 2.4808\n",
      "6111: lr: 0.0050, train loss: 2.4347, val loss: 2.5085\n",
      "6112: lr: 0.0050, train loss: 2.4322, val loss: 2.4906\n",
      "6113: lr: 0.0050, train loss: 2.4307, val loss: 2.4809\n",
      "6114: lr: 0.0050, train loss: 2.4897, val loss: 2.5553\n",
      "6115: lr: 0.0050, train loss: 2.4767, val loss: 2.4508\n",
      "6116: lr: 0.0050, train loss: 2.4384, val loss: 2.5124\n",
      "6117: lr: 0.0050, train loss: 2.4506, val loss: 2.4721\n",
      "6118: lr: 0.0050, train loss: 2.4281, val loss: 2.4559\n",
      "6119: lr: 0.0050, train loss: 2.4767, val loss: 2.5101\n",
      "6120: lr: 0.0050, train loss: 2.4995, val loss: 2.4591\n",
      "6121: lr: 0.0050, train loss: 2.4640, val loss: 2.5203\n",
      "6122: lr: 0.0050, train loss: 2.4572, val loss: 2.5713\n",
      "6123: lr: 0.0050, train loss: 2.4620, val loss: 2.5226\n",
      "6124: lr: 0.0050, train loss: 2.5108, val loss: 2.4449\n",
      "6125: lr: 0.0050, train loss: 2.4720, val loss: 2.4918\n",
      "6126: lr: 0.0050, train loss: 2.4382, val loss: 2.4650\n",
      "6127: lr: 0.0050, train loss: 2.4937, val loss: 2.4745\n",
      "6128: lr: 0.0050, train loss: 2.4840, val loss: 2.4760\n",
      "6129: lr: 0.0050, train loss: 2.4437, val loss: 2.4774\n",
      "6130: lr: 0.0050, train loss: 2.4462, val loss: 2.4737\n",
      "6131: lr: 0.0050, train loss: 2.4804, val loss: 2.4889\n",
      "6132: lr: 0.0050, train loss: 2.4572, val loss: 2.4986\n",
      "6133: lr: 0.0050, train loss: 2.4468, val loss: 2.5240\n",
      "6134: lr: 0.0050, train loss: 2.4452, val loss: 2.4841\n",
      "6135: lr: 0.0050, train loss: 2.4526, val loss: 2.4546\n",
      "6136: lr: 0.0050, train loss: 2.4504, val loss: 2.4771\n",
      "6137: lr: 0.0050, train loss: 2.4637, val loss: 2.4767\n",
      "6138: lr: 0.0050, train loss: 2.3887, val loss: 2.4737\n",
      "6139: lr: 0.0050, train loss: 2.4377, val loss: 2.5286\n",
      "6140: lr: 0.0050, train loss: 2.4352, val loss: 2.4828\n",
      "6141: lr: 0.0050, train loss: 2.4435, val loss: 2.4922\n",
      "6142: lr: 0.0050, train loss: 2.4406, val loss: 2.5078\n",
      "6143: lr: 0.0050, train loss: 2.4867, val loss: 2.4423\n",
      "6144: lr: 0.0050, train loss: 2.4726, val loss: 2.4859\n",
      "6145: lr: 0.0050, train loss: 2.4789, val loss: 2.5057\n",
      "6146: lr: 0.0050, train loss: 2.4565, val loss: 2.4823\n",
      "6147: lr: 0.0050, train loss: 2.4651, val loss: 2.4768\n",
      "6148: lr: 0.0050, train loss: 2.4805, val loss: 2.5109\n",
      "6149: lr: 0.0050, train loss: 2.4521, val loss: 2.4948\n",
      "6150: lr: 0.0050, train loss: 2.4630, val loss: 2.4309\n",
      "6151: lr: 0.0050, train loss: 2.4611, val loss: 2.5177\n",
      "6152: lr: 0.0050, train loss: 2.4409, val loss: 2.4517\n",
      "6153: lr: 0.0050, train loss: 2.4943, val loss: 2.5047\n",
      "6154: lr: 0.0050, train loss: 2.4209, val loss: 2.4319\n",
      "6155: lr: 0.0050, train loss: 2.4603, val loss: 2.5038\n",
      "6156: lr: 0.0050, train loss: 2.4923, val loss: 2.4735\n",
      "6157: lr: 0.0050, train loss: 2.4352, val loss: 2.5241\n",
      "6158: lr: 0.0050, train loss: 2.4335, val loss: 2.5112\n",
      "6159: lr: 0.0050, train loss: 2.4891, val loss: 2.4950\n",
      "6160: lr: 0.0050, train loss: 2.4654, val loss: 2.4520\n",
      "6161: lr: 0.0050, train loss: 2.4513, val loss: 2.4788\n",
      "6162: lr: 0.0050, train loss: 2.4975, val loss: 2.4544\n",
      "6163: lr: 0.0050, train loss: 2.4886, val loss: 2.4638\n",
      "6164: lr: 0.0050, train loss: 2.4170, val loss: 2.4609\n",
      "6165: lr: 0.0050, train loss: 2.4643, val loss: 2.4435\n",
      "6166: lr: 0.0050, train loss: 2.4464, val loss: 2.4714\n",
      "6167: lr: 0.0050, train loss: 2.4565, val loss: 2.4976\n",
      "6168: lr: 0.0050, train loss: 2.4532, val loss: 2.4936\n",
      "6169: lr: 0.0050, train loss: 2.4532, val loss: 2.4824\n",
      "6170: lr: 0.0050, train loss: 2.4446, val loss: 2.4974\n",
      "6171: lr: 0.0050, train loss: 2.4053, val loss: 2.4563\n",
      "6172: lr: 0.0050, train loss: 2.4276, val loss: 2.4529\n",
      "6173: lr: 0.0050, train loss: 2.4723, val loss: 2.4989\n",
      "6174: lr: 0.0050, train loss: 2.4348, val loss: 2.5246\n",
      "6175: lr: 0.0050, train loss: 2.4426, val loss: 2.4670\n",
      "6176: lr: 0.0050, train loss: 2.4844, val loss: 2.5036\n",
      "6177: lr: 0.0050, train loss: 2.4626, val loss: 2.4896\n",
      "6178: lr: 0.0050, train loss: 2.4603, val loss: 2.5080\n",
      "6179: lr: 0.0050, train loss: 2.5164, val loss: 2.4558\n",
      "6180: lr: 0.0050, train loss: 2.4662, val loss: 2.4863\n",
      "6181: lr: 0.0050, train loss: 2.4190, val loss: 2.5194\n",
      "6182: lr: 0.0050, train loss: 2.4584, val loss: 2.4671\n",
      "6183: lr: 0.0050, train loss: 2.4173, val loss: 2.4799\n",
      "6184: lr: 0.0050, train loss: 2.4385, val loss: 2.4810\n",
      "6185: lr: 0.0050, train loss: 2.4814, val loss: 2.4108\n",
      "6186: lr: 0.0050, train loss: 2.5063, val loss: 2.4419\n",
      "6187: lr: 0.0050, train loss: 2.4603, val loss: 2.5490\n",
      "6188: lr: 0.0050, train loss: 2.4467, val loss: 2.4922\n",
      "6189: lr: 0.0050, train loss: 2.4800, val loss: 2.5208\n",
      "6190: lr: 0.0050, train loss: 2.4728, val loss: 2.4555\n",
      "6191: lr: 0.0050, train loss: 2.4811, val loss: 2.4515\n",
      "6192: lr: 0.0050, train loss: 2.4455, val loss: 2.4999\n",
      "6193: lr: 0.0050, train loss: 2.4240, val loss: 2.4620\n",
      "6194: lr: 0.0050, train loss: 2.4686, val loss: 2.5229\n",
      "6195: lr: 0.0050, train loss: 2.4473, val loss: 2.5032\n",
      "6196: lr: 0.0050, train loss: 2.4559, val loss: 2.4750\n",
      "6197: lr: 0.0050, train loss: 2.4477, val loss: 2.5090\n",
      "6198: lr: 0.0050, train loss: 2.4491, val loss: 2.4769\n",
      "6199: lr: 0.0050, train loss: 2.4337, val loss: 2.4478\n",
      "6200: lr: 0.0050, train loss: 2.4458, val loss: 2.5152\n",
      "6201: lr: 0.0050, train loss: 2.4344, val loss: 2.5044\n",
      "6202: lr: 0.0050, train loss: 2.4087, val loss: 2.4500\n",
      "6203: lr: 0.0050, train loss: 2.4402, val loss: 2.4417\n",
      "6204: lr: 0.0050, train loss: 2.4469, val loss: 2.4988\n",
      "6205: lr: 0.0050, train loss: 2.4338, val loss: 2.4742\n",
      "6206: lr: 0.0050, train loss: 2.4594, val loss: 2.5034\n",
      "6207: lr: 0.0050, train loss: 2.4273, val loss: 2.4994\n",
      "6208: lr: 0.0050, train loss: 2.4796, val loss: 2.4971\n",
      "6209: lr: 0.0050, train loss: 2.4476, val loss: 2.4423\n",
      "6210: lr: 0.0050, train loss: 2.4589, val loss: 2.5346\n",
      "6211: lr: 0.0050, train loss: 2.5052, val loss: 2.5236\n",
      "6212: lr: 0.0050, train loss: 2.4661, val loss: 2.4960\n",
      "6213: lr: 0.0050, train loss: 2.4764, val loss: 2.4473\n",
      "6214: lr: 0.0050, train loss: 2.4393, val loss: 2.5019\n",
      "6215: lr: 0.0050, train loss: 2.4597, val loss: 2.4772\n",
      "6216: lr: 0.0050, train loss: 2.4294, val loss: 2.4336\n",
      "6217: lr: 0.0050, train loss: 2.4353, val loss: 2.4674\n",
      "6218: lr: 0.0050, train loss: 2.4620, val loss: 2.4514\n",
      "6219: lr: 0.0050, train loss: 2.4529, val loss: 2.4904\n",
      "6220: lr: 0.0050, train loss: 2.4807, val loss: 2.4852\n",
      "6221: lr: 0.0050, train loss: 2.4316, val loss: 2.4446\n",
      "6222: lr: 0.0050, train loss: 2.4831, val loss: 2.4939\n",
      "6223: lr: 0.0050, train loss: 2.5171, val loss: 2.4439\n",
      "6224: lr: 0.0050, train loss: 2.4557, val loss: 2.4838\n",
      "6225: lr: 0.0050, train loss: 2.4716, val loss: 2.4939\n",
      "6226: lr: 0.0050, train loss: 2.4655, val loss: 2.5624\n",
      "6227: lr: 0.0050, train loss: 2.4864, val loss: 2.5119\n",
      "6228: lr: 0.0050, train loss: 2.4433, val loss: 2.4697\n",
      "6229: lr: 0.0050, train loss: 2.4583, val loss: 2.4595\n",
      "6230: lr: 0.0050, train loss: 2.3805, val loss: 2.4988\n",
      "6231: lr: 0.0050, train loss: 2.4461, val loss: 2.4521\n",
      "6232: lr: 0.0050, train loss: 2.4614, val loss: 2.4490\n",
      "6233: lr: 0.0050, train loss: 2.4820, val loss: 2.4870\n",
      "6234: lr: 0.0050, train loss: 2.4762, val loss: 2.4856\n",
      "6235: lr: 0.0050, train loss: 2.4516, val loss: 2.4467\n",
      "6236: lr: 0.0050, train loss: 2.4647, val loss: 2.4895\n",
      "6237: lr: 0.0050, train loss: 2.4767, val loss: 2.4971\n",
      "6238: lr: 0.0050, train loss: 2.4718, val loss: 2.5152\n",
      "6239: lr: 0.0050, train loss: 2.4957, val loss: 2.4968\n",
      "6240: lr: 0.0050, train loss: 2.4258, val loss: 2.4596\n",
      "6241: lr: 0.0050, train loss: 2.4766, val loss: 2.4781\n",
      "6242: lr: 0.0050, train loss: 2.4502, val loss: 2.4902\n",
      "6243: lr: 0.0050, train loss: 2.4646, val loss: 2.4660\n",
      "6244: lr: 0.0050, train loss: 2.4827, val loss: 2.5341\n",
      "6245: lr: 0.0050, train loss: 2.4996, val loss: 2.4719\n",
      "6246: lr: 0.0050, train loss: 2.4577, val loss: 2.4977\n",
      "6247: lr: 0.0050, train loss: 2.4404, val loss: 2.5154\n",
      "6248: lr: 0.0050, train loss: 2.4390, val loss: 2.4663\n",
      "6249: lr: 0.0050, train loss: 2.4344, val loss: 2.4897\n",
      "6250: lr: 0.0050, train loss: 2.4376, val loss: 2.4582\n",
      "6251: lr: 0.0050, train loss: 2.4662, val loss: 2.4667\n",
      "6252: lr: 0.0050, train loss: 2.4467, val loss: 2.5136\n",
      "6253: lr: 0.0050, train loss: 2.4666, val loss: 2.5075\n",
      "6254: lr: 0.0050, train loss: 2.4125, val loss: 2.5023\n",
      "6255: lr: 0.0050, train loss: 2.4218, val loss: 2.4395\n",
      "6256: lr: 0.0050, train loss: 2.4240, val loss: 2.4452\n",
      "6257: lr: 0.0050, train loss: 2.4733, val loss: 2.4302\n",
      "6258: lr: 0.0050, train loss: 2.4651, val loss: 2.5390\n",
      "6259: lr: 0.0050, train loss: 2.4692, val loss: 2.5365\n",
      "6260: lr: 0.0050, train loss: 2.4223, val loss: 2.5147\n",
      "6261: lr: 0.0050, train loss: 2.4797, val loss: 2.4756\n",
      "6262: lr: 0.0050, train loss: 2.4376, val loss: 2.4720\n",
      "6263: lr: 0.0050, train loss: 2.4364, val loss: 2.4369\n",
      "6264: lr: 0.0050, train loss: 2.4753, val loss: 2.4924\n",
      "6265: lr: 0.0050, train loss: 2.4750, val loss: 2.4726\n",
      "6266: lr: 0.0050, train loss: 2.5121, val loss: 2.4905\n",
      "6267: lr: 0.0050, train loss: 2.5048, val loss: 2.5079\n",
      "6268: lr: 0.0050, train loss: 2.4440, val loss: 2.5208\n",
      "6269: lr: 0.0050, train loss: 2.4741, val loss: 2.4980\n",
      "6270: lr: 0.0050, train loss: 2.4699, val loss: 2.5048\n",
      "6271: lr: 0.0050, train loss: 2.4645, val loss: 2.5050\n",
      "6272: lr: 0.0050, train loss: 2.4375, val loss: 2.5043\n",
      "6273: lr: 0.0050, train loss: 2.4233, val loss: 2.4982\n",
      "6274: lr: 0.0050, train loss: 2.4974, val loss: 2.5101\n",
      "6275: lr: 0.0050, train loss: 2.4746, val loss: 2.5007\n",
      "6276: lr: 0.0050, train loss: 2.4276, val loss: 2.4197\n",
      "6277: lr: 0.0050, train loss: 2.4347, val loss: 2.4238\n",
      "6278: lr: 0.0050, train loss: 2.4933, val loss: 2.4812\n",
      "6279: lr: 0.0050, train loss: 2.4531, val loss: 2.4359\n",
      "6280: lr: 0.0050, train loss: 2.4286, val loss: 2.5006\n",
      "6281: lr: 0.0050, train loss: 2.4343, val loss: 2.4546\n",
      "6282: lr: 0.0050, train loss: 2.4231, val loss: 2.4891\n",
      "6283: lr: 0.0050, train loss: 2.4984, val loss: 2.4552\n",
      "6284: lr: 0.0050, train loss: 2.4337, val loss: 2.5064\n",
      "6285: lr: 0.0050, train loss: 2.4806, val loss: 2.4660\n",
      "6286: lr: 0.0050, train loss: 2.4759, val loss: 2.4408\n",
      "6287: lr: 0.0050, train loss: 2.4857, val loss: 2.5010\n",
      "6288: lr: 0.0050, train loss: 2.4595, val loss: 2.5090\n",
      "6289: lr: 0.0050, train loss: 2.4820, val loss: 2.4507\n",
      "6290: lr: 0.0050, train loss: 2.4797, val loss: 2.5257\n",
      "6291: lr: 0.0050, train loss: 2.4594, val loss: 2.5229\n",
      "6292: lr: 0.0050, train loss: 2.4727, val loss: 2.4512\n",
      "6293: lr: 0.0050, train loss: 2.4167, val loss: 2.4882\n",
      "6294: lr: 0.0050, train loss: 2.4560, val loss: 2.4576\n",
      "6295: lr: 0.0050, train loss: 2.4808, val loss: 2.4844\n",
      "6296: lr: 0.0050, train loss: 2.4162, val loss: 2.4761\n",
      "6297: lr: 0.0050, train loss: 2.4565, val loss: 2.5182\n",
      "6298: lr: 0.0050, train loss: 2.4704, val loss: 2.4888\n",
      "6299: lr: 0.0050, train loss: 2.4765, val loss: 2.4918\n",
      "6300: lr: 0.0050, train loss: 2.4843, val loss: 2.5362\n",
      "6301: lr: 0.0050, train loss: 2.4476, val loss: 2.5071\n",
      "6302: lr: 0.0050, train loss: 2.4398, val loss: 2.5173\n",
      "6303: lr: 0.0050, train loss: 2.4552, val loss: 2.5257\n",
      "6304: lr: 0.0050, train loss: 2.4562, val loss: 2.4826\n",
      "6305: lr: 0.0050, train loss: 2.4496, val loss: 2.4869\n",
      "6306: lr: 0.0050, train loss: 2.4961, val loss: 2.5181\n",
      "6307: lr: 0.0050, train loss: 2.4572, val loss: 2.4589\n",
      "6308: lr: 0.0050, train loss: 2.4733, val loss: 2.5109\n",
      "6309: lr: 0.0050, train loss: 2.4423, val loss: 2.4645\n",
      "6310: lr: 0.0050, train loss: 2.4930, val loss: 2.4745\n",
      "6311: lr: 0.0050, train loss: 2.5007, val loss: 2.4628\n",
      "6312: lr: 0.0050, train loss: 2.3949, val loss: 2.4912\n",
      "6313: lr: 0.0050, train loss: 2.4408, val loss: 2.5023\n",
      "6314: lr: 0.0050, train loss: 2.4318, val loss: 2.4745\n",
      "6315: lr: 0.0050, train loss: 2.4635, val loss: 2.4776\n",
      "6316: lr: 0.0050, train loss: 2.4920, val loss: 2.5163\n",
      "6317: lr: 0.0050, train loss: 2.4523, val loss: 2.4739\n",
      "6318: lr: 0.0050, train loss: 2.4505, val loss: 2.4731\n",
      "6319: lr: 0.0050, train loss: 2.4650, val loss: 2.5355\n",
      "6320: lr: 0.0050, train loss: 2.4712, val loss: 2.5546\n",
      "6321: lr: 0.0050, train loss: 2.4935, val loss: 2.4788\n",
      "6322: lr: 0.0050, train loss: 2.4123, val loss: 2.4655\n",
      "6323: lr: 0.0050, train loss: 2.4652, val loss: 2.4797\n",
      "6324: lr: 0.0050, train loss: 2.4318, val loss: 2.4631\n",
      "6325: lr: 0.0050, train loss: 2.4264, val loss: 2.4520\n",
      "6326: lr: 0.0050, train loss: 2.4705, val loss: 2.4539\n",
      "6327: lr: 0.0050, train loss: 2.4670, val loss: 2.4694\n",
      "6328: lr: 0.0050, train loss: 2.4982, val loss: 2.4314\n",
      "6329: lr: 0.0050, train loss: 2.4823, val loss: 2.4540\n",
      "6330: lr: 0.0050, train loss: 2.4327, val loss: 2.4819\n",
      "6331: lr: 0.0050, train loss: 2.4606, val loss: 2.4664\n",
      "6332: lr: 0.0050, train loss: 2.4271, val loss: 2.5644\n",
      "6333: lr: 0.0050, train loss: 2.4352, val loss: 2.4505\n",
      "6334: lr: 0.0050, train loss: 2.4362, val loss: 2.4272\n",
      "6335: lr: 0.0050, train loss: 2.4487, val loss: 2.4738\n",
      "6336: lr: 0.0050, train loss: 2.4417, val loss: 2.4707\n",
      "6337: lr: 0.0050, train loss: 2.4548, val loss: 2.4985\n",
      "6338: lr: 0.0050, train loss: 2.4685, val loss: 2.4646\n",
      "6339: lr: 0.0050, train loss: 2.4898, val loss: 2.4202\n",
      "6340: lr: 0.0050, train loss: 2.4403, val loss: 2.4493\n",
      "6341: lr: 0.0050, train loss: 2.4385, val loss: 2.4909\n",
      "6342: lr: 0.0050, train loss: 2.4387, val loss: 2.4897\n",
      "6343: lr: 0.0050, train loss: 2.4560, val loss: 2.4485\n",
      "6344: lr: 0.0050, train loss: 2.4366, val loss: 2.4848\n",
      "6345: lr: 0.0050, train loss: 2.4908, val loss: 2.4517\n",
      "6346: lr: 0.0050, train loss: 2.4714, val loss: 2.5083\n",
      "6347: lr: 0.0050, train loss: 2.4824, val loss: 2.4866\n",
      "6348: lr: 0.0050, train loss: 2.4876, val loss: 2.4954\n",
      "6349: lr: 0.0050, train loss: 2.4376, val loss: 2.5176\n",
      "6350: lr: 0.0050, train loss: 2.4581, val loss: 2.4465\n",
      "6351: lr: 0.0050, train loss: 2.4263, val loss: 2.5336\n",
      "6352: lr: 0.0050, train loss: 2.5180, val loss: 2.4681\n",
      "6353: lr: 0.0050, train loss: 2.4512, val loss: 2.4842\n",
      "6354: lr: 0.0050, train loss: 2.4359, val loss: 2.5008\n",
      "6355: lr: 0.0050, train loss: 2.4319, val loss: 2.4888\n",
      "6356: lr: 0.0050, train loss: 2.5027, val loss: 2.4758\n",
      "6357: lr: 0.0050, train loss: 2.4308, val loss: 2.5164\n",
      "6358: lr: 0.0050, train loss: 2.4525, val loss: 2.5157\n",
      "6359: lr: 0.0050, train loss: 2.4167, val loss: 2.5668\n",
      "6360: lr: 0.0050, train loss: 2.4796, val loss: 2.4729\n",
      "6361: lr: 0.0050, train loss: 2.4382, val loss: 2.4468\n",
      "6362: lr: 0.0050, train loss: 2.4541, val loss: 2.5467\n",
      "6363: lr: 0.0050, train loss: 2.4835, val loss: 2.4778\n",
      "6364: lr: 0.0050, train loss: 2.4532, val loss: 2.5188\n",
      "6365: lr: 0.0050, train loss: 2.4241, val loss: 2.4792\n",
      "6366: lr: 0.0050, train loss: 2.4780, val loss: 2.4856\n",
      "6367: lr: 0.0050, train loss: 2.4543, val loss: 2.4496\n",
      "6368: lr: 0.0050, train loss: 2.4484, val loss: 2.4285\n",
      "6369: lr: 0.0050, train loss: 2.4373, val loss: 2.4749\n",
      "6370: lr: 0.0050, train loss: 2.4486, val loss: 2.4833\n",
      "6371: lr: 0.0050, train loss: 2.4162, val loss: 2.4472\n",
      "6372: lr: 0.0050, train loss: 2.4936, val loss: 2.4966\n",
      "6373: lr: 0.0050, train loss: 2.4435, val loss: 2.5302\n",
      "6374: lr: 0.0050, train loss: 2.5203, val loss: 2.4674\n",
      "6375: lr: 0.0050, train loss: 2.4681, val loss: 2.5262\n",
      "6376: lr: 0.0050, train loss: 2.4528, val loss: 2.4996\n",
      "6377: lr: 0.0050, train loss: 2.4258, val loss: 2.4807\n",
      "6378: lr: 0.0050, train loss: 2.4751, val loss: 2.4593\n",
      "6379: lr: 0.0050, train loss: 2.4483, val loss: 2.5287\n",
      "6380: lr: 0.0050, train loss: 2.4949, val loss: 2.4779\n",
      "6381: lr: 0.0050, train loss: 2.4443, val loss: 2.5141\n",
      "6382: lr: 0.0050, train loss: 2.4252, val loss: 2.4664\n",
      "6383: lr: 0.0050, train loss: 2.4809, val loss: 2.5200\n",
      "6384: lr: 0.0050, train loss: 2.4801, val loss: 2.4856\n",
      "6385: lr: 0.0050, train loss: 2.4370, val loss: 2.5080\n",
      "6386: lr: 0.0050, train loss: 2.4670, val loss: 2.4774\n",
      "6387: lr: 0.0050, train loss: 2.4067, val loss: 2.4826\n",
      "6388: lr: 0.0050, train loss: 2.4827, val loss: 2.4697\n",
      "6389: lr: 0.0050, train loss: 2.4191, val loss: 2.4815\n",
      "6390: lr: 0.0050, train loss: 2.4348, val loss: 2.4859\n",
      "6391: lr: 0.0050, train loss: 2.4917, val loss: 2.5406\n",
      "6392: lr: 0.0050, train loss: 2.4656, val loss: 2.5348\n",
      "6393: lr: 0.0050, train loss: 2.4508, val loss: 2.5094\n",
      "6394: lr: 0.0050, train loss: 2.4400, val loss: 2.4884\n",
      "6395: lr: 0.0050, train loss: 2.4524, val loss: 2.4936\n",
      "6396: lr: 0.0050, train loss: 2.4074, val loss: 2.4453\n",
      "6397: lr: 0.0050, train loss: 2.4216, val loss: 2.5250\n",
      "6398: lr: 0.0050, train loss: 2.4833, val loss: 2.4623\n",
      "6399: lr: 0.0050, train loss: 2.5035, val loss: 2.5086\n",
      "6400: lr: 0.0050, train loss: 2.4410, val loss: 2.4563\n",
      "6401: lr: 0.0050, train loss: 2.4272, val loss: 2.4752\n",
      "6402: lr: 0.0050, train loss: 2.4555, val loss: 2.5131\n",
      "6403: lr: 0.0050, train loss: 2.4675, val loss: 2.5146\n",
      "6404: lr: 0.0050, train loss: 2.4960, val loss: 2.4574\n",
      "6405: lr: 0.0050, train loss: 2.4689, val loss: 2.4575\n",
      "6406: lr: 0.0050, train loss: 2.4489, val loss: 2.4839\n",
      "6407: lr: 0.0050, train loss: 2.4823, val loss: 2.4735\n",
      "6408: lr: 0.0050, train loss: 2.4663, val loss: 2.5013\n",
      "6409: lr: 0.0050, train loss: 2.4461, val loss: 2.4781\n",
      "6410: lr: 0.0050, train loss: 2.4846, val loss: 2.4707\n",
      "6411: lr: 0.0050, train loss: 2.4371, val loss: 2.5123\n",
      "6412: lr: 0.0050, train loss: 2.4755, val loss: 2.4935\n",
      "6413: lr: 0.0050, train loss: 2.4692, val loss: 2.5238\n",
      "6414: lr: 0.0050, train loss: 2.4914, val loss: 2.4750\n",
      "6415: lr: 0.0050, train loss: 2.4632, val loss: 2.4819\n",
      "6416: lr: 0.0050, train loss: 2.4572, val loss: 2.4897\n",
      "6417: lr: 0.0050, train loss: 2.4532, val loss: 2.4769\n",
      "6418: lr: 0.0050, train loss: 2.4371, val loss: 2.5312\n",
      "6419: lr: 0.0050, train loss: 2.4244, val loss: 2.4987\n",
      "6420: lr: 0.0050, train loss: 2.4623, val loss: 2.4855\n",
      "6421: lr: 0.0050, train loss: 2.4207, val loss: 2.5127\n",
      "6422: lr: 0.0050, train loss: 2.4518, val loss: 2.4832\n",
      "6423: lr: 0.0050, train loss: 2.4988, val loss: 2.5232\n",
      "6424: lr: 0.0050, train loss: 2.4677, val loss: 2.4451\n",
      "6425: lr: 0.0050, train loss: 2.4499, val loss: 2.4896\n",
      "6426: lr: 0.0050, train loss: 2.4263, val loss: 2.4576\n",
      "6427: lr: 0.0050, train loss: 2.4567, val loss: 2.4624\n",
      "6428: lr: 0.0050, train loss: 2.5005, val loss: 2.4729\n",
      "6429: lr: 0.0050, train loss: 2.4374, val loss: 2.4637\n",
      "6430: lr: 0.0050, train loss: 2.4782, val loss: 2.4672\n",
      "6431: lr: 0.0050, train loss: 2.4684, val loss: 2.4669\n",
      "6432: lr: 0.0050, train loss: 2.4553, val loss: 2.5240\n",
      "6433: lr: 0.0050, train loss: 2.4663, val loss: 2.4593\n",
      "6434: lr: 0.0050, train loss: 2.4118, val loss: 2.4635\n",
      "6435: lr: 0.0050, train loss: 2.4490, val loss: 2.4947\n",
      "6436: lr: 0.0050, train loss: 2.4726, val loss: 2.4675\n",
      "6437: lr: 0.0050, train loss: 2.4506, val loss: 2.5004\n",
      "6438: lr: 0.0050, train loss: 2.4356, val loss: 2.5042\n",
      "6439: lr: 0.0050, train loss: 2.4320, val loss: 2.5430\n",
      "6440: lr: 0.0050, train loss: 2.4702, val loss: 2.4770\n",
      "6441: lr: 0.0050, train loss: 2.4877, val loss: 2.5419\n",
      "6442: lr: 0.0050, train loss: 2.4389, val loss: 2.4528\n",
      "6443: lr: 0.0050, train loss: 2.4628, val loss: 2.4743\n",
      "6444: lr: 0.0050, train loss: 2.4472, val loss: 2.4197\n",
      "6445: lr: 0.0050, train loss: 2.4551, val loss: 2.5049\n",
      "6446: lr: 0.0050, train loss: 2.4760, val loss: 2.5281\n",
      "6447: lr: 0.0050, train loss: 2.4821, val loss: 2.5012\n",
      "6448: lr: 0.0050, train loss: 2.4731, val loss: 2.4998\n",
      "6449: lr: 0.0050, train loss: 2.4166, val loss: 2.5109\n",
      "6450: lr: 0.0050, train loss: 2.3900, val loss: 2.4688\n",
      "6451: lr: 0.0050, train loss: 2.4560, val loss: 2.5032\n",
      "6452: lr: 0.0050, train loss: 2.5000, val loss: 2.4901\n",
      "6453: lr: 0.0050, train loss: 2.4399, val loss: 2.5057\n",
      "6454: lr: 0.0050, train loss: 2.4563, val loss: 2.5409\n",
      "6455: lr: 0.0050, train loss: 2.4680, val loss: 2.4549\n",
      "6456: lr: 0.0050, train loss: 2.4621, val loss: 2.4658\n",
      "6457: lr: 0.0050, train loss: 2.4578, val loss: 2.4813\n",
      "6458: lr: 0.0050, train loss: 2.4606, val loss: 2.4439\n",
      "6459: lr: 0.0050, train loss: 2.4655, val loss: 2.4930\n",
      "6460: lr: 0.0050, train loss: 2.4376, val loss: 2.4991\n",
      "6461: lr: 0.0050, train loss: 2.4720, val loss: 2.4872\n",
      "6462: lr: 0.0050, train loss: 2.4937, val loss: 2.4741\n",
      "6463: lr: 0.0050, train loss: 2.4546, val loss: 2.4751\n",
      "6464: lr: 0.0050, train loss: 2.4504, val loss: 2.4554\n",
      "6465: lr: 0.0050, train loss: 2.4525, val loss: 2.5185\n",
      "6466: lr: 0.0050, train loss: 2.4165, val loss: 2.5158\n",
      "6467: lr: 0.0050, train loss: 2.4557, val loss: 2.4931\n",
      "6468: lr: 0.0050, train loss: 2.4092, val loss: 2.5069\n",
      "6469: lr: 0.0050, train loss: 2.5005, val loss: 2.5521\n",
      "6470: lr: 0.0050, train loss: 2.4457, val loss: 2.4768\n",
      "6471: lr: 0.0050, train loss: 2.4357, val loss: 2.5107\n",
      "6472: lr: 0.0050, train loss: 2.4785, val loss: 2.5244\n",
      "6473: lr: 0.0050, train loss: 2.4653, val loss: 2.4969\n",
      "6474: lr: 0.0050, train loss: 2.4334, val loss: 2.5205\n",
      "6475: lr: 0.0050, train loss: 2.4793, val loss: 2.4684\n",
      "6476: lr: 0.0050, train loss: 2.4508, val loss: 2.4824\n",
      "6477: lr: 0.0050, train loss: 2.4637, val loss: 2.4572\n",
      "6478: lr: 0.0050, train loss: 2.4772, val loss: 2.5132\n",
      "6479: lr: 0.0050, train loss: 2.4490, val loss: 2.4866\n",
      "6480: lr: 0.0050, train loss: 2.5012, val loss: 2.4475\n",
      "6481: lr: 0.0050, train loss: 2.4106, val loss: 2.4645\n",
      "6482: lr: 0.0050, train loss: 2.4569, val loss: 2.5149\n",
      "6483: lr: 0.0050, train loss: 2.4577, val loss: 2.5006\n",
      "6484: lr: 0.0050, train loss: 2.4230, val loss: 2.4977\n",
      "6485: lr: 0.0050, train loss: 2.4569, val loss: 2.5229\n",
      "6486: lr: 0.0050, train loss: 2.4592, val loss: 2.4891\n",
      "6487: lr: 0.0050, train loss: 2.4341, val loss: 2.4965\n",
      "6488: lr: 0.0050, train loss: 2.4486, val loss: 2.4940\n",
      "6489: lr: 0.0050, train loss: 2.4415, val loss: 2.4863\n",
      "6490: lr: 0.0050, train loss: 2.4257, val loss: 2.4928\n",
      "6491: lr: 0.0050, train loss: 2.4037, val loss: 2.4802\n",
      "6492: lr: 0.0050, train loss: 2.4469, val loss: 2.5170\n",
      "6493: lr: 0.0050, train loss: 2.4610, val loss: 2.4976\n",
      "6494: lr: 0.0050, train loss: 2.4593, val loss: 2.4980\n",
      "6495: lr: 0.0050, train loss: 2.4622, val loss: 2.4668\n",
      "6496: lr: 0.0050, train loss: 2.4636, val loss: 2.4937\n",
      "6497: lr: 0.0050, train loss: 2.4410, val loss: 2.4561\n",
      "6498: lr: 0.0050, train loss: 2.4104, val loss: 2.4914\n",
      "6499: lr: 0.0050, train loss: 2.4230, val loss: 2.5165\n",
      "6500: lr: 0.0050, train loss: 2.4673, val loss: 2.4947\n",
      "6501: lr: 0.0050, train loss: 2.4575, val loss: 2.4975\n",
      "6502: lr: 0.0050, train loss: 2.4146, val loss: 2.4921\n",
      "6503: lr: 0.0050, train loss: 2.4722, val loss: 2.5148\n",
      "6504: lr: 0.0050, train loss: 2.4503, val loss: 2.4660\n",
      "6505: lr: 0.0050, train loss: 2.4646, val loss: 2.4944\n",
      "6506: lr: 0.0050, train loss: 2.4547, val loss: 2.5429\n",
      "6507: lr: 0.0050, train loss: 2.4306, val loss: 2.5058\n",
      "6508: lr: 0.0050, train loss: 2.4208, val loss: 2.4776\n",
      "6509: lr: 0.0050, train loss: 2.4831, val loss: 2.4779\n",
      "6510: lr: 0.0050, train loss: 2.4725, val loss: 2.4891\n",
      "6511: lr: 0.0050, train loss: 2.4632, val loss: 2.5111\n",
      "6512: lr: 0.0050, train loss: 2.4403, val loss: 2.4819\n",
      "6513: lr: 0.0050, train loss: 2.4547, val loss: 2.5127\n",
      "6514: lr: 0.0050, train loss: 2.4848, val loss: 2.4458\n",
      "6515: lr: 0.0050, train loss: 2.4656, val loss: 2.4552\n",
      "6516: lr: 0.0050, train loss: 2.4330, val loss: 2.4881\n",
      "6517: lr: 0.0050, train loss: 2.4429, val loss: 2.4844\n",
      "6518: lr: 0.0050, train loss: 2.4545, val loss: 2.4499\n",
      "6519: lr: 0.0050, train loss: 2.4694, val loss: 2.4655\n",
      "6520: lr: 0.0050, train loss: 2.4813, val loss: 2.4549\n",
      "6521: lr: 0.0050, train loss: 2.4705, val loss: 2.4516\n",
      "6522: lr: 0.0050, train loss: 2.4428, val loss: 2.4356\n",
      "6523: lr: 0.0050, train loss: 2.4491, val loss: 2.4974\n",
      "6524: lr: 0.0050, train loss: 2.4683, val loss: 2.4645\n",
      "6525: lr: 0.0050, train loss: 2.4413, val loss: 2.4496\n",
      "6526: lr: 0.0050, train loss: 2.4298, val loss: 2.4875\n",
      "6527: lr: 0.0050, train loss: 2.4477, val loss: 2.4798\n",
      "6528: lr: 0.0050, train loss: 2.4180, val loss: 2.4576\n",
      "6529: lr: 0.0050, train loss: 2.4312, val loss: 2.4372\n",
      "6530: lr: 0.0050, train loss: 2.4587, val loss: 2.4843\n",
      "6531: lr: 0.0050, train loss: 2.4546, val loss: 2.4789\n",
      "6532: lr: 0.0050, train loss: 2.4041, val loss: 2.5648\n",
      "6533: lr: 0.0050, train loss: 2.4374, val loss: 2.4707\n",
      "6534: lr: 0.0050, train loss: 2.4629, val loss: 2.4934\n",
      "6535: lr: 0.0050, train loss: 2.5104, val loss: 2.4669\n",
      "6536: lr: 0.0050, train loss: 2.4931, val loss: 2.4491\n",
      "6537: lr: 0.0050, train loss: 2.4726, val loss: 2.4792\n",
      "6538: lr: 0.0050, train loss: 2.4730, val loss: 2.4220\n",
      "6539: lr: 0.0050, train loss: 2.4574, val loss: 2.4635\n",
      "6540: lr: 0.0050, train loss: 2.4241, val loss: 2.4884\n",
      "6541: lr: 0.0050, train loss: 2.4400, val loss: 2.5048\n",
      "6542: lr: 0.0050, train loss: 2.4864, val loss: 2.4929\n",
      "6543: lr: 0.0050, train loss: 2.4852, val loss: 2.4871\n",
      "6544: lr: 0.0050, train loss: 2.4402, val loss: 2.4354\n",
      "6545: lr: 0.0050, train loss: 2.4347, val loss: 2.5351\n",
      "6546: lr: 0.0050, train loss: 2.4605, val loss: 2.4804\n",
      "6547: lr: 0.0050, train loss: 2.4590, val loss: 2.4778\n",
      "6548: lr: 0.0050, train loss: 2.4292, val loss: 2.4873\n",
      "6549: lr: 0.0050, train loss: 2.4337, val loss: 2.5196\n",
      "6550: lr: 0.0050, train loss: 2.4275, val loss: 2.5158\n",
      "6551: lr: 0.0050, train loss: 2.5008, val loss: 2.4518\n",
      "6552: lr: 0.0050, train loss: 2.4621, val loss: 2.4906\n",
      "6553: lr: 0.0050, train loss: 2.4690, val loss: 2.4513\n",
      "6554: lr: 0.0050, train loss: 2.4484, val loss: 2.5339\n",
      "6555: lr: 0.0050, train loss: 2.4688, val loss: 2.4748\n",
      "6556: lr: 0.0050, train loss: 2.5000, val loss: 2.4935\n",
      "6557: lr: 0.0050, train loss: 2.4338, val loss: 2.4705\n",
      "6558: lr: 0.0050, train loss: 2.4671, val loss: 2.4766\n",
      "6559: lr: 0.0050, train loss: 2.4644, val loss: 2.5225\n",
      "6560: lr: 0.0050, train loss: 2.4350, val loss: 2.4577\n",
      "6561: lr: 0.0050, train loss: 2.4568, val loss: 2.4954\n",
      "6562: lr: 0.0050, train loss: 2.4580, val loss: 2.4952\n",
      "6563: lr: 0.0050, train loss: 2.4880, val loss: 2.4473\n",
      "6564: lr: 0.0050, train loss: 2.4210, val loss: 2.4753\n",
      "6565: lr: 0.0050, train loss: 2.4884, val loss: 2.4547\n",
      "6566: lr: 0.0050, train loss: 2.4526, val loss: 2.5006\n",
      "6567: lr: 0.0050, train loss: 2.4654, val loss: 2.5038\n",
      "6568: lr: 0.0050, train loss: 2.4901, val loss: 2.5415\n",
      "6569: lr: 0.0050, train loss: 2.4332, val loss: 2.4551\n",
      "6570: lr: 0.0050, train loss: 2.4994, val loss: 2.4539\n",
      "6571: lr: 0.0050, train loss: 2.4415, val loss: 2.4551\n",
      "6572: lr: 0.0050, train loss: 2.4599, val loss: 2.4567\n",
      "6573: lr: 0.0050, train loss: 2.4975, val loss: 2.5101\n",
      "6574: lr: 0.0050, train loss: 2.4420, val loss: 2.4320\n",
      "6575: lr: 0.0050, train loss: 2.4917, val loss: 2.4775\n",
      "6576: lr: 0.0050, train loss: 2.4185, val loss: 2.5336\n",
      "6577: lr: 0.0050, train loss: 2.4702, val loss: 2.4916\n",
      "6578: lr: 0.0050, train loss: 2.4670, val loss: 2.5021\n",
      "6579: lr: 0.0050, train loss: 2.4865, val loss: 2.4497\n",
      "6580: lr: 0.0050, train loss: 2.4449, val loss: 2.4979\n",
      "6581: lr: 0.0050, train loss: 2.4737, val loss: 2.5084\n",
      "6582: lr: 0.0050, train loss: 2.5092, val loss: 2.4497\n",
      "6583: lr: 0.0050, train loss: 2.4322, val loss: 2.4867\n",
      "6584: lr: 0.0050, train loss: 2.4561, val loss: 2.4697\n",
      "6585: lr: 0.0050, train loss: 2.4864, val loss: 2.4400\n",
      "6586: lr: 0.0050, train loss: 2.4059, val loss: 2.4304\n",
      "6587: lr: 0.0050, train loss: 2.4647, val loss: 2.4656\n",
      "6588: lr: 0.0050, train loss: 2.4238, val loss: 2.5183\n",
      "6589: lr: 0.0050, train loss: 2.4911, val loss: 2.4883\n",
      "6590: lr: 0.0050, train loss: 2.4240, val loss: 2.4666\n",
      "6591: lr: 0.0050, train loss: 2.4608, val loss: 2.4725\n",
      "6592: lr: 0.0050, train loss: 2.4411, val loss: 2.5008\n",
      "6593: lr: 0.0050, train loss: 2.4699, val loss: 2.4836\n",
      "6594: lr: 0.0050, train loss: 2.3996, val loss: 2.5071\n",
      "6595: lr: 0.0050, train loss: 2.4844, val loss: 2.4744\n",
      "6596: lr: 0.0050, train loss: 2.4361, val loss: 2.5088\n",
      "6597: lr: 0.0050, train loss: 2.4449, val loss: 2.4926\n",
      "6598: lr: 0.0050, train loss: 2.4967, val loss: 2.4786\n",
      "6599: lr: 0.0050, train loss: 2.4505, val loss: 2.4651\n",
      "6600: lr: 0.0050, train loss: 2.4733, val loss: 2.5205\n",
      "6601: lr: 0.0050, train loss: 2.4731, val loss: 2.4690\n",
      "6602: lr: 0.0050, train loss: 2.4190, val loss: 2.4728\n",
      "6603: lr: 0.0050, train loss: 2.4433, val loss: 2.4726\n",
      "6604: lr: 0.0050, train loss: 2.4736, val loss: 2.5130\n",
      "6605: lr: 0.0050, train loss: 2.4365, val loss: 2.4978\n",
      "6606: lr: 0.0050, train loss: 2.4814, val loss: 2.5141\n",
      "6607: lr: 0.0050, train loss: 2.4723, val loss: 2.5254\n",
      "6608: lr: 0.0050, train loss: 2.4455, val loss: 2.4649\n",
      "6609: lr: 0.0050, train loss: 2.4213, val loss: 2.4591\n",
      "6610: lr: 0.0050, train loss: 2.4768, val loss: 2.4789\n",
      "6611: lr: 0.0050, train loss: 2.4342, val loss: 2.4379\n",
      "6612: lr: 0.0050, train loss: 2.4742, val loss: 2.5166\n",
      "6613: lr: 0.0050, train loss: 2.4515, val loss: 2.4987\n",
      "6614: lr: 0.0050, train loss: 2.4390, val loss: 2.4530\n",
      "6615: lr: 0.0050, train loss: 2.4489, val loss: 2.4863\n",
      "6616: lr: 0.0050, train loss: 2.4797, val loss: 2.4746\n",
      "6617: lr: 0.0050, train loss: 2.4246, val loss: 2.4880\n",
      "6618: lr: 0.0050, train loss: 2.4741, val loss: 2.4785\n",
      "6619: lr: 0.0050, train loss: 2.4468, val loss: 2.4417\n",
      "6620: lr: 0.0050, train loss: 2.4519, val loss: 2.4499\n",
      "6621: lr: 0.0050, train loss: 2.4529, val loss: 2.4838\n",
      "6622: lr: 0.0050, train loss: 2.4475, val loss: 2.4955\n",
      "6623: lr: 0.0050, train loss: 2.4287, val loss: 2.4732\n",
      "6624: lr: 0.0050, train loss: 2.4083, val loss: 2.4782\n",
      "6625: lr: 0.0050, train loss: 2.4754, val loss: 2.4734\n",
      "6626: lr: 0.0050, train loss: 2.4385, val loss: 2.4778\n",
      "6627: lr: 0.0050, train loss: 2.4716, val loss: 2.5221\n",
      "6628: lr: 0.0050, train loss: 2.4959, val loss: 2.4945\n",
      "6629: lr: 0.0050, train loss: 2.4538, val loss: 2.4610\n",
      "6630: lr: 0.0050, train loss: 2.4505, val loss: 2.4746\n",
      "6631: lr: 0.0050, train loss: 2.4364, val loss: 2.4829\n",
      "6632: lr: 0.0050, train loss: 2.4459, val loss: 2.4576\n",
      "6633: lr: 0.0050, train loss: 2.4531, val loss: 2.4726\n",
      "6634: lr: 0.0050, train loss: 2.4644, val loss: 2.4358\n",
      "6635: lr: 0.0050, train loss: 2.4655, val loss: 2.4898\n",
      "6636: lr: 0.0050, train loss: 2.4632, val loss: 2.5416\n",
      "6637: lr: 0.0050, train loss: 2.4315, val loss: 2.4965\n",
      "6638: lr: 0.0050, train loss: 2.4308, val loss: 2.5226\n",
      "6639: lr: 0.0050, train loss: 2.4601, val loss: 2.4596\n",
      "6640: lr: 0.0050, train loss: 2.4152, val loss: 2.4982\n",
      "6641: lr: 0.0050, train loss: 2.4112, val loss: 2.5107\n",
      "6642: lr: 0.0050, train loss: 2.4955, val loss: 2.4856\n",
      "6643: lr: 0.0050, train loss: 2.4549, val loss: 2.5467\n",
      "6644: lr: 0.0050, train loss: 2.4905, val loss: 2.4787\n",
      "6645: lr: 0.0050, train loss: 2.4814, val loss: 2.4679\n",
      "6646: lr: 0.0050, train loss: 2.4805, val loss: 2.5379\n",
      "6647: lr: 0.0050, train loss: 2.4462, val loss: 2.5387\n",
      "6648: lr: 0.0050, train loss: 2.4668, val loss: 2.5196\n",
      "6649: lr: 0.0050, train loss: 2.4391, val loss: 2.4581\n",
      "6650: lr: 0.0050, train loss: 2.4441, val loss: 2.4817\n",
      "6651: lr: 0.0050, train loss: 2.4718, val loss: 2.4935\n",
      "6652: lr: 0.0050, train loss: 2.4354, val loss: 2.4826\n",
      "6653: lr: 0.0050, train loss: 2.4069, val loss: 2.4828\n",
      "6654: lr: 0.0050, train loss: 2.4840, val loss: 2.5272\n",
      "6655: lr: 0.0050, train loss: 2.4828, val loss: 2.4886\n",
      "6656: lr: 0.0050, train loss: 2.4652, val loss: 2.5571\n",
      "6657: lr: 0.0050, train loss: 2.4364, val loss: 2.4729\n",
      "6658: lr: 0.0050, train loss: 2.4605, val loss: 2.5089\n",
      "6659: lr: 0.0050, train loss: 2.4684, val loss: 2.4599\n",
      "6660: lr: 0.0050, train loss: 2.4560, val loss: 2.4691\n",
      "6661: lr: 0.0050, train loss: 2.3740, val loss: 2.4795\n",
      "6662: lr: 0.0050, train loss: 2.4583, val loss: 2.5042\n",
      "6663: lr: 0.0050, train loss: 2.4088, val loss: 2.5044\n",
      "6664: lr: 0.0050, train loss: 2.4312, val loss: 2.4896\n",
      "6665: lr: 0.0050, train loss: 2.3784, val loss: 2.4825\n",
      "6666: lr: 0.0050, train loss: 2.4551, val loss: 2.4625\n",
      "6667: lr: 0.0050, train loss: 2.4590, val loss: 2.4876\n",
      "6668: lr: 0.0050, train loss: 2.4390, val loss: 2.4594\n",
      "6669: lr: 0.0050, train loss: 2.4560, val loss: 2.5168\n",
      "6670: lr: 0.0050, train loss: 2.4849, val loss: 2.5120\n",
      "6671: lr: 0.0050, train loss: 2.4550, val loss: 2.4880\n",
      "6672: lr: 0.0050, train loss: 2.4359, val loss: 2.5094\n",
      "6673: lr: 0.0050, train loss: 2.4488, val loss: 2.5043\n",
      "6674: lr: 0.0050, train loss: 2.4495, val loss: 2.4760\n",
      "6675: lr: 0.0050, train loss: 2.4643, val loss: 2.4559\n",
      "6676: lr: 0.0050, train loss: 2.4578, val loss: 2.5044\n",
      "6677: lr: 0.0050, train loss: 2.4485, val loss: 2.4310\n",
      "6678: lr: 0.0050, train loss: 2.4224, val loss: 2.4908\n",
      "6679: lr: 0.0050, train loss: 2.4252, val loss: 2.4880\n",
      "6680: lr: 0.0050, train loss: 2.5165, val loss: 2.4847\n",
      "6681: lr: 0.0050, train loss: 2.4643, val loss: 2.4705\n",
      "6682: lr: 0.0050, train loss: 2.4329, val loss: 2.4693\n",
      "6683: lr: 0.0050, train loss: 2.4553, val loss: 2.5198\n",
      "6684: lr: 0.0050, train loss: 2.4538, val loss: 2.4961\n",
      "6685: lr: 0.0050, train loss: 2.4309, val loss: 2.4699\n",
      "6686: lr: 0.0050, train loss: 2.4249, val loss: 2.4703\n",
      "6687: lr: 0.0050, train loss: 2.4738, val loss: 2.5333\n",
      "6688: lr: 0.0050, train loss: 2.4235, val loss: 2.4596\n",
      "6689: lr: 0.0050, train loss: 2.4636, val loss: 2.5472\n",
      "6690: lr: 0.0050, train loss: 2.4784, val loss: 2.5139\n",
      "6691: lr: 0.0050, train loss: 2.4547, val loss: 2.4683\n",
      "6692: lr: 0.0050, train loss: 2.4875, val loss: 2.5031\n",
      "6693: lr: 0.0050, train loss: 2.4440, val loss: 2.5193\n",
      "6694: lr: 0.0050, train loss: 2.4461, val loss: 2.4778\n",
      "6695: lr: 0.0050, train loss: 2.4287, val loss: 2.5179\n",
      "6696: lr: 0.0050, train loss: 2.4648, val loss: 2.5412\n",
      "6697: lr: 0.0050, train loss: 2.4713, val loss: 2.5149\n",
      "6698: lr: 0.0050, train loss: 2.4557, val loss: 2.4775\n",
      "6699: lr: 0.0050, train loss: 2.4758, val loss: 2.4914\n",
      "6700: lr: 0.0050, train loss: 2.4989, val loss: 2.5161\n",
      "6701: lr: 0.0050, train loss: 2.4865, val loss: 2.4639\n",
      "6702: lr: 0.0050, train loss: 2.4103, val loss: 2.4803\n",
      "6703: lr: 0.0050, train loss: 2.4778, val loss: 2.5138\n",
      "6704: lr: 0.0050, train loss: 2.4443, val loss: 2.4511\n",
      "6705: lr: 0.0050, train loss: 2.4461, val loss: 2.4911\n",
      "6706: lr: 0.0050, train loss: 2.4674, val loss: 2.4899\n",
      "6707: lr: 0.0050, train loss: 2.4580, val loss: 2.4594\n",
      "6708: lr: 0.0050, train loss: 2.4454, val loss: 2.5127\n",
      "6709: lr: 0.0050, train loss: 2.4394, val loss: 2.4869\n",
      "6710: lr: 0.0050, train loss: 2.4983, val loss: 2.4950\n",
      "6711: lr: 0.0050, train loss: 2.4997, val loss: 2.4852\n",
      "6712: lr: 0.0050, train loss: 2.4767, val loss: 2.4351\n",
      "6713: lr: 0.0050, train loss: 2.4662, val loss: 2.4751\n",
      "6714: lr: 0.0050, train loss: 2.4251, val loss: 2.4974\n",
      "6715: lr: 0.0050, train loss: 2.4718, val loss: 2.5130\n",
      "6716: lr: 0.0050, train loss: 2.4203, val loss: 2.4584\n",
      "6717: lr: 0.0050, train loss: 2.4182, val loss: 2.4422\n",
      "6718: lr: 0.0050, train loss: 2.4568, val loss: 2.5150\n",
      "6719: lr: 0.0050, train loss: 2.4985, val loss: 2.4808\n",
      "6720: lr: 0.0050, train loss: 2.4638, val loss: 2.4744\n",
      "6721: lr: 0.0050, train loss: 2.4365, val loss: 2.5299\n",
      "6722: lr: 0.0050, train loss: 2.4372, val loss: 2.4953\n",
      "6723: lr: 0.0050, train loss: 2.4576, val loss: 2.4967\n",
      "6724: lr: 0.0050, train loss: 2.4751, val loss: 2.5091\n",
      "6725: lr: 0.0050, train loss: 2.4582, val loss: 2.4702\n",
      "6726: lr: 0.0050, train loss: 2.4881, val loss: 2.4956\n",
      "6727: lr: 0.0050, train loss: 2.4597, val loss: 2.5387\n",
      "6728: lr: 0.0050, train loss: 2.4907, val loss: 2.4932\n",
      "6729: lr: 0.0050, train loss: 2.3997, val loss: 2.5356\n",
      "6730: lr: 0.0050, train loss: 2.4228, val loss: 2.4799\n",
      "6731: lr: 0.0050, train loss: 2.4567, val loss: 2.5117\n",
      "6732: lr: 0.0050, train loss: 2.4266, val loss: 2.5220\n",
      "6733: lr: 0.0050, train loss: 2.4419, val loss: 2.4927\n",
      "6734: lr: 0.0050, train loss: 2.4704, val loss: 2.4743\n",
      "6735: lr: 0.0050, train loss: 2.4315, val loss: 2.4869\n",
      "6736: lr: 0.0050, train loss: 2.4190, val loss: 2.5107\n",
      "6737: lr: 0.0050, train loss: 2.4617, val loss: 2.4787\n",
      "6738: lr: 0.0050, train loss: 2.3999, val loss: 2.5053\n",
      "6739: lr: 0.0050, train loss: 2.4514, val loss: 2.4084\n",
      "6740: lr: 0.0050, train loss: 2.4300, val loss: 2.5010\n",
      "6741: lr: 0.0050, train loss: 2.4963, val loss: 2.4990\n",
      "6742: lr: 0.0050, train loss: 2.4832, val loss: 2.4816\n",
      "6743: lr: 0.0050, train loss: 2.4960, val loss: 2.5110\n",
      "6744: lr: 0.0050, train loss: 2.4730, val loss: 2.4891\n",
      "6745: lr: 0.0050, train loss: 2.4841, val loss: 2.4921\n",
      "6746: lr: 0.0050, train loss: 2.4567, val loss: 2.5020\n",
      "6747: lr: 0.0050, train loss: 2.4578, val loss: 2.5280\n",
      "6748: lr: 0.0050, train loss: 2.4755, val loss: 2.4687\n",
      "6749: lr: 0.0050, train loss: 2.4247, val loss: 2.4760\n",
      "6750: lr: 0.0050, train loss: 2.5305, val loss: 2.4668\n",
      "6751: lr: 0.0050, train loss: 2.4610, val loss: 2.4981\n",
      "6752: lr: 0.0050, train loss: 2.5320, val loss: 2.5277\n",
      "6753: lr: 0.0050, train loss: 2.4605, val loss: 2.5148\n",
      "6754: lr: 0.0050, train loss: 2.4687, val loss: 2.4954\n",
      "6755: lr: 0.0050, train loss: 2.4584, val loss: 2.4836\n",
      "6756: lr: 0.0050, train loss: 2.4519, val loss: 2.5341\n",
      "6757: lr: 0.0050, train loss: 2.4369, val loss: 2.4604\n",
      "6758: lr: 0.0050, train loss: 2.4434, val loss: 2.5442\n",
      "6759: lr: 0.0050, train loss: 2.4463, val loss: 2.4824\n",
      "6760: lr: 0.0050, train loss: 2.4425, val loss: 2.5239\n",
      "6761: lr: 0.0050, train loss: 2.5069, val loss: 2.5093\n",
      "6762: lr: 0.0050, train loss: 2.4444, val loss: 2.4856\n",
      "6763: lr: 0.0050, train loss: 2.4963, val loss: 2.4917\n",
      "6764: lr: 0.0050, train loss: 2.4877, val loss: 2.5031\n",
      "6765: lr: 0.0050, train loss: 2.4420, val loss: 2.5451\n",
      "6766: lr: 0.0050, train loss: 2.4170, val loss: 2.5186\n",
      "6767: lr: 0.0050, train loss: 2.4390, val loss: 2.5199\n",
      "6768: lr: 0.0050, train loss: 2.4337, val loss: 2.4697\n",
      "6769: lr: 0.0050, train loss: 2.4360, val loss: 2.4853\n",
      "6770: lr: 0.0050, train loss: 2.4433, val loss: 2.4060\n",
      "6771: lr: 0.0050, train loss: 2.4060, val loss: 2.4658\n",
      "6772: lr: 0.0050, train loss: 2.4779, val loss: 2.4790\n",
      "6773: lr: 0.0050, train loss: 2.4020, val loss: 2.4484\n",
      "6774: lr: 0.0050, train loss: 2.4625, val loss: 2.4724\n",
      "6775: lr: 0.0050, train loss: 2.4463, val loss: 2.4905\n",
      "6776: lr: 0.0050, train loss: 2.4606, val loss: 2.5020\n",
      "6777: lr: 0.0050, train loss: 2.4311, val loss: 2.5455\n",
      "6778: lr: 0.0050, train loss: 2.4394, val loss: 2.4841\n",
      "6779: lr: 0.0050, train loss: 2.4362, val loss: 2.5003\n",
      "6780: lr: 0.0050, train loss: 2.4524, val loss: 2.5427\n",
      "6781: lr: 0.0050, train loss: 2.4422, val loss: 2.4466\n",
      "6782: lr: 0.0050, train loss: 2.4184, val loss: 2.4852\n",
      "6783: lr: 0.0050, train loss: 2.4381, val loss: 2.4863\n",
      "6784: lr: 0.0050, train loss: 2.4478, val loss: 2.5333\n",
      "6785: lr: 0.0050, train loss: 2.4284, val loss: 2.4864\n",
      "6786: lr: 0.0050, train loss: 2.4462, val loss: 2.4507\n",
      "6787: lr: 0.0050, train loss: 2.4719, val loss: 2.5019\n",
      "6788: lr: 0.0050, train loss: 2.4559, val loss: 2.4679\n",
      "6789: lr: 0.0050, train loss: 2.4965, val loss: 2.4798\n",
      "6790: lr: 0.0050, train loss: 2.4713, val loss: 2.4869\n",
      "6791: lr: 0.0050, train loss: 2.4529, val loss: 2.4768\n",
      "6792: lr: 0.0050, train loss: 2.4382, val loss: 2.5283\n",
      "6793: lr: 0.0050, train loss: 2.4002, val loss: 2.4151\n",
      "6794: lr: 0.0050, train loss: 2.4564, val loss: 2.4662\n",
      "6795: lr: 0.0050, train loss: 2.5039, val loss: 2.4729\n",
      "6796: lr: 0.0050, train loss: 2.5119, val loss: 2.5024\n",
      "6797: lr: 0.0050, train loss: 2.4221, val loss: 2.5122\n",
      "6798: lr: 0.0050, train loss: 2.4126, val loss: 2.5114\n",
      "6799: lr: 0.0050, train loss: 2.5120, val loss: 2.5270\n",
      "6800: lr: 0.0050, train loss: 2.4632, val loss: 2.4876\n",
      "6801: lr: 0.0050, train loss: 2.4519, val loss: 2.4619\n",
      "6802: lr: 0.0050, train loss: 2.4547, val loss: 2.4727\n",
      "6803: lr: 0.0050, train loss: 2.4675, val loss: 2.4614\n",
      "6804: lr: 0.0050, train loss: 2.4399, val loss: 2.4794\n",
      "6805: lr: 0.0050, train loss: 2.4679, val loss: 2.5168\n",
      "6806: lr: 0.0050, train loss: 2.4045, val loss: 2.4868\n",
      "6807: lr: 0.0050, train loss: 2.4579, val loss: 2.4923\n",
      "6808: lr: 0.0050, train loss: 2.4703, val loss: 2.4550\n",
      "6809: lr: 0.0050, train loss: 2.4092, val loss: 2.5131\n",
      "6810: lr: 0.0050, train loss: 2.4412, val loss: 2.5690\n",
      "6811: lr: 0.0050, train loss: 2.4375, val loss: 2.5437\n",
      "6812: lr: 0.0050, train loss: 2.4653, val loss: 2.4885\n",
      "6813: lr: 0.0050, train loss: 2.4803, val loss: 2.5211\n",
      "6814: lr: 0.0050, train loss: 2.4689, val loss: 2.5110\n",
      "6815: lr: 0.0050, train loss: 2.4623, val loss: 2.5631\n",
      "6816: lr: 0.0050, train loss: 2.4416, val loss: 2.5009\n",
      "6817: lr: 0.0050, train loss: 2.4361, val loss: 2.4350\n",
      "6818: lr: 0.0050, train loss: 2.4427, val loss: 2.4571\n",
      "6819: lr: 0.0050, train loss: 2.4481, val loss: 2.4711\n",
      "6820: lr: 0.0050, train loss: 2.4575, val loss: 2.5596\n",
      "6821: lr: 0.0050, train loss: 2.4689, val loss: 2.4429\n",
      "6822: lr: 0.0050, train loss: 2.5045, val loss: 2.5002\n",
      "6823: lr: 0.0050, train loss: 2.4369, val loss: 2.4509\n",
      "6824: lr: 0.0050, train loss: 2.4876, val loss: 2.4705\n",
      "6825: lr: 0.0050, train loss: 2.4143, val loss: 2.5456\n",
      "6826: lr: 0.0050, train loss: 2.4235, val loss: 2.4958\n",
      "6827: lr: 0.0050, train loss: 2.3953, val loss: 2.5111\n",
      "6828: lr: 0.0050, train loss: 2.4221, val loss: 2.5318\n",
      "6829: lr: 0.0050, train loss: 2.4548, val loss: 2.4950\n",
      "6830: lr: 0.0050, train loss: 2.4381, val loss: 2.4532\n",
      "6831: lr: 0.0050, train loss: 2.4054, val loss: 2.4938\n",
      "6832: lr: 0.0050, train loss: 2.4278, val loss: 2.4656\n",
      "6833: lr: 0.0050, train loss: 2.4331, val loss: 2.4559\n",
      "6834: lr: 0.0050, train loss: 2.4466, val loss: 2.5347\n",
      "6835: lr: 0.0050, train loss: 2.4361, val loss: 2.4435\n",
      "6836: lr: 0.0050, train loss: 2.4052, val loss: 2.4931\n",
      "6837: lr: 0.0050, train loss: 2.4697, val loss: 2.4519\n",
      "6838: lr: 0.0050, train loss: 2.4919, val loss: 2.4774\n",
      "6839: lr: 0.0050, train loss: 2.4510, val loss: 2.5036\n",
      "6840: lr: 0.0050, train loss: 2.4850, val loss: 2.4641\n",
      "6841: lr: 0.0050, train loss: 2.4329, val loss: 2.4898\n",
      "6842: lr: 0.0050, train loss: 2.4409, val loss: 2.4751\n",
      "6843: lr: 0.0050, train loss: 2.4302, val loss: 2.4786\n",
      "6844: lr: 0.0050, train loss: 2.4969, val loss: 2.4710\n",
      "6845: lr: 0.0050, train loss: 2.4153, val loss: 2.5012\n",
      "6846: lr: 0.0050, train loss: 2.5450, val loss: 2.4437\n",
      "6847: lr: 0.0050, train loss: 2.5114, val loss: 2.4959\n",
      "6848: lr: 0.0050, train loss: 2.4717, val loss: 2.5151\n",
      "6849: lr: 0.0050, train loss: 2.4303, val loss: 2.4784\n",
      "6850: lr: 0.0050, train loss: 2.4102, val loss: 2.4668\n",
      "6851: lr: 0.0050, train loss: 2.4838, val loss: 2.4916\n",
      "6852: lr: 0.0050, train loss: 2.4791, val loss: 2.5395\n",
      "6853: lr: 0.0050, train loss: 2.4711, val loss: 2.5104\n",
      "6854: lr: 0.0050, train loss: 2.4629, val loss: 2.4583\n",
      "6855: lr: 0.0050, train loss: 2.4547, val loss: 2.4737\n",
      "6856: lr: 0.0050, train loss: 2.4570, val loss: 2.5284\n",
      "6857: lr: 0.0050, train loss: 2.4448, val loss: 2.4862\n",
      "6858: lr: 0.0050, train loss: 2.4472, val loss: 2.5369\n",
      "6859: lr: 0.0050, train loss: 2.4524, val loss: 2.5015\n",
      "6860: lr: 0.0050, train loss: 2.4824, val loss: 2.4495\n",
      "6861: lr: 0.0050, train loss: 2.4790, val loss: 2.4861\n",
      "6862: lr: 0.0050, train loss: 2.4044, val loss: 2.5126\n",
      "6863: lr: 0.0050, train loss: 2.4819, val loss: 2.4706\n",
      "6864: lr: 0.0050, train loss: 2.4938, val loss: 2.4903\n",
      "6865: lr: 0.0050, train loss: 2.4888, val loss: 2.4908\n",
      "6866: lr: 0.0050, train loss: 2.4578, val loss: 2.4891\n",
      "6867: lr: 0.0050, train loss: 2.4703, val loss: 2.4562\n",
      "6868: lr: 0.0050, train loss: 2.4465, val loss: 2.4829\n",
      "6869: lr: 0.0050, train loss: 2.4908, val loss: 2.4608\n",
      "6870: lr: 0.0050, train loss: 2.4639, val loss: 2.4604\n",
      "6871: lr: 0.0050, train loss: 2.3989, val loss: 2.5010\n",
      "6872: lr: 0.0050, train loss: 2.4353, val loss: 2.4893\n",
      "6873: lr: 0.0050, train loss: 2.4589, val loss: 2.5259\n",
      "6874: lr: 0.0050, train loss: 2.3958, val loss: 2.4758\n",
      "6875: lr: 0.0050, train loss: 2.4933, val loss: 2.4313\n",
      "6876: lr: 0.0050, train loss: 2.4553, val loss: 2.4818\n",
      "6877: lr: 0.0050, train loss: 2.4454, val loss: 2.4472\n",
      "6878: lr: 0.0050, train loss: 2.4370, val loss: 2.5203\n",
      "6879: lr: 0.0050, train loss: 2.4638, val loss: 2.5096\n",
      "6880: lr: 0.0050, train loss: 2.4417, val loss: 2.5262\n",
      "6881: lr: 0.0050, train loss: 2.4682, val loss: 2.4572\n",
      "6882: lr: 0.0050, train loss: 2.4412, val loss: 2.4697\n",
      "6883: lr: 0.0050, train loss: 2.4460, val loss: 2.4832\n",
      "6884: lr: 0.0050, train loss: 2.4707, val loss: 2.4984\n",
      "6885: lr: 0.0050, train loss: 2.4062, val loss: 2.4518\n",
      "6886: lr: 0.0050, train loss: 2.4433, val loss: 2.5177\n",
      "6887: lr: 0.0050, train loss: 2.3998, val loss: 2.4535\n",
      "6888: lr: 0.0050, train loss: 2.4654, val loss: 2.4798\n",
      "6889: lr: 0.0050, train loss: 2.4488, val loss: 2.5062\n",
      "6890: lr: 0.0050, train loss: 2.4228, val loss: 2.5154\n",
      "6891: lr: 0.0050, train loss: 2.4770, val loss: 2.4531\n",
      "6892: lr: 0.0050, train loss: 2.4573, val loss: 2.4693\n",
      "6893: lr: 0.0050, train loss: 2.4784, val loss: 2.4722\n",
      "6894: lr: 0.0050, train loss: 2.4536, val loss: 2.4965\n",
      "6895: lr: 0.0050, train loss: 2.4609, val loss: 2.4988\n",
      "6896: lr: 0.0050, train loss: 2.4616, val loss: 2.4571\n",
      "6897: lr: 0.0050, train loss: 2.4415, val loss: 2.4573\n",
      "6898: lr: 0.0050, train loss: 2.4454, val loss: 2.5081\n",
      "6899: lr: 0.0050, train loss: 2.4786, val loss: 2.5012\n",
      "6900: lr: 0.0050, train loss: 2.4900, val loss: 2.4978\n",
      "6901: lr: 0.0050, train loss: 2.4380, val loss: 2.4724\n",
      "6902: lr: 0.0050, train loss: 2.4658, val loss: 2.4590\n",
      "6903: lr: 0.0050, train loss: 2.4204, val loss: 2.4897\n",
      "6904: lr: 0.0050, train loss: 2.4403, val loss: 2.4674\n",
      "6905: lr: 0.0050, train loss: 2.4891, val loss: 2.4773\n",
      "6906: lr: 0.0050, train loss: 2.4716, val loss: 2.5210\n",
      "6907: lr: 0.0050, train loss: 2.4259, val loss: 2.4676\n",
      "6908: lr: 0.0050, train loss: 2.4492, val loss: 2.4944\n",
      "6909: lr: 0.0050, train loss: 2.4462, val loss: 2.5138\n",
      "6910: lr: 0.0050, train loss: 2.4944, val loss: 2.4716\n",
      "6911: lr: 0.0050, train loss: 2.4175, val loss: 2.4604\n",
      "6912: lr: 0.0050, train loss: 2.4450, val loss: 2.4931\n",
      "6913: lr: 0.0050, train loss: 2.4190, val loss: 2.4482\n",
      "6914: lr: 0.0050, train loss: 2.4337, val loss: 2.5626\n",
      "6915: lr: 0.0050, train loss: 2.4062, val loss: 2.4630\n",
      "6916: lr: 0.0050, train loss: 2.4444, val loss: 2.5108\n",
      "6917: lr: 0.0050, train loss: 2.4158, val loss: 2.4529\n",
      "6918: lr: 0.0050, train loss: 2.4931, val loss: 2.4671\n",
      "6919: lr: 0.0050, train loss: 2.4785, val loss: 2.4568\n",
      "6920: lr: 0.0050, train loss: 2.4305, val loss: 2.4631\n",
      "6921: lr: 0.0050, train loss: 2.4740, val loss: 2.4422\n",
      "6922: lr: 0.0050, train loss: 2.4445, val loss: 2.4255\n",
      "6923: lr: 0.0050, train loss: 2.4283, val loss: 2.4793\n",
      "6924: lr: 0.0050, train loss: 2.4952, val loss: 2.4785\n",
      "6925: lr: 0.0050, train loss: 2.4753, val loss: 2.5146\n",
      "6926: lr: 0.0050, train loss: 2.4794, val loss: 2.4515\n",
      "6927: lr: 0.0050, train loss: 2.4332, val loss: 2.4672\n",
      "6928: lr: 0.0050, train loss: 2.4561, val loss: 2.5313\n",
      "6929: lr: 0.0050, train loss: 2.4667, val loss: 2.4625\n",
      "6930: lr: 0.0050, train loss: 2.4522, val loss: 2.5403\n",
      "6931: lr: 0.0050, train loss: 2.4459, val loss: 2.4998\n",
      "6932: lr: 0.0050, train loss: 2.4836, val loss: 2.4790\n",
      "6933: lr: 0.0050, train loss: 2.4247, val loss: 2.4737\n",
      "6934: lr: 0.0050, train loss: 2.4569, val loss: 2.4744\n",
      "6935: lr: 0.0050, train loss: 2.4350, val loss: 2.4786\n",
      "6936: lr: 0.0050, train loss: 2.4142, val loss: 2.4656\n",
      "6937: lr: 0.0050, train loss: 2.4195, val loss: 2.5097\n",
      "6938: lr: 0.0050, train loss: 2.4709, val loss: 2.5182\n",
      "6939: lr: 0.0050, train loss: 2.4626, val loss: 2.5275\n",
      "6940: lr: 0.0050, train loss: 2.4421, val loss: 2.4955\n",
      "6941: lr: 0.0050, train loss: 2.4613, val loss: 2.5088\n",
      "6942: lr: 0.0050, train loss: 2.4420, val loss: 2.4624\n",
      "6943: lr: 0.0050, train loss: 2.4473, val loss: 2.5296\n",
      "6944: lr: 0.0050, train loss: 2.5052, val loss: 2.5173\n",
      "6945: lr: 0.0050, train loss: 2.4827, val loss: 2.5011\n",
      "6946: lr: 0.0050, train loss: 2.4300, val loss: 2.4739\n",
      "6947: lr: 0.0050, train loss: 2.5006, val loss: 2.5149\n",
      "6948: lr: 0.0050, train loss: 2.4970, val loss: 2.4965\n",
      "6949: lr: 0.0050, train loss: 2.4251, val loss: 2.4761\n",
      "6950: lr: 0.0050, train loss: 2.4341, val loss: 2.4983\n",
      "6951: lr: 0.0050, train loss: 2.4394, val loss: 2.4959\n",
      "6952: lr: 0.0050, train loss: 2.4408, val loss: 2.4905\n",
      "6953: lr: 0.0050, train loss: 2.4896, val loss: 2.4940\n",
      "6954: lr: 0.0050, train loss: 2.4322, val loss: 2.4673\n",
      "6955: lr: 0.0050, train loss: 2.4437, val loss: 2.5028\n",
      "6956: lr: 0.0050, train loss: 2.4040, val loss: 2.4637\n",
      "6957: lr: 0.0050, train loss: 2.4625, val loss: 2.5131\n",
      "6958: lr: 0.0050, train loss: 2.4624, val loss: 2.4726\n",
      "6959: lr: 0.0050, train loss: 2.5154, val loss: 2.4892\n",
      "6960: lr: 0.0050, train loss: 2.4730, val loss: 2.4693\n",
      "6961: lr: 0.0050, train loss: 2.4195, val loss: 2.4978\n",
      "6962: lr: 0.0050, train loss: 2.3923, val loss: 2.5101\n",
      "6963: lr: 0.0050, train loss: 2.4360, val loss: 2.4529\n",
      "6964: lr: 0.0050, train loss: 2.4082, val loss: 2.4719\n",
      "6965: lr: 0.0050, train loss: 2.4491, val loss: 2.5089\n",
      "6966: lr: 0.0050, train loss: 2.4811, val loss: 2.5137\n",
      "6967: lr: 0.0050, train loss: 2.4675, val loss: 2.4888\n",
      "6968: lr: 0.0050, train loss: 2.4723, val loss: 2.4813\n",
      "6969: lr: 0.0050, train loss: 2.4920, val loss: 2.5389\n",
      "6970: lr: 0.0050, train loss: 2.5381, val loss: 2.4942\n",
      "6971: lr: 0.0050, train loss: 2.4060, val loss: 2.4754\n",
      "6972: lr: 0.0050, train loss: 2.4694, val loss: 2.4996\n",
      "6973: lr: 0.0050, train loss: 2.4343, val loss: 2.5273\n",
      "6974: lr: 0.0050, train loss: 2.4845, val loss: 2.4765\n",
      "6975: lr: 0.0050, train loss: 2.4499, val loss: 2.4921\n",
      "6976: lr: 0.0050, train loss: 2.4795, val loss: 2.5229\n",
      "6977: lr: 0.0050, train loss: 2.4300, val loss: 2.5265\n",
      "6978: lr: 0.0050, train loss: 2.4260, val loss: 2.4778\n",
      "6979: lr: 0.0050, train loss: 2.4748, val loss: 2.4773\n",
      "6980: lr: 0.0050, train loss: 2.4647, val loss: 2.4615\n",
      "6981: lr: 0.0050, train loss: 2.4791, val loss: 2.4644\n",
      "6982: lr: 0.0050, train loss: 2.4354, val loss: 2.4990\n",
      "6983: lr: 0.0050, train loss: 2.3943, val loss: 2.4648\n",
      "6984: lr: 0.0050, train loss: 2.4757, val loss: 2.5000\n",
      "6985: lr: 0.0050, train loss: 2.4905, val loss: 2.5038\n",
      "6986: lr: 0.0050, train loss: 2.4517, val loss: 2.4740\n",
      "6987: lr: 0.0050, train loss: 2.4779, val loss: 2.4962\n",
      "6988: lr: 0.0050, train loss: 2.4367, val loss: 2.4695\n",
      "6989: lr: 0.0050, train loss: 2.4783, val loss: 2.4404\n",
      "6990: lr: 0.0050, train loss: 2.4745, val loss: 2.4867\n",
      "6991: lr: 0.0050, train loss: 2.4617, val loss: 2.4505\n",
      "6992: lr: 0.0050, train loss: 2.4716, val loss: 2.4275\n",
      "6993: lr: 0.0050, train loss: 2.4746, val loss: 2.4880\n",
      "6994: lr: 0.0050, train loss: 2.4665, val loss: 2.4870\n",
      "6995: lr: 0.0050, train loss: 2.4610, val loss: 2.5278\n",
      "6996: lr: 0.0050, train loss: 2.4261, val loss: 2.5262\n",
      "6997: lr: 0.0050, train loss: 2.4672, val loss: 2.5210\n",
      "6998: lr: 0.0050, train loss: 2.4349, val loss: 2.4609\n",
      "6999: lr: 0.0050, train loss: 2.4811, val loss: 2.4444\n",
      "7000: lr: 0.0050, train loss: 2.4132, val loss: 2.4786\n",
      "7001: lr: 0.0050, train loss: 2.4729, val loss: 2.4737\n",
      "7002: lr: 0.0050, train loss: 2.4840, val loss: 2.4855\n",
      "7003: lr: 0.0050, train loss: 2.4882, val loss: 2.4489\n",
      "7004: lr: 0.0050, train loss: 2.4617, val loss: 2.5215\n",
      "7005: lr: 0.0050, train loss: 2.4807, val loss: 2.5561\n",
      "7006: lr: 0.0050, train loss: 2.4837, val loss: 2.4837\n",
      "7007: lr: 0.0050, train loss: 2.4783, val loss: 2.4837\n",
      "7008: lr: 0.0050, train loss: 2.4992, val loss: 2.4900\n",
      "7009: lr: 0.0050, train loss: 2.4955, val loss: 2.5289\n",
      "7010: lr: 0.0050, train loss: 2.4779, val loss: 2.5064\n",
      "7011: lr: 0.0050, train loss: 2.4375, val loss: 2.4967\n",
      "7012: lr: 0.0050, train loss: 2.4610, val loss: 2.4472\n",
      "7013: lr: 0.0050, train loss: 2.3947, val loss: 2.4756\n",
      "7014: lr: 0.0050, train loss: 2.4644, val loss: 2.4790\n",
      "7015: lr: 0.0050, train loss: 2.4633, val loss: 2.4845\n",
      "7016: lr: 0.0050, train loss: 2.4207, val loss: 2.4478\n",
      "7017: lr: 0.0050, train loss: 2.4329, val loss: 2.4647\n",
      "7018: lr: 0.0050, train loss: 2.4859, val loss: 2.5503\n",
      "7019: lr: 0.0050, train loss: 2.4579, val loss: 2.4901\n",
      "7020: lr: 0.0050, train loss: 2.4391, val loss: 2.4997\n",
      "7021: lr: 0.0050, train loss: 2.4402, val loss: 2.4669\n",
      "7022: lr: 0.0050, train loss: 2.4674, val loss: 2.5121\n",
      "7023: lr: 0.0050, train loss: 2.4545, val loss: 2.4988\n",
      "7024: lr: 0.0050, train loss: 2.4397, val loss: 2.5275\n",
      "7025: lr: 0.0050, train loss: 2.4479, val loss: 2.4884\n",
      "7026: lr: 0.0050, train loss: 2.4379, val loss: 2.4824\n",
      "7027: lr: 0.0050, train loss: 2.3992, val loss: 2.5018\n",
      "7028: lr: 0.0050, train loss: 2.4036, val loss: 2.4445\n",
      "7029: lr: 0.0050, train loss: 2.4979, val loss: 2.4799\n",
      "7030: lr: 0.0050, train loss: 2.4601, val loss: 2.5366\n",
      "7031: lr: 0.0050, train loss: 2.4674, val loss: 2.4819\n",
      "7032: lr: 0.0050, train loss: 2.4478, val loss: 2.4726\n",
      "7033: lr: 0.0050, train loss: 2.4712, val loss: 2.4945\n",
      "7034: lr: 0.0050, train loss: 2.5240, val loss: 2.5073\n",
      "7035: lr: 0.0050, train loss: 2.4276, val loss: 2.4782\n",
      "7036: lr: 0.0050, train loss: 2.4730, val loss: 2.4670\n",
      "7037: lr: 0.0050, train loss: 2.4511, val loss: 2.4455\n",
      "7038: lr: 0.0050, train loss: 2.4579, val loss: 2.4739\n",
      "7039: lr: 0.0050, train loss: 2.4819, val loss: 2.5260\n",
      "7040: lr: 0.0050, train loss: 2.4373, val loss: 2.4895\n",
      "7041: lr: 0.0050, train loss: 2.4698, val loss: 2.4046\n",
      "7042: lr: 0.0050, train loss: 2.4848, val loss: 2.4652\n",
      "7043: lr: 0.0050, train loss: 2.4729, val loss: 2.4426\n",
      "7044: lr: 0.0050, train loss: 2.4635, val loss: 2.5064\n",
      "7045: lr: 0.0050, train loss: 2.4760, val loss: 2.5231\n",
      "7046: lr: 0.0050, train loss: 2.4582, val loss: 2.4415\n",
      "7047: lr: 0.0050, train loss: 2.4704, val loss: 2.4629\n",
      "7048: lr: 0.0050, train loss: 2.4701, val loss: 2.4561\n",
      "7049: lr: 0.0050, train loss: 2.5209, val loss: 2.4939\n",
      "7050: lr: 0.0050, train loss: 2.4472, val loss: 2.4804\n",
      "7051: lr: 0.0050, train loss: 2.4709, val loss: 2.4565\n",
      "7052: lr: 0.0050, train loss: 2.4827, val loss: 2.4818\n",
      "7053: lr: 0.0050, train loss: 2.4393, val loss: 2.4846\n",
      "7054: lr: 0.0050, train loss: 2.5046, val loss: 2.5022\n",
      "7055: lr: 0.0050, train loss: 2.4751, val loss: 2.4871\n",
      "7056: lr: 0.0050, train loss: 2.4746, val loss: 2.4919\n",
      "7057: lr: 0.0050, train loss: 2.4476, val loss: 2.5009\n",
      "7058: lr: 0.0050, train loss: 2.4318, val loss: 2.4910\n",
      "7059: lr: 0.0050, train loss: 2.4651, val loss: 2.5074\n",
      "7060: lr: 0.0050, train loss: 2.5008, val loss: 2.5097\n",
      "7061: lr: 0.0050, train loss: 2.4643, val loss: 2.5287\n",
      "7062: lr: 0.0050, train loss: 2.4116, val loss: 2.5298\n",
      "7063: lr: 0.0050, train loss: 2.4468, val loss: 2.4690\n",
      "7064: lr: 0.0050, train loss: 2.4375, val loss: 2.4981\n",
      "7065: lr: 0.0050, train loss: 2.4322, val loss: 2.4783\n",
      "7066: lr: 0.0050, train loss: 2.4410, val loss: 2.4894\n",
      "7067: lr: 0.0050, train loss: 2.4365, val loss: 2.5355\n",
      "7068: lr: 0.0050, train loss: 2.4888, val loss: 2.4996\n",
      "7069: lr: 0.0050, train loss: 2.4364, val loss: 2.4758\n",
      "7070: lr: 0.0050, train loss: 2.4494, val loss: 2.4490\n",
      "7071: lr: 0.0050, train loss: 2.4331, val loss: 2.4724\n",
      "7072: lr: 0.0050, train loss: 2.4584, val loss: 2.4766\n",
      "7073: lr: 0.0050, train loss: 2.4820, val loss: 2.4777\n",
      "7074: lr: 0.0050, train loss: 2.4519, val loss: 2.4837\n",
      "7075: lr: 0.0050, train loss: 2.4363, val loss: 2.5302\n",
      "7076: lr: 0.0050, train loss: 2.4511, val loss: 2.4375\n",
      "7077: lr: 0.0050, train loss: 2.4245, val loss: 2.4768\n",
      "7078: lr: 0.0050, train loss: 2.4074, val loss: 2.4729\n",
      "7079: lr: 0.0050, train loss: 2.5048, val loss: 2.4605\n",
      "7080: lr: 0.0050, train loss: 2.4552, val loss: 2.5015\n",
      "7081: lr: 0.0050, train loss: 2.3958, val loss: 2.4749\n",
      "7082: lr: 0.0050, train loss: 2.4587, val loss: 2.5177\n",
      "7083: lr: 0.0050, train loss: 2.4719, val loss: 2.5295\n",
      "7084: lr: 0.0050, train loss: 2.4675, val loss: 2.4994\n",
      "7085: lr: 0.0050, train loss: 2.4872, val loss: 2.5063\n",
      "7086: lr: 0.0050, train loss: 2.4593, val loss: 2.5161\n",
      "7087: lr: 0.0050, train loss: 2.4524, val loss: 2.4585\n",
      "7088: lr: 0.0050, train loss: 2.4565, val loss: 2.5083\n",
      "7089: lr: 0.0050, train loss: 2.4239, val loss: 2.5071\n",
      "7090: lr: 0.0050, train loss: 2.4491, val loss: 2.4698\n",
      "7091: lr: 0.0050, train loss: 2.4695, val loss: 2.4743\n",
      "7092: lr: 0.0050, train loss: 2.4574, val loss: 2.4996\n",
      "7093: lr: 0.0050, train loss: 2.4490, val loss: 2.4785\n",
      "7094: lr: 0.0050, train loss: 2.4212, val loss: 2.5301\n",
      "7095: lr: 0.0050, train loss: 2.4162, val loss: 2.5057\n",
      "7096: lr: 0.0050, train loss: 2.4659, val loss: 2.4819\n",
      "7097: lr: 0.0050, train loss: 2.4283, val loss: 2.4919\n",
      "7098: lr: 0.0050, train loss: 2.4118, val loss: 2.5313\n",
      "7099: lr: 0.0050, train loss: 2.4534, val loss: 2.4788\n",
      "7100: lr: 0.0050, train loss: 2.4633, val loss: 2.5649\n",
      "7101: lr: 0.0050, train loss: 2.4868, val loss: 2.4784\n",
      "7102: lr: 0.0050, train loss: 2.4231, val loss: 2.4438\n",
      "7103: lr: 0.0050, train loss: 2.4570, val loss: 2.4759\n",
      "7104: lr: 0.0050, train loss: 2.4987, val loss: 2.4602\n",
      "7105: lr: 0.0050, train loss: 2.5014, val loss: 2.4865\n",
      "7106: lr: 0.0050, train loss: 2.4114, val loss: 2.4976\n",
      "7107: lr: 0.0050, train loss: 2.4058, val loss: 2.5013\n",
      "7108: lr: 0.0050, train loss: 2.4652, val loss: 2.4446\n",
      "7109: lr: 0.0050, train loss: 2.4756, val loss: 2.4267\n",
      "7110: lr: 0.0050, train loss: 2.4880, val loss: 2.4373\n",
      "7111: lr: 0.0050, train loss: 2.4488, val loss: 2.5131\n",
      "7112: lr: 0.0050, train loss: 2.4522, val loss: 2.5219\n",
      "7113: lr: 0.0050, train loss: 2.4243, val loss: 2.4542\n",
      "7114: lr: 0.0050, train loss: 2.4794, val loss: 2.5077\n",
      "7115: lr: 0.0050, train loss: 2.4808, val loss: 2.5245\n",
      "7116: lr: 0.0050, train loss: 2.4323, val loss: 2.4942\n",
      "7117: lr: 0.0050, train loss: 2.4500, val loss: 2.4518\n",
      "7118: lr: 0.0050, train loss: 2.4884, val loss: 2.4946\n",
      "7119: lr: 0.0050, train loss: 2.4690, val loss: 2.5268\n",
      "7120: lr: 0.0050, train loss: 2.4576, val loss: 2.5544\n",
      "7121: lr: 0.0050, train loss: 2.4591, val loss: 2.5205\n",
      "7122: lr: 0.0050, train loss: 2.4375, val loss: 2.4622\n",
      "7123: lr: 0.0050, train loss: 2.4271, val loss: 2.5199\n",
      "7124: lr: 0.0050, train loss: 2.4379, val loss: 2.4671\n",
      "7125: lr: 0.0050, train loss: 2.4512, val loss: 2.5264\n",
      "7126: lr: 0.0050, train loss: 2.4636, val loss: 2.4912\n",
      "7127: lr: 0.0050, train loss: 2.4470, val loss: 2.5441\n",
      "7128: lr: 0.0050, train loss: 2.4671, val loss: 2.4794\n",
      "7129: lr: 0.0050, train loss: 2.4594, val loss: 2.4959\n",
      "7130: lr: 0.0050, train loss: 2.4510, val loss: 2.4631\n",
      "7131: lr: 0.0050, train loss: 2.4698, val loss: 2.4683\n",
      "7132: lr: 0.0050, train loss: 2.4668, val loss: 2.5189\n",
      "7133: lr: 0.0050, train loss: 2.4371, val loss: 2.4980\n",
      "7134: lr: 0.0050, train loss: 2.4318, val loss: 2.4782\n",
      "7135: lr: 0.0050, train loss: 2.4394, val loss: 2.4338\n",
      "7136: lr: 0.0050, train loss: 2.4504, val loss: 2.5373\n",
      "7137: lr: 0.0050, train loss: 2.4558, val loss: 2.4827\n",
      "7138: lr: 0.0050, train loss: 2.4278, val loss: 2.5286\n",
      "7139: lr: 0.0050, train loss: 2.4152, val loss: 2.5118\n",
      "7140: lr: 0.0050, train loss: 2.4720, val loss: 2.4620\n",
      "7141: lr: 0.0050, train loss: 2.4721, val loss: 2.4865\n",
      "7142: lr: 0.0050, train loss: 2.4886, val loss: 2.4994\n",
      "7143: lr: 0.0050, train loss: 2.4353, val loss: 2.4459\n",
      "7144: lr: 0.0050, train loss: 2.4483, val loss: 2.5069\n",
      "7145: lr: 0.0050, train loss: 2.4634, val loss: 2.5016\n",
      "7146: lr: 0.0050, train loss: 2.4392, val loss: 2.5398\n",
      "7147: lr: 0.0050, train loss: 2.3931, val loss: 2.5117\n",
      "7148: lr: 0.0050, train loss: 2.4498, val loss: 2.4897\n",
      "7149: lr: 0.0050, train loss: 2.4192, val loss: 2.4545\n",
      "7150: lr: 0.0050, train loss: 2.4089, val loss: 2.4803\n",
      "7151: lr: 0.0050, train loss: 2.4455, val loss: 2.4321\n",
      "7152: lr: 0.0050, train loss: 2.5111, val loss: 2.5211\n",
      "7153: lr: 0.0050, train loss: 2.4485, val loss: 2.4844\n",
      "7154: lr: 0.0050, train loss: 2.4426, val loss: 2.5335\n",
      "7155: lr: 0.0050, train loss: 2.4738, val loss: 2.4830\n",
      "7156: lr: 0.0050, train loss: 2.5025, val loss: 2.4997\n",
      "7157: lr: 0.0050, train loss: 2.4315, val loss: 2.5133\n",
      "7158: lr: 0.0050, train loss: 2.4715, val loss: 2.4791\n",
      "7159: lr: 0.0050, train loss: 2.4647, val loss: 2.5135\n",
      "7160: lr: 0.0050, train loss: 2.4166, val loss: 2.5080\n",
      "7161: lr: 0.0050, train loss: 2.4689, val loss: 2.4685\n",
      "7162: lr: 0.0050, train loss: 2.4652, val loss: 2.4727\n",
      "7163: lr: 0.0050, train loss: 2.4582, val loss: 2.5035\n",
      "7164: lr: 0.0050, train loss: 2.4647, val loss: 2.5237\n",
      "7165: lr: 0.0050, train loss: 2.4365, val loss: 2.4726\n",
      "7166: lr: 0.0050, train loss: 2.4237, val loss: 2.4352\n",
      "7167: lr: 0.0050, train loss: 2.4757, val loss: 2.4972\n",
      "7168: lr: 0.0050, train loss: 2.4352, val loss: 2.5058\n",
      "7169: lr: 0.0050, train loss: 2.4757, val loss: 2.4761\n",
      "7170: lr: 0.0050, train loss: 2.4468, val loss: 2.4138\n",
      "7171: lr: 0.0050, train loss: 2.4663, val loss: 2.5612\n",
      "7172: lr: 0.0050, train loss: 2.4586, val loss: 2.4993\n",
      "7173: lr: 0.0050, train loss: 2.4886, val loss: 2.4518\n",
      "7174: lr: 0.0050, train loss: 2.4135, val loss: 2.4995\n",
      "7175: lr: 0.0050, train loss: 2.4507, val loss: 2.4946\n",
      "7176: lr: 0.0050, train loss: 2.4556, val loss: 2.5394\n",
      "7177: lr: 0.0050, train loss: 2.5019, val loss: 2.5266\n",
      "7178: lr: 0.0050, train loss: 2.4830, val loss: 2.5135\n",
      "7179: lr: 0.0050, train loss: 2.4631, val loss: 2.4439\n",
      "7180: lr: 0.0050, train loss: 2.4692, val loss: 2.5275\n",
      "7181: lr: 0.0050, train loss: 2.4184, val loss: 2.5004\n",
      "7182: lr: 0.0050, train loss: 2.4765, val loss: 2.4932\n",
      "7183: lr: 0.0050, train loss: 2.4477, val loss: 2.4867\n",
      "7184: lr: 0.0050, train loss: 2.4752, val loss: 2.4731\n",
      "7185: lr: 0.0050, train loss: 2.4461, val loss: 2.4910\n",
      "7186: lr: 0.0050, train loss: 2.4535, val loss: 2.4902\n",
      "7187: lr: 0.0050, train loss: 2.4689, val loss: 2.4914\n",
      "7188: lr: 0.0050, train loss: 2.4543, val loss: 2.5099\n",
      "7189: lr: 0.0050, train loss: 2.4824, val loss: 2.4644\n",
      "7190: lr: 0.0050, train loss: 2.4490, val loss: 2.4540\n",
      "7191: lr: 0.0050, train loss: 2.4579, val loss: 2.5313\n",
      "7192: lr: 0.0050, train loss: 2.4620, val loss: 2.4809\n",
      "7193: lr: 0.0050, train loss: 2.4679, val loss: 2.4736\n",
      "7194: lr: 0.0050, train loss: 2.4724, val loss: 2.5308\n",
      "7195: lr: 0.0050, train loss: 2.4664, val loss: 2.4972\n",
      "7196: lr: 0.0050, train loss: 2.4619, val loss: 2.4735\n",
      "7197: lr: 0.0050, train loss: 2.4368, val loss: 2.5370\n",
      "7198: lr: 0.0050, train loss: 2.4873, val loss: 2.4945\n",
      "7199: lr: 0.0050, train loss: 2.4368, val loss: 2.5125\n",
      "7200: lr: 0.0050, train loss: 2.5079, val loss: 2.4805\n",
      "7201: lr: 0.0050, train loss: 2.4950, val loss: 2.4458\n",
      "7202: lr: 0.0050, train loss: 2.3642, val loss: 2.4449\n",
      "7203: lr: 0.0050, train loss: 2.4832, val loss: 2.5344\n",
      "7204: lr: 0.0050, train loss: 2.4429, val loss: 2.4771\n",
      "7205: lr: 0.0050, train loss: 2.4211, val loss: 2.5178\n",
      "7206: lr: 0.0050, train loss: 2.4718, val loss: 2.4544\n",
      "7207: lr: 0.0050, train loss: 2.4734, val loss: 2.5127\n",
      "7208: lr: 0.0050, train loss: 2.4414, val loss: 2.4432\n",
      "7209: lr: 0.0050, train loss: 2.4305, val loss: 2.5192\n",
      "7210: lr: 0.0050, train loss: 2.4723, val loss: 2.4950\n",
      "7211: lr: 0.0050, train loss: 2.4539, val loss: 2.4803\n",
      "7212: lr: 0.0050, train loss: 2.4679, val loss: 2.5396\n",
      "7213: lr: 0.0050, train loss: 2.4617, val loss: 2.5058\n",
      "7214: lr: 0.0050, train loss: 2.4479, val loss: 2.4555\n",
      "7215: lr: 0.0050, train loss: 2.4594, val loss: 2.4912\n",
      "7216: lr: 0.0050, train loss: 2.4891, val loss: 2.5140\n",
      "7217: lr: 0.0050, train loss: 2.4210, val loss: 2.4923\n",
      "7218: lr: 0.0050, train loss: 2.4840, val loss: 2.5180\n",
      "7219: lr: 0.0050, train loss: 2.4817, val loss: 2.4609\n",
      "7220: lr: 0.0050, train loss: 2.4339, val loss: 2.4390\n",
      "7221: lr: 0.0050, train loss: 2.4817, val loss: 2.4438\n",
      "7222: lr: 0.0050, train loss: 2.4796, val loss: 2.4950\n",
      "7223: lr: 0.0050, train loss: 2.4759, val loss: 2.4987\n",
      "7224: lr: 0.0050, train loss: 2.4605, val loss: 2.4929\n",
      "7225: lr: 0.0050, train loss: 2.4589, val loss: 2.4905\n",
      "7226: lr: 0.0050, train loss: 2.4552, val loss: 2.4623\n",
      "7227: lr: 0.0050, train loss: 2.4614, val loss: 2.4617\n",
      "7228: lr: 0.0050, train loss: 2.4711, val loss: 2.4717\n",
      "7229: lr: 0.0050, train loss: 2.4587, val loss: 2.4668\n",
      "7230: lr: 0.0050, train loss: 2.4687, val loss: 2.4521\n",
      "7231: lr: 0.0050, train loss: 2.4785, val loss: 2.4792\n",
      "7232: lr: 0.0050, train loss: 2.4235, val loss: 2.4353\n",
      "7233: lr: 0.0050, train loss: 2.4797, val loss: 2.5030\n",
      "7234: lr: 0.0050, train loss: 2.4472, val loss: 2.4653\n",
      "7235: lr: 0.0050, train loss: 2.4250, val loss: 2.4927\n",
      "7236: lr: 0.0050, train loss: 2.4619, val loss: 2.5108\n",
      "7237: lr: 0.0050, train loss: 2.4876, val loss: 2.5309\n",
      "7238: lr: 0.0050, train loss: 2.4680, val loss: 2.4988\n",
      "7239: lr: 0.0050, train loss: 2.4769, val loss: 2.5305\n",
      "7240: lr: 0.0050, train loss: 2.4461, val loss: 2.4947\n",
      "7241: lr: 0.0050, train loss: 2.4503, val loss: 2.5097\n",
      "7242: lr: 0.0050, train loss: 2.4296, val loss: 2.4248\n",
      "7243: lr: 0.0050, train loss: 2.4263, val loss: 2.5120\n",
      "7244: lr: 0.0050, train loss: 2.4589, val loss: 2.4613\n",
      "7245: lr: 0.0050, train loss: 2.4177, val loss: 2.4646\n",
      "7246: lr: 0.0050, train loss: 2.4261, val loss: 2.4788\n",
      "7247: lr: 0.0050, train loss: 2.4487, val loss: 2.5589\n",
      "7248: lr: 0.0050, train loss: 2.4878, val loss: 2.4996\n",
      "7249: lr: 0.0050, train loss: 2.4456, val loss: 2.4743\n",
      "7250: lr: 0.0050, train loss: 2.4253, val loss: 2.5328\n",
      "7251: lr: 0.0050, train loss: 2.4596, val loss: 2.4691\n",
      "7252: lr: 0.0050, train loss: 2.4532, val loss: 2.4833\n",
      "7253: lr: 0.0050, train loss: 2.4491, val loss: 2.4415\n",
      "7254: lr: 0.0050, train loss: 2.4468, val loss: 2.4661\n",
      "7255: lr: 0.0050, train loss: 2.4207, val loss: 2.4842\n",
      "7256: lr: 0.0050, train loss: 2.4890, val loss: 2.4537\n",
      "7257: lr: 0.0050, train loss: 2.4440, val loss: 2.4626\n",
      "7258: lr: 0.0050, train loss: 2.4592, val loss: 2.4817\n",
      "7259: lr: 0.0050, train loss: 2.4738, val loss: 2.5239\n",
      "7260: lr: 0.0050, train loss: 2.4525, val loss: 2.4486\n",
      "7261: lr: 0.0050, train loss: 2.4296, val loss: 2.4717\n",
      "7262: lr: 0.0050, train loss: 2.4357, val loss: 2.4682\n",
      "7263: lr: 0.0050, train loss: 2.4686, val loss: 2.5188\n",
      "7264: lr: 0.0050, train loss: 2.4219, val loss: 2.4757\n",
      "7265: lr: 0.0050, train loss: 2.4726, val loss: 2.4894\n",
      "7266: lr: 0.0050, train loss: 2.4630, val loss: 2.5223\n",
      "7267: lr: 0.0050, train loss: 2.4758, val loss: 2.4916\n",
      "7268: lr: 0.0050, train loss: 2.4758, val loss: 2.5073\n",
      "7269: lr: 0.0050, train loss: 2.4808, val loss: 2.4709\n",
      "7270: lr: 0.0050, train loss: 2.4510, val loss: 2.5246\n",
      "7271: lr: 0.0050, train loss: 2.4317, val loss: 2.4498\n",
      "7272: lr: 0.0050, train loss: 2.4651, val loss: 2.4725\n",
      "7273: lr: 0.0050, train loss: 2.5016, val loss: 2.5016\n",
      "7274: lr: 0.0050, train loss: 2.4596, val loss: 2.4579\n",
      "7275: lr: 0.0050, train loss: 2.4798, val loss: 2.5073\n",
      "7276: lr: 0.0050, train loss: 2.4330, val loss: 2.5059\n",
      "7277: lr: 0.0050, train loss: 2.4693, val loss: 2.4264\n",
      "7278: lr: 0.0050, train loss: 2.4457, val loss: 2.4565\n",
      "7279: lr: 0.0050, train loss: 2.4907, val loss: 2.4829\n",
      "7280: lr: 0.0050, train loss: 2.4368, val loss: 2.4841\n",
      "7281: lr: 0.0050, train loss: 2.4404, val loss: 2.5228\n",
      "7282: lr: 0.0050, train loss: 2.4318, val loss: 2.4995\n",
      "7283: lr: 0.0050, train loss: 2.4258, val loss: 2.4659\n",
      "7284: lr: 0.0050, train loss: 2.4675, val loss: 2.4659\n",
      "7285: lr: 0.0050, train loss: 2.4618, val loss: 2.5255\n",
      "7286: lr: 0.0050, train loss: 2.4653, val loss: 2.5216\n",
      "7287: lr: 0.0050, train loss: 2.4393, val loss: 2.4653\n",
      "7288: lr: 0.0050, train loss: 2.4356, val loss: 2.4675\n",
      "7289: lr: 0.0050, train loss: 2.4565, val loss: 2.5120\n",
      "7290: lr: 0.0050, train loss: 2.4684, val loss: 2.4943\n",
      "7291: lr: 0.0050, train loss: 2.4570, val loss: 2.5052\n",
      "7292: lr: 0.0050, train loss: 2.4491, val loss: 2.4786\n",
      "7293: lr: 0.0050, train loss: 2.4600, val loss: 2.5173\n",
      "7294: lr: 0.0050, train loss: 2.4677, val loss: 2.4650\n",
      "7295: lr: 0.0050, train loss: 2.4591, val loss: 2.5313\n",
      "7296: lr: 0.0050, train loss: 2.5029, val loss: 2.5115\n",
      "7297: lr: 0.0050, train loss: 2.4325, val loss: 2.4999\n",
      "7298: lr: 0.0050, train loss: 2.4530, val loss: 2.4581\n",
      "7299: lr: 0.0050, train loss: 2.4422, val loss: 2.4934\n",
      "7300: lr: 0.0050, train loss: 2.4729, val loss: 2.5311\n",
      "7301: lr: 0.0050, train loss: 2.4769, val loss: 2.4807\n",
      "7302: lr: 0.0050, train loss: 2.4462, val loss: 2.5065\n",
      "7303: lr: 0.0050, train loss: 2.4727, val loss: 2.4818\n",
      "7304: lr: 0.0050, train loss: 2.4543, val loss: 2.4649\n",
      "7305: lr: 0.0050, train loss: 2.4550, val loss: 2.5038\n",
      "7306: lr: 0.0050, train loss: 2.4697, val loss: 2.4702\n",
      "7307: lr: 0.0050, train loss: 2.4855, val loss: 2.5308\n",
      "7308: lr: 0.0050, train loss: 2.4394, val loss: 2.4119\n",
      "7309: lr: 0.0050, train loss: 2.4783, val loss: 2.4480\n",
      "7310: lr: 0.0050, train loss: 2.4327, val loss: 2.4978\n",
      "7311: lr: 0.0050, train loss: 2.4998, val loss: 2.4724\n",
      "7312: lr: 0.0050, train loss: 2.4546, val loss: 2.4611\n",
      "7313: lr: 0.0050, train loss: 2.4622, val loss: 2.5105\n",
      "7314: lr: 0.0050, train loss: 2.4563, val loss: 2.4710\n",
      "7315: lr: 0.0050, train loss: 2.4531, val loss: 2.4775\n",
      "7316: lr: 0.0050, train loss: 2.4466, val loss: 2.4560\n",
      "7317: lr: 0.0050, train loss: 2.4393, val loss: 2.4489\n",
      "7318: lr: 0.0050, train loss: 2.4668, val loss: 2.4592\n",
      "7319: lr: 0.0050, train loss: 2.4501, val loss: 2.4651\n",
      "7320: lr: 0.0050, train loss: 2.4356, val loss: 2.5281\n",
      "7321: lr: 0.0050, train loss: 2.4658, val loss: 2.4920\n",
      "7322: lr: 0.0050, train loss: 2.4509, val loss: 2.4419\n",
      "7323: lr: 0.0050, train loss: 2.4240, val loss: 2.5269\n",
      "7324: lr: 0.0050, train loss: 2.4686, val loss: 2.4951\n",
      "7325: lr: 0.0050, train loss: 2.4713, val loss: 2.5429\n",
      "7326: lr: 0.0050, train loss: 2.4197, val loss: 2.4865\n",
      "7327: lr: 0.0050, train loss: 2.4555, val loss: 2.4846\n",
      "7328: lr: 0.0050, train loss: 2.4658, val loss: 2.4661\n",
      "7329: lr: 0.0050, train loss: 2.4556, val loss: 2.5353\n",
      "7330: lr: 0.0050, train loss: 2.4296, val loss: 2.4717\n",
      "7331: lr: 0.0050, train loss: 2.4793, val loss: 2.5273\n",
      "7332: lr: 0.0050, train loss: 2.4589, val loss: 2.4493\n",
      "7333: lr: 0.0050, train loss: 2.4392, val loss: 2.4927\n",
      "7334: lr: 0.0050, train loss: 2.4705, val loss: 2.4665\n",
      "7335: lr: 0.0050, train loss: 2.4445, val loss: 2.5263\n",
      "7336: lr: 0.0050, train loss: 2.4428, val loss: 2.5076\n",
      "7337: lr: 0.0050, train loss: 2.4300, val loss: 2.5051\n",
      "7338: lr: 0.0050, train loss: 2.4854, val loss: 2.4998\n",
      "7339: lr: 0.0050, train loss: 2.4496, val loss: 2.4419\n",
      "7340: lr: 0.0050, train loss: 2.4607, val loss: 2.4994\n",
      "7341: lr: 0.0050, train loss: 2.4043, val loss: 2.5093\n",
      "7342: lr: 0.0050, train loss: 2.4569, val loss: 2.5126\n",
      "7343: lr: 0.0050, train loss: 2.4474, val loss: 2.5003\n",
      "7344: lr: 0.0050, train loss: 2.4989, val loss: 2.5250\n",
      "7345: lr: 0.0050, train loss: 2.4437, val loss: 2.4977\n",
      "7346: lr: 0.0050, train loss: 2.4575, val loss: 2.4702\n",
      "7347: lr: 0.0050, train loss: 2.4547, val loss: 2.5137\n",
      "7348: lr: 0.0050, train loss: 2.4589, val loss: 2.4873\n",
      "7349: lr: 0.0050, train loss: 2.4305, val loss: 2.4723\n",
      "7350: lr: 0.0050, train loss: 2.4395, val loss: 2.5096\n",
      "7351: lr: 0.0050, train loss: 2.4448, val loss: 2.5150\n",
      "7352: lr: 0.0050, train loss: 2.4435, val loss: 2.4429\n",
      "7353: lr: 0.0050, train loss: 2.4642, val loss: 2.4703\n",
      "7354: lr: 0.0050, train loss: 2.4599, val loss: 2.4584\n",
      "7355: lr: 0.0050, train loss: 2.4555, val loss: 2.4356\n",
      "7356: lr: 0.0050, train loss: 2.4515, val loss: 2.4536\n",
      "7357: lr: 0.0050, train loss: 2.4083, val loss: 2.4886\n",
      "7358: lr: 0.0050, train loss: 2.4982, val loss: 2.4943\n",
      "7359: lr: 0.0050, train loss: 2.4287, val loss: 2.4834\n",
      "7360: lr: 0.0050, train loss: 2.4762, val loss: 2.4963\n",
      "7361: lr: 0.0050, train loss: 2.4468, val loss: 2.4879\n",
      "7362: lr: 0.0050, train loss: 2.4670, val loss: 2.5069\n",
      "7363: lr: 0.0050, train loss: 2.4490, val loss: 2.4686\n",
      "7364: lr: 0.0050, train loss: 2.4304, val loss: 2.4542\n",
      "7365: lr: 0.0050, train loss: 2.4339, val loss: 2.4808\n",
      "7366: lr: 0.0050, train loss: 2.4911, val loss: 2.5543\n",
      "7367: lr: 0.0050, train loss: 2.4272, val loss: 2.5082\n",
      "7368: lr: 0.0050, train loss: 2.4881, val loss: 2.4930\n",
      "7369: lr: 0.0050, train loss: 2.4300, val loss: 2.4366\n",
      "7370: lr: 0.0050, train loss: 2.3984, val loss: 2.4910\n",
      "7371: lr: 0.0050, train loss: 2.4493, val loss: 2.4863\n",
      "7372: lr: 0.0050, train loss: 2.4416, val loss: 2.5124\n",
      "7373: lr: 0.0050, train loss: 2.4761, val loss: 2.4763\n",
      "7374: lr: 0.0050, train loss: 2.4524, val loss: 2.4668\n",
      "7375: lr: 0.0050, train loss: 2.4229, val loss: 2.4970\n",
      "7376: lr: 0.0050, train loss: 2.4925, val loss: 2.4967\n",
      "7377: lr: 0.0050, train loss: 2.4480, val loss: 2.4376\n",
      "7378: lr: 0.0050, train loss: 2.5049, val loss: 2.4949\n",
      "7379: lr: 0.0050, train loss: 2.4540, val loss: 2.5129\n",
      "7380: lr: 0.0050, train loss: 2.4702, val loss: 2.4683\n",
      "7381: lr: 0.0050, train loss: 2.4942, val loss: 2.4707\n",
      "7382: lr: 0.0050, train loss: 2.4507, val loss: 2.4793\n",
      "7383: lr: 0.0050, train loss: 2.4913, val loss: 2.4972\n",
      "7384: lr: 0.0050, train loss: 2.4485, val loss: 2.4407\n",
      "7385: lr: 0.0050, train loss: 2.4421, val loss: 2.4876\n",
      "7386: lr: 0.0050, train loss: 2.4383, val loss: 2.4940\n",
      "7387: lr: 0.0050, train loss: 2.4590, val loss: 2.5259\n",
      "7388: lr: 0.0050, train loss: 2.4364, val loss: 2.4836\n",
      "7389: lr: 0.0050, train loss: 2.4820, val loss: 2.4594\n",
      "7390: lr: 0.0050, train loss: 2.4597, val loss: 2.5306\n",
      "7391: lr: 0.0050, train loss: 2.4774, val loss: 2.4384\n",
      "7392: lr: 0.0050, train loss: 2.4545, val loss: 2.4574\n",
      "7393: lr: 0.0050, train loss: 2.4862, val loss: 2.4774\n",
      "7394: lr: 0.0050, train loss: 2.4341, val loss: 2.5018\n",
      "7395: lr: 0.0050, train loss: 2.4311, val loss: 2.4875\n",
      "7396: lr: 0.0050, train loss: 2.4641, val loss: 2.4600\n",
      "7397: lr: 0.0050, train loss: 2.4729, val loss: 2.5064\n",
      "7398: lr: 0.0050, train loss: 2.4624, val loss: 2.4725\n",
      "7399: lr: 0.0050, train loss: 2.4568, val loss: 2.4857\n",
      "7400: lr: 0.0050, train loss: 2.4093, val loss: 2.5079\n",
      "7401: lr: 0.0050, train loss: 2.4253, val loss: 2.4798\n",
      "7402: lr: 0.0050, train loss: 2.4233, val loss: 2.5184\n",
      "7403: lr: 0.0050, train loss: 2.4753, val loss: 2.4812\n",
      "7404: lr: 0.0050, train loss: 2.4327, val loss: 2.5326\n",
      "7405: lr: 0.0050, train loss: 2.4885, val loss: 2.5077\n",
      "7406: lr: 0.0050, train loss: 2.4583, val loss: 2.4930\n",
      "7407: lr: 0.0050, train loss: 2.4396, val loss: 2.4989\n",
      "7408: lr: 0.0050, train loss: 2.4657, val loss: 2.5126\n",
      "7409: lr: 0.0050, train loss: 2.4375, val loss: 2.5575\n",
      "7410: lr: 0.0050, train loss: 2.4813, val loss: 2.4582\n",
      "7411: lr: 0.0050, train loss: 2.4181, val loss: 2.4310\n",
      "7412: lr: 0.0050, train loss: 2.4649, val loss: 2.4791\n",
      "7413: lr: 0.0050, train loss: 2.4338, val loss: 2.4631\n",
      "7414: lr: 0.0050, train loss: 2.4481, val loss: 2.4976\n",
      "7415: lr: 0.0050, train loss: 2.4103, val loss: 2.4854\n",
      "7416: lr: 0.0050, train loss: 2.4513, val loss: 2.5354\n",
      "7417: lr: 0.0050, train loss: 2.4329, val loss: 2.4789\n",
      "7418: lr: 0.0050, train loss: 2.4499, val loss: 2.4757\n",
      "7419: lr: 0.0050, train loss: 2.4863, val loss: 2.5075\n",
      "7420: lr: 0.0050, train loss: 2.4323, val loss: 2.4388\n",
      "7421: lr: 0.0050, train loss: 2.4389, val loss: 2.4656\n",
      "7422: lr: 0.0050, train loss: 2.4852, val loss: 2.5075\n",
      "7423: lr: 0.0050, train loss: 2.4616, val loss: 2.5051\n",
      "7424: lr: 0.0050, train loss: 2.4929, val loss: 2.4728\n",
      "7425: lr: 0.0050, train loss: 2.4162, val loss: 2.4623\n",
      "7426: lr: 0.0050, train loss: 2.4579, val loss: 2.5027\n",
      "7427: lr: 0.0050, train loss: 2.4646, val loss: 2.4617\n",
      "7428: lr: 0.0050, train loss: 2.4498, val loss: 2.4636\n",
      "7429: lr: 0.0050, train loss: 2.4656, val loss: 2.4722\n",
      "7430: lr: 0.0050, train loss: 2.4378, val loss: 2.4707\n",
      "7431: lr: 0.0050, train loss: 2.4760, val loss: 2.4524\n",
      "7432: lr: 0.0050, train loss: 2.4687, val loss: 2.4647\n",
      "7433: lr: 0.0050, train loss: 2.4203, val loss: 2.5199\n",
      "7434: lr: 0.0050, train loss: 2.4636, val loss: 2.5276\n",
      "7435: lr: 0.0050, train loss: 2.4914, val loss: 2.4532\n",
      "7436: lr: 0.0050, train loss: 2.4421, val loss: 2.4823\n",
      "7437: lr: 0.0050, train loss: 2.4741, val loss: 2.4816\n",
      "7438: lr: 0.0050, train loss: 2.4462, val loss: 2.4302\n",
      "7439: lr: 0.0050, train loss: 2.4331, val loss: 2.4583\n",
      "7440: lr: 0.0050, train loss: 2.3934, val loss: 2.5233\n",
      "7441: lr: 0.0050, train loss: 2.4451, val loss: 2.4693\n",
      "7442: lr: 0.0050, train loss: 2.4274, val loss: 2.4737\n",
      "7443: lr: 0.0050, train loss: 2.4542, val loss: 2.4776\n",
      "7444: lr: 0.0050, train loss: 2.5197, val loss: 2.4918\n",
      "7445: lr: 0.0050, train loss: 2.4418, val loss: 2.4738\n",
      "7446: lr: 0.0050, train loss: 2.4459, val loss: 2.5245\n",
      "7447: lr: 0.0050, train loss: 2.4511, val loss: 2.5148\n",
      "7448: lr: 0.0050, train loss: 2.4777, val loss: 2.5433\n",
      "7449: lr: 0.0050, train loss: 2.4360, val loss: 2.5367\n",
      "7450: lr: 0.0050, train loss: 2.3931, val loss: 2.4994\n",
      "7451: lr: 0.0050, train loss: 2.4327, val loss: 2.4832\n",
      "7452: lr: 0.0050, train loss: 2.4581, val loss: 2.5063\n",
      "7453: lr: 0.0050, train loss: 2.4753, val loss: 2.4159\n",
      "7454: lr: 0.0050, train loss: 2.4309, val loss: 2.4999\n",
      "7455: lr: 0.0050, train loss: 2.4264, val loss: 2.4573\n",
      "7456: lr: 0.0050, train loss: 2.4150, val loss: 2.4623\n",
      "7457: lr: 0.0050, train loss: 2.4620, val loss: 2.5067\n",
      "7458: lr: 0.0050, train loss: 2.4385, val loss: 2.4449\n",
      "7459: lr: 0.0050, train loss: 2.4424, val loss: 2.4058\n",
      "7460: lr: 0.0050, train loss: 2.4503, val loss: 2.5155\n",
      "7461: lr: 0.0050, train loss: 2.4424, val loss: 2.4861\n",
      "7462: lr: 0.0050, train loss: 2.4403, val loss: 2.5122\n",
      "7463: lr: 0.0050, train loss: 2.4434, val loss: 2.5716\n",
      "7464: lr: 0.0050, train loss: 2.4734, val loss: 2.5075\n",
      "7465: lr: 0.0050, train loss: 2.4576, val loss: 2.4824\n",
      "7466: lr: 0.0050, train loss: 2.4648, val loss: 2.5145\n",
      "7467: lr: 0.0050, train loss: 2.5053, val loss: 2.4998\n",
      "7468: lr: 0.0050, train loss: 2.4607, val loss: 2.4030\n",
      "7469: lr: 0.0050, train loss: 2.4516, val loss: 2.5340\n",
      "7470: lr: 0.0050, train loss: 2.4614, val loss: 2.4598\n",
      "7471: lr: 0.0050, train loss: 2.4004, val loss: 2.5143\n",
      "7472: lr: 0.0050, train loss: 2.4505, val loss: 2.4881\n",
      "7473: lr: 0.0050, train loss: 2.4569, val loss: 2.4595\n",
      "7474: lr: 0.0050, train loss: 2.4504, val loss: 2.4942\n",
      "7475: lr: 0.0050, train loss: 2.4286, val loss: 2.4977\n",
      "7476: lr: 0.0050, train loss: 2.4414, val loss: 2.4435\n",
      "7477: lr: 0.0050, train loss: 2.4226, val loss: 2.5420\n",
      "7478: lr: 0.0050, train loss: 2.4048, val loss: 2.4529\n",
      "7479: lr: 0.0050, train loss: 2.5049, val loss: 2.4761\n",
      "7480: lr: 0.0050, train loss: 2.4341, val loss: 2.4871\n",
      "7481: lr: 0.0050, train loss: 2.4388, val loss: 2.4776\n",
      "7482: lr: 0.0050, train loss: 2.4474, val loss: 2.4579\n",
      "7483: lr: 0.0050, train loss: 2.4159, val loss: 2.5124\n",
      "7484: lr: 0.0050, train loss: 2.4694, val loss: 2.5104\n",
      "7485: lr: 0.0050, train loss: 2.5239, val loss: 2.4591\n",
      "7486: lr: 0.0050, train loss: 2.4381, val loss: 2.4636\n",
      "7487: lr: 0.0050, train loss: 2.4078, val loss: 2.4469\n",
      "7488: lr: 0.0050, train loss: 2.4512, val loss: 2.5000\n",
      "7489: lr: 0.0050, train loss: 2.4698, val loss: 2.4755\n",
      "7490: lr: 0.0050, train loss: 2.4874, val loss: 2.4874\n",
      "7491: lr: 0.0050, train loss: 2.4398, val loss: 2.5108\n",
      "7492: lr: 0.0050, train loss: 2.4359, val loss: 2.4875\n",
      "7493: lr: 0.0050, train loss: 2.4610, val loss: 2.5351\n",
      "7494: lr: 0.0050, train loss: 2.4504, val loss: 2.5004\n",
      "7495: lr: 0.0050, train loss: 2.4842, val loss: 2.4876\n",
      "7496: lr: 0.0050, train loss: 2.4895, val loss: 2.4740\n",
      "7497: lr: 0.0050, train loss: 2.5123, val loss: 2.4465\n",
      "7498: lr: 0.0050, train loss: 2.4742, val loss: 2.5031\n",
      "7499: lr: 0.0050, train loss: 2.4504, val loss: 2.4835\n",
      "7500: lr: 0.0050, train loss: 2.4255, val loss: 2.4706\n",
      "7501: lr: 0.0050, train loss: 2.4510, val loss: 2.5061\n",
      "7502: lr: 0.0050, train loss: 2.4617, val loss: 2.4790\n",
      "7503: lr: 0.0050, train loss: 2.4236, val loss: 2.4794\n",
      "7504: lr: 0.0050, train loss: 2.4737, val loss: 2.4959\n",
      "7505: lr: 0.0050, train loss: 2.4749, val loss: 2.5053\n",
      "7506: lr: 0.0050, train loss: 2.4301, val loss: 2.4506\n",
      "7507: lr: 0.0050, train loss: 2.4765, val loss: 2.4627\n",
      "7508: lr: 0.0050, train loss: 2.4381, val loss: 2.4889\n",
      "7509: lr: 0.0050, train loss: 2.4214, val loss: 2.4549\n",
      "7510: lr: 0.0050, train loss: 2.3948, val loss: 2.4608\n",
      "7511: lr: 0.0050, train loss: 2.4194, val loss: 2.4919\n",
      "7512: lr: 0.0050, train loss: 2.4786, val loss: 2.4894\n",
      "7513: lr: 0.0050, train loss: 2.5010, val loss: 2.4688\n",
      "7514: lr: 0.0050, train loss: 2.4495, val loss: 2.5097\n",
      "7515: lr: 0.0050, train loss: 2.4792, val loss: 2.4648\n",
      "7516: lr: 0.0050, train loss: 2.4503, val loss: 2.4640\n",
      "7517: lr: 0.0050, train loss: 2.4210, val loss: 2.4876\n",
      "7518: lr: 0.0050, train loss: 2.4852, val loss: 2.5235\n",
      "7519: lr: 0.0050, train loss: 2.4070, val loss: 2.5198\n",
      "7520: lr: 0.0050, train loss: 2.4587, val loss: 2.5192\n",
      "7521: lr: 0.0050, train loss: 2.4061, val loss: 2.4798\n",
      "7522: lr: 0.0050, train loss: 2.4729, val loss: 2.4963\n",
      "7523: lr: 0.0050, train loss: 2.4578, val loss: 2.5125\n",
      "7524: lr: 0.0050, train loss: 2.4818, val loss: 2.4379\n",
      "7525: lr: 0.0050, train loss: 2.4326, val loss: 2.4856\n",
      "7526: lr: 0.0050, train loss: 2.4529, val loss: 2.5013\n",
      "7527: lr: 0.0050, train loss: 2.5247, val loss: 2.4713\n",
      "7528: lr: 0.0050, train loss: 2.4577, val loss: 2.5306\n",
      "7529: lr: 0.0050, train loss: 2.4305, val loss: 2.5157\n",
      "7530: lr: 0.0050, train loss: 2.4681, val loss: 2.4823\n",
      "7531: lr: 0.0050, train loss: 2.4882, val loss: 2.5363\n",
      "7532: lr: 0.0050, train loss: 2.4898, val loss: 2.4787\n",
      "7533: lr: 0.0050, train loss: 2.4876, val loss: 2.4912\n",
      "7534: lr: 0.0050, train loss: 2.4693, val loss: 2.5195\n",
      "7535: lr: 0.0050, train loss: 2.4229, val loss: 2.4769\n",
      "7536: lr: 0.0050, train loss: 2.4796, val loss: 2.5250\n",
      "7537: lr: 0.0050, train loss: 2.4810, val loss: 2.5042\n",
      "7538: lr: 0.0050, train loss: 2.4414, val loss: 2.5352\n",
      "7539: lr: 0.0050, train loss: 2.4231, val loss: 2.4810\n",
      "7540: lr: 0.0050, train loss: 2.4977, val loss: 2.4601\n",
      "7541: lr: 0.0050, train loss: 2.4772, val loss: 2.4419\n",
      "7542: lr: 0.0050, train loss: 2.4696, val loss: 2.4761\n",
      "7543: lr: 0.0050, train loss: 2.5028, val loss: 2.4814\n",
      "7544: lr: 0.0050, train loss: 2.4312, val loss: 2.5379\n",
      "7545: lr: 0.0050, train loss: 2.4627, val loss: 2.4972\n",
      "7546: lr: 0.0050, train loss: 2.4751, val loss: 2.4589\n",
      "7547: lr: 0.0050, train loss: 2.4206, val loss: 2.4486\n",
      "7548: lr: 0.0050, train loss: 2.4767, val loss: 2.4599\n",
      "7549: lr: 0.0050, train loss: 2.4373, val loss: 2.4992\n",
      "7550: lr: 0.0050, train loss: 2.4070, val loss: 2.4229\n",
      "7551: lr: 0.0050, train loss: 2.4264, val loss: 2.5063\n",
      "7552: lr: 0.0050, train loss: 2.4503, val loss: 2.5001\n",
      "7553: lr: 0.0050, train loss: 2.5175, val loss: 2.4940\n",
      "7554: lr: 0.0050, train loss: 2.4323, val loss: 2.4860\n",
      "7555: lr: 0.0050, train loss: 2.4379, val loss: 2.5228\n",
      "7556: lr: 0.0050, train loss: 2.4350, val loss: 2.5148\n",
      "7557: lr: 0.0050, train loss: 2.4243, val loss: 2.5125\n",
      "7558: lr: 0.0050, train loss: 2.4240, val loss: 2.4860\n",
      "7559: lr: 0.0050, train loss: 2.4535, val loss: 2.4706\n",
      "7560: lr: 0.0050, train loss: 2.4873, val loss: 2.5194\n",
      "7561: lr: 0.0050, train loss: 2.4700, val loss: 2.5024\n",
      "7562: lr: 0.0050, train loss: 2.4571, val loss: 2.4677\n",
      "7563: lr: 0.0050, train loss: 2.4418, val loss: 2.4865\n",
      "7564: lr: 0.0050, train loss: 2.4159, val loss: 2.4615\n",
      "7565: lr: 0.0050, train loss: 2.4770, val loss: 2.5216\n",
      "7566: lr: 0.0050, train loss: 2.4497, val loss: 2.4360\n",
      "7567: lr: 0.0050, train loss: 2.4363, val loss: 2.4930\n",
      "7568: lr: 0.0050, train loss: 2.4254, val loss: 2.4264\n",
      "7569: lr: 0.0050, train loss: 2.4593, val loss: 2.4429\n",
      "7570: lr: 0.0050, train loss: 2.4242, val loss: 2.4416\n",
      "7571: lr: 0.0050, train loss: 2.4387, val loss: 2.5292\n",
      "7572: lr: 0.0050, train loss: 2.4439, val loss: 2.5182\n",
      "7573: lr: 0.0050, train loss: 2.4025, val loss: 2.4489\n",
      "7574: lr: 0.0050, train loss: 2.4585, val loss: 2.5129\n",
      "7575: lr: 0.0050, train loss: 2.4521, val loss: 2.4794\n",
      "7576: lr: 0.0050, train loss: 2.4488, val loss: 2.4598\n",
      "7577: lr: 0.0050, train loss: 2.4640, val loss: 2.4959\n",
      "7578: lr: 0.0050, train loss: 2.4562, val loss: 2.4774\n",
      "7579: lr: 0.0050, train loss: 2.4176, val loss: 2.4947\n",
      "7580: lr: 0.0050, train loss: 2.4613, val loss: 2.4704\n",
      "7581: lr: 0.0050, train loss: 2.4585, val loss: 2.4724\n",
      "7582: lr: 0.0050, train loss: 2.4952, val loss: 2.4877\n",
      "7583: lr: 0.0050, train loss: 2.4489, val loss: 2.5025\n",
      "7584: lr: 0.0050, train loss: 2.4619, val loss: 2.5975\n",
      "7585: lr: 0.0050, train loss: 2.4682, val loss: 2.4858\n",
      "7586: lr: 0.0050, train loss: 2.4582, val loss: 2.4976\n",
      "7587: lr: 0.0050, train loss: 2.4535, val loss: 2.4550\n",
      "7588: lr: 0.0050, train loss: 2.4645, val loss: 2.4855\n",
      "7589: lr: 0.0050, train loss: 2.5128, val loss: 2.4439\n",
      "7590: lr: 0.0050, train loss: 2.4797, val loss: 2.4701\n",
      "7591: lr: 0.0050, train loss: 2.4566, val loss: 2.5180\n",
      "7592: lr: 0.0050, train loss: 2.4192, val loss: 2.5073\n",
      "7593: lr: 0.0050, train loss: 2.4513, val loss: 2.5196\n",
      "7594: lr: 0.0050, train loss: 2.4894, val loss: 2.4808\n",
      "7595: lr: 0.0050, train loss: 2.4786, val loss: 2.4879\n",
      "7596: lr: 0.0050, train loss: 2.4485, val loss: 2.4784\n",
      "7597: lr: 0.0050, train loss: 2.4581, val loss: 2.4972\n",
      "7598: lr: 0.0050, train loss: 2.4494, val loss: 2.4440\n",
      "7599: lr: 0.0050, train loss: 2.4175, val loss: 2.4534\n",
      "7600: lr: 0.0050, train loss: 2.4409, val loss: 2.5144\n",
      "7601: lr: 0.0050, train loss: 2.4058, val loss: 2.5192\n",
      "7602: lr: 0.0050, train loss: 2.4701, val loss: 2.4368\n",
      "7603: lr: 0.0050, train loss: 2.5004, val loss: 2.4598\n",
      "7604: lr: 0.0050, train loss: 2.4384, val loss: 2.4750\n",
      "7605: lr: 0.0050, train loss: 2.4752, val loss: 2.4702\n",
      "7606: lr: 0.0050, train loss: 2.4981, val loss: 2.5527\n",
      "7607: lr: 0.0050, train loss: 2.4509, val loss: 2.4853\n",
      "7608: lr: 0.0050, train loss: 2.4197, val loss: 2.4689\n",
      "7609: lr: 0.0050, train loss: 2.4617, val loss: 2.4923\n",
      "7610: lr: 0.0050, train loss: 2.5403, val loss: 2.4886\n",
      "7611: lr: 0.0050, train loss: 2.4476, val loss: 2.5095\n",
      "7612: lr: 0.0050, train loss: 2.4272, val loss: 2.5375\n",
      "7613: lr: 0.0050, train loss: 2.4575, val loss: 2.4899\n",
      "7614: lr: 0.0050, train loss: 2.4566, val loss: 2.4599\n",
      "7615: lr: 0.0050, train loss: 2.4045, val loss: 2.4727\n",
      "7616: lr: 0.0050, train loss: 2.4315, val loss: 2.4764\n",
      "7617: lr: 0.0050, train loss: 2.4346, val loss: 2.4423\n",
      "7618: lr: 0.0050, train loss: 2.4898, val loss: 2.4792\n",
      "7619: lr: 0.0050, train loss: 2.4494, val loss: 2.5015\n",
      "7620: lr: 0.0050, train loss: 2.4723, val loss: 2.4478\n",
      "7621: lr: 0.0050, train loss: 2.4308, val loss: 2.4616\n",
      "7622: lr: 0.0050, train loss: 2.4920, val loss: 2.5177\n",
      "7623: lr: 0.0050, train loss: 2.4070, val loss: 2.4639\n",
      "7624: lr: 0.0050, train loss: 2.4295, val loss: 2.5329\n",
      "7625: lr: 0.0050, train loss: 2.4903, val loss: 2.4687\n",
      "7626: lr: 0.0050, train loss: 2.4678, val loss: 2.4755\n",
      "7627: lr: 0.0050, train loss: 2.4224, val loss: 2.4471\n",
      "7628: lr: 0.0050, train loss: 2.4843, val loss: 2.4792\n",
      "7629: lr: 0.0050, train loss: 2.4268, val loss: 2.4698\n",
      "7630: lr: 0.0050, train loss: 2.4328, val loss: 2.5107\n",
      "7631: lr: 0.0050, train loss: 2.4341, val loss: 2.4681\n",
      "7632: lr: 0.0050, train loss: 2.5166, val loss: 2.4642\n",
      "7633: lr: 0.0050, train loss: 2.4544, val loss: 2.4964\n",
      "7634: lr: 0.0050, train loss: 2.4370, val loss: 2.4800\n",
      "7635: lr: 0.0050, train loss: 2.4367, val loss: 2.4620\n",
      "7636: lr: 0.0050, train loss: 2.4462, val loss: 2.4744\n",
      "7637: lr: 0.0050, train loss: 2.4824, val loss: 2.5254\n",
      "7638: lr: 0.0050, train loss: 2.4449, val loss: 2.4860\n",
      "7639: lr: 0.0050, train loss: 2.4531, val loss: 2.4684\n",
      "7640: lr: 0.0050, train loss: 2.4376, val loss: 2.4690\n",
      "7641: lr: 0.0050, train loss: 2.4420, val loss: 2.5047\n",
      "7642: lr: 0.0050, train loss: 2.4519, val loss: 2.4451\n",
      "7643: lr: 0.0050, train loss: 2.4601, val loss: 2.4353\n",
      "7644: lr: 0.0050, train loss: 2.4702, val loss: 2.4803\n",
      "7645: lr: 0.0050, train loss: 2.4789, val loss: 2.4611\n",
      "7646: lr: 0.0050, train loss: 2.4205, val loss: 2.5203\n",
      "7647: lr: 0.0050, train loss: 2.4745, val loss: 2.4842\n",
      "7648: lr: 0.0050, train loss: 2.4710, val loss: 2.5015\n",
      "7649: lr: 0.0050, train loss: 2.4366, val loss: 2.5103\n",
      "7650: lr: 0.0050, train loss: 2.3876, val loss: 2.4864\n",
      "7651: lr: 0.0050, train loss: 2.4673, val loss: 2.4514\n",
      "7652: lr: 0.0050, train loss: 2.4635, val loss: 2.4841\n",
      "7653: lr: 0.0050, train loss: 2.4241, val loss: 2.5061\n",
      "7654: lr: 0.0050, train loss: 2.4647, val loss: 2.4356\n",
      "7655: lr: 0.0050, train loss: 2.4596, val loss: 2.4376\n",
      "7656: lr: 0.0050, train loss: 2.4874, val loss: 2.5091\n",
      "7657: lr: 0.0050, train loss: 2.4783, val loss: 2.4990\n",
      "7658: lr: 0.0050, train loss: 2.4407, val loss: 2.4768\n",
      "7659: lr: 0.0050, train loss: 2.4341, val loss: 2.5362\n",
      "7660: lr: 0.0050, train loss: 2.4502, val loss: 2.4351\n",
      "7661: lr: 0.0050, train loss: 2.4456, val loss: 2.4806\n",
      "7662: lr: 0.0050, train loss: 2.4550, val loss: 2.4706\n",
      "7663: lr: 0.0050, train loss: 2.4652, val loss: 2.4942\n",
      "7664: lr: 0.0050, train loss: 2.4513, val loss: 2.4761\n",
      "7665: lr: 0.0050, train loss: 2.4904, val loss: 2.5063\n",
      "7666: lr: 0.0050, train loss: 2.4516, val loss: 2.5017\n",
      "7667: lr: 0.0050, train loss: 2.4743, val loss: 2.4992\n",
      "7668: lr: 0.0050, train loss: 2.4826, val loss: 2.5359\n",
      "7669: lr: 0.0050, train loss: 2.3888, val loss: 2.4066\n",
      "7670: lr: 0.0050, train loss: 2.4452, val loss: 2.5276\n",
      "7671: lr: 0.0050, train loss: 2.4909, val loss: 2.4672\n",
      "7672: lr: 0.0050, train loss: 2.4590, val loss: 2.4517\n",
      "7673: lr: 0.0050, train loss: 2.4581, val loss: 2.4781\n",
      "7674: lr: 0.0050, train loss: 2.4793, val loss: 2.4557\n",
      "7675: lr: 0.0050, train loss: 2.4204, val loss: 2.5128\n",
      "7676: lr: 0.0050, train loss: 2.4561, val loss: 2.5046\n",
      "7677: lr: 0.0050, train loss: 2.4321, val loss: 2.4869\n",
      "7678: lr: 0.0050, train loss: 2.4587, val loss: 2.4805\n",
      "7679: lr: 0.0050, train loss: 2.4654, val loss: 2.5298\n",
      "7680: lr: 0.0050, train loss: 2.4396, val loss: 2.5226\n",
      "7681: lr: 0.0050, train loss: 2.4633, val loss: 2.5517\n",
      "7682: lr: 0.0050, train loss: 2.4634, val loss: 2.4573\n",
      "7683: lr: 0.0050, train loss: 2.4547, val loss: 2.5001\n",
      "7684: lr: 0.0050, train loss: 2.4509, val loss: 2.4603\n",
      "7685: lr: 0.0050, train loss: 2.4630, val loss: 2.5527\n",
      "7686: lr: 0.0050, train loss: 2.4866, val loss: 2.5356\n",
      "7687: lr: 0.0050, train loss: 2.3944, val loss: 2.5035\n",
      "7688: lr: 0.0050, train loss: 2.4431, val loss: 2.4973\n",
      "7689: lr: 0.0050, train loss: 2.4437, val loss: 2.5034\n",
      "7690: lr: 0.0050, train loss: 2.4539, val loss: 2.5280\n",
      "7691: lr: 0.0050, train loss: 2.4685, val loss: 2.4354\n",
      "7692: lr: 0.0050, train loss: 2.4419, val loss: 2.5207\n",
      "7693: lr: 0.0050, train loss: 2.4596, val loss: 2.4557\n",
      "7694: lr: 0.0050, train loss: 2.4377, val loss: 2.4710\n",
      "7695: lr: 0.0050, train loss: 2.4480, val loss: 2.5344\n",
      "7696: lr: 0.0050, train loss: 2.4665, val loss: 2.5330\n",
      "7697: lr: 0.0050, train loss: 2.4655, val loss: 2.4675\n",
      "7698: lr: 0.0050, train loss: 2.4475, val loss: 2.4316\n",
      "7699: lr: 0.0050, train loss: 2.4071, val loss: 2.4065\n",
      "7700: lr: 0.0050, train loss: 2.4760, val loss: 2.5053\n",
      "7701: lr: 0.0050, train loss: 2.4119, val loss: 2.4866\n",
      "7702: lr: 0.0050, train loss: 2.4191, val loss: 2.4613\n",
      "7703: lr: 0.0050, train loss: 2.4707, val loss: 2.5171\n",
      "7704: lr: 0.0050, train loss: 2.4494, val loss: 2.4749\n",
      "7705: lr: 0.0050, train loss: 2.5048, val loss: 2.4726\n",
      "7706: lr: 0.0050, train loss: 2.4213, val loss: 2.4548\n",
      "7707: lr: 0.0050, train loss: 2.4595, val loss: 2.5204\n",
      "7708: lr: 0.0050, train loss: 2.4571, val loss: 2.4720\n",
      "7709: lr: 0.0050, train loss: 2.4934, val loss: 2.4494\n",
      "7710: lr: 0.0050, train loss: 2.4510, val loss: 2.4803\n",
      "7711: lr: 0.0050, train loss: 2.4371, val loss: 2.4868\n",
      "7712: lr: 0.0050, train loss: 2.4176, val loss: 2.4914\n",
      "7713: lr: 0.0050, train loss: 2.4810, val loss: 2.4704\n",
      "7714: lr: 0.0050, train loss: 2.4603, val loss: 2.5330\n",
      "7715: lr: 0.0050, train loss: 2.4848, val loss: 2.5044\n",
      "7716: lr: 0.0050, train loss: 2.4220, val loss: 2.4911\n",
      "7717: lr: 0.0050, train loss: 2.4699, val loss: 2.5039\n",
      "7718: lr: 0.0050, train loss: 2.4077, val loss: 2.4742\n",
      "7719: lr: 0.0050, train loss: 2.4059, val loss: 2.4463\n",
      "7720: lr: 0.0050, train loss: 2.4442, val loss: 2.4502\n",
      "7721: lr: 0.0050, train loss: 2.4593, val loss: 2.4976\n",
      "7722: lr: 0.0050, train loss: 2.4443, val loss: 2.5062\n",
      "7723: lr: 0.0050, train loss: 2.4489, val loss: 2.4584\n",
      "7724: lr: 0.0050, train loss: 2.4433, val loss: 2.5275\n",
      "7725: lr: 0.0050, train loss: 2.4394, val loss: 2.4519\n",
      "7726: lr: 0.0050, train loss: 2.4410, val loss: 2.4812\n",
      "7727: lr: 0.0050, train loss: 2.5211, val loss: 2.5076\n",
      "7728: lr: 0.0050, train loss: 2.4028, val loss: 2.4840\n",
      "7729: lr: 0.0050, train loss: 2.4467, val loss: 2.4889\n",
      "7730: lr: 0.0050, train loss: 2.4649, val loss: 2.5065\n",
      "7731: lr: 0.0050, train loss: 2.4602, val loss: 2.5287\n",
      "7732: lr: 0.0050, train loss: 2.4656, val loss: 2.4804\n",
      "7733: lr: 0.0050, train loss: 2.4576, val loss: 2.5221\n",
      "7734: lr: 0.0050, train loss: 2.4403, val loss: 2.4645\n",
      "7735: lr: 0.0050, train loss: 2.4463, val loss: 2.4592\n",
      "7736: lr: 0.0050, train loss: 2.4749, val loss: 2.4718\n",
      "7737: lr: 0.0050, train loss: 2.3996, val loss: 2.4490\n",
      "7738: lr: 0.0050, train loss: 2.4468, val loss: 2.5152\n",
      "7739: lr: 0.0050, train loss: 2.4787, val loss: 2.5366\n",
      "7740: lr: 0.0050, train loss: 2.4485, val loss: 2.5056\n",
      "7741: lr: 0.0050, train loss: 2.4845, val loss: 2.4997\n",
      "7742: lr: 0.0050, train loss: 2.4358, val loss: 2.4925\n",
      "7743: lr: 0.0050, train loss: 2.4536, val loss: 2.5247\n",
      "7744: lr: 0.0050, train loss: 2.4112, val loss: 2.4968\n",
      "7745: lr: 0.0050, train loss: 2.4976, val loss: 2.4655\n",
      "7746: lr: 0.0050, train loss: 2.4220, val loss: 2.4955\n",
      "7747: lr: 0.0050, train loss: 2.4264, val loss: 2.4753\n",
      "7748: lr: 0.0050, train loss: 2.4482, val loss: 2.4847\n",
      "7749: lr: 0.0050, train loss: 2.4610, val loss: 2.4738\n",
      "7750: lr: 0.0050, train loss: 2.4677, val loss: 2.5174\n",
      "7751: lr: 0.0050, train loss: 2.4680, val loss: 2.4984\n",
      "7752: lr: 0.0050, train loss: 2.4787, val loss: 2.4796\n",
      "7753: lr: 0.0050, train loss: 2.4281, val loss: 2.4438\n",
      "7754: lr: 0.0050, train loss: 2.4559, val loss: 2.5087\n",
      "7755: lr: 0.0050, train loss: 2.4919, val loss: 2.5302\n",
      "7756: lr: 0.0050, train loss: 2.4781, val loss: 2.4599\n",
      "7757: lr: 0.0050, train loss: 2.4733, val loss: 2.4752\n",
      "7758: lr: 0.0050, train loss: 2.4528, val loss: 2.4175\n",
      "7759: lr: 0.0050, train loss: 2.4432, val loss: 2.4739\n",
      "7760: lr: 0.0050, train loss: 2.4000, val loss: 2.4914\n",
      "7761: lr: 0.0050, train loss: 2.4374, val loss: 2.4507\n",
      "7762: lr: 0.0050, train loss: 2.4348, val loss: 2.5471\n",
      "7763: lr: 0.0050, train loss: 2.4286, val loss: 2.4757\n",
      "7764: lr: 0.0050, train loss: 2.4546, val loss: 2.4327\n",
      "7765: lr: 0.0050, train loss: 2.4397, val loss: 2.4255\n",
      "7766: lr: 0.0050, train loss: 2.4666, val loss: 2.4954\n",
      "7767: lr: 0.0050, train loss: 2.3948, val loss: 2.5196\n",
      "7768: lr: 0.0050, train loss: 2.4820, val loss: 2.4898\n",
      "7769: lr: 0.0050, train loss: 2.4380, val loss: 2.4574\n",
      "7770: lr: 0.0050, train loss: 2.4770, val loss: 2.5197\n",
      "7771: lr: 0.0050, train loss: 2.4673, val loss: 2.5040\n",
      "7772: lr: 0.0050, train loss: 2.4433, val loss: 2.4771\n",
      "7773: lr: 0.0050, train loss: 2.4565, val loss: 2.5260\n",
      "7774: lr: 0.0050, train loss: 2.4680, val loss: 2.4100\n",
      "7775: lr: 0.0050, train loss: 2.4765, val loss: 2.5383\n",
      "7776: lr: 0.0050, train loss: 2.4611, val loss: 2.5283\n",
      "7777: lr: 0.0050, train loss: 2.4892, val loss: 2.4780\n",
      "7778: lr: 0.0050, train loss: 2.4376, val loss: 2.4583\n",
      "7779: lr: 0.0050, train loss: 2.4350, val loss: 2.4928\n",
      "7780: lr: 0.0050, train loss: 2.4955, val loss: 2.4849\n",
      "7781: lr: 0.0050, train loss: 2.4609, val loss: 2.4672\n",
      "7782: lr: 0.0050, train loss: 2.4389, val loss: 2.4226\n",
      "7783: lr: 0.0050, train loss: 2.4707, val loss: 2.4450\n",
      "7784: lr: 0.0050, train loss: 2.5051, val loss: 2.5231\n",
      "7785: lr: 0.0050, train loss: 2.4591, val loss: 2.4861\n",
      "7786: lr: 0.0050, train loss: 2.4616, val loss: 2.4866\n",
      "7787: lr: 0.0050, train loss: 2.4599, val loss: 2.4736\n",
      "7788: lr: 0.0050, train loss: 2.4382, val loss: 2.5254\n",
      "7789: lr: 0.0050, train loss: 2.4964, val loss: 2.4989\n",
      "7790: lr: 0.0050, train loss: 2.4363, val loss: 2.4588\n",
      "7791: lr: 0.0050, train loss: 2.4252, val loss: 2.4768\n",
      "7792: lr: 0.0050, train loss: 2.4704, val loss: 2.4891\n",
      "7793: lr: 0.0050, train loss: 2.4201, val loss: 2.4997\n",
      "7794: lr: 0.0050, train loss: 2.4420, val loss: 2.4968\n",
      "7795: lr: 0.0050, train loss: 2.4489, val loss: 2.5066\n",
      "7796: lr: 0.0050, train loss: 2.5009, val loss: 2.4477\n",
      "7797: lr: 0.0050, train loss: 2.4595, val loss: 2.4918\n",
      "7798: lr: 0.0050, train loss: 2.4238, val loss: 2.4723\n",
      "7799: lr: 0.0050, train loss: 2.4223, val loss: 2.4810\n",
      "7800: lr: 0.0050, train loss: 2.5389, val loss: 2.4961\n",
      "7801: lr: 0.0050, train loss: 2.4396, val loss: 2.5099\n",
      "7802: lr: 0.0050, train loss: 2.4214, val loss: 2.5144\n",
      "7803: lr: 0.0050, train loss: 2.4674, val loss: 2.4881\n",
      "7804: lr: 0.0050, train loss: 2.4710, val loss: 2.4462\n",
      "7805: lr: 0.0050, train loss: 2.4235, val loss: 2.5189\n",
      "7806: lr: 0.0050, train loss: 2.4340, val loss: 2.4779\n",
      "7807: lr: 0.0050, train loss: 2.4285, val loss: 2.5037\n",
      "7808: lr: 0.0050, train loss: 2.4394, val loss: 2.5622\n",
      "7809: lr: 0.0050, train loss: 2.4491, val loss: 2.4683\n",
      "7810: lr: 0.0050, train loss: 2.4069, val loss: 2.5148\n",
      "7811: lr: 0.0050, train loss: 2.4952, val loss: 2.5341\n",
      "7812: lr: 0.0050, train loss: 2.4716, val loss: 2.4725\n",
      "7813: lr: 0.0050, train loss: 2.4569, val loss: 2.4580\n",
      "7814: lr: 0.0050, train loss: 2.3984, val loss: 2.4823\n",
      "7815: lr: 0.0050, train loss: 2.4428, val loss: 2.4895\n",
      "7816: lr: 0.0050, train loss: 2.4898, val loss: 2.4248\n",
      "7817: lr: 0.0050, train loss: 2.4141, val loss: 2.5078\n",
      "7818: lr: 0.0050, train loss: 2.4897, val loss: 2.4469\n",
      "7819: lr: 0.0050, train loss: 2.4792, val loss: 2.4950\n",
      "7820: lr: 0.0050, train loss: 2.4667, val loss: 2.4300\n",
      "7821: lr: 0.0050, train loss: 2.4037, val loss: 2.4542\n",
      "7822: lr: 0.0050, train loss: 2.4354, val loss: 2.4722\n",
      "7823: lr: 0.0050, train loss: 2.4088, val loss: 2.5029\n",
      "7824: lr: 0.0050, train loss: 2.5048, val loss: 2.4739\n",
      "7825: lr: 0.0050, train loss: 2.4806, val loss: 2.4823\n",
      "7826: lr: 0.0050, train loss: 2.4418, val loss: 2.5520\n",
      "7827: lr: 0.0050, train loss: 2.4611, val loss: 2.5035\n",
      "7828: lr: 0.0050, train loss: 2.4423, val loss: 2.4747\n",
      "7829: lr: 0.0050, train loss: 2.4207, val loss: 2.4744\n",
      "7830: lr: 0.0050, train loss: 2.4790, val loss: 2.4776\n",
      "7831: lr: 0.0050, train loss: 2.3837, val loss: 2.5586\n",
      "7832: lr: 0.0050, train loss: 2.4142, val loss: 2.4644\n",
      "7833: lr: 0.0050, train loss: 2.4250, val loss: 2.4902\n",
      "7834: lr: 0.0050, train loss: 2.4811, val loss: 2.4414\n",
      "7835: lr: 0.0050, train loss: 2.4768, val loss: 2.4653\n",
      "7836: lr: 0.0050, train loss: 2.4066, val loss: 2.4765\n",
      "7837: lr: 0.0050, train loss: 2.4190, val loss: 2.4603\n",
      "7838: lr: 0.0050, train loss: 2.3910, val loss: 2.5019\n",
      "7839: lr: 0.0050, train loss: 2.4182, val loss: 2.4977\n",
      "7840: lr: 0.0050, train loss: 2.4782, val loss: 2.4702\n",
      "7841: lr: 0.0050, train loss: 2.5123, val loss: 2.5329\n",
      "7842: lr: 0.0050, train loss: 2.4750, val loss: 2.5055\n",
      "7843: lr: 0.0050, train loss: 2.4130, val loss: 2.5273\n",
      "7844: lr: 0.0050, train loss: 2.4518, val loss: 2.4864\n",
      "7845: lr: 0.0050, train loss: 2.4589, val loss: 2.4396\n",
      "7846: lr: 0.0050, train loss: 2.4493, val loss: 2.4796\n",
      "7847: lr: 0.0050, train loss: 2.5016, val loss: 2.4848\n",
      "7848: lr: 0.0050, train loss: 2.4498, val loss: 2.4683\n",
      "7849: lr: 0.0050, train loss: 2.4723, val loss: 2.4857\n",
      "7850: lr: 0.0050, train loss: 2.4582, val loss: 2.4977\n",
      "7851: lr: 0.0050, train loss: 2.4724, val loss: 2.4848\n",
      "7852: lr: 0.0050, train loss: 2.4591, val loss: 2.5035\n",
      "7853: lr: 0.0050, train loss: 2.5087, val loss: 2.5929\n",
      "7854: lr: 0.0050, train loss: 2.4962, val loss: 2.4981\n",
      "7855: lr: 0.0050, train loss: 2.4302, val loss: 2.4668\n",
      "7856: lr: 0.0050, train loss: 2.4460, val loss: 2.4644\n",
      "7857: lr: 0.0050, train loss: 2.4436, val loss: 2.3879\n",
      "7858: lr: 0.0050, train loss: 2.4208, val loss: 2.4915\n",
      "7859: lr: 0.0050, train loss: 2.4360, val loss: 2.4351\n",
      "7860: lr: 0.0050, train loss: 2.4154, val loss: 2.4931\n",
      "7861: lr: 0.0050, train loss: 2.4959, val loss: 2.4989\n",
      "7862: lr: 0.0050, train loss: 2.4552, val loss: 2.4629\n",
      "7863: lr: 0.0050, train loss: 2.4598, val loss: 2.4672\n",
      "7864: lr: 0.0050, train loss: 2.4449, val loss: 2.4487\n",
      "7865: lr: 0.0050, train loss: 2.4850, val loss: 2.4834\n",
      "7866: lr: 0.0050, train loss: 2.4433, val loss: 2.4595\n",
      "7867: lr: 0.0050, train loss: 2.4501, val loss: 2.4741\n",
      "7868: lr: 0.0050, train loss: 2.4531, val loss: 2.4536\n",
      "7869: lr: 0.0050, train loss: 2.4869, val loss: 2.4645\n",
      "7870: lr: 0.0050, train loss: 2.4757, val loss: 2.4537\n",
      "7871: lr: 0.0050, train loss: 2.4406, val loss: 2.4728\n",
      "7872: lr: 0.0050, train loss: 2.4948, val loss: 2.4309\n",
      "7873: lr: 0.0050, train loss: 2.4950, val loss: 2.5203\n",
      "7874: lr: 0.0050, train loss: 2.4689, val loss: 2.4859\n",
      "7875: lr: 0.0050, train loss: 2.4892, val loss: 2.4968\n",
      "7876: lr: 0.0050, train loss: 2.4874, val loss: 2.4350\n",
      "7877: lr: 0.0050, train loss: 2.4430, val loss: 2.4832\n",
      "7878: lr: 0.0050, train loss: 2.4668, val loss: 2.4728\n",
      "7879: lr: 0.0050, train loss: 2.4721, val loss: 2.4783\n",
      "7880: lr: 0.0050, train loss: 2.4323, val loss: 2.4774\n",
      "7881: lr: 0.0050, train loss: 2.4790, val loss: 2.4720\n",
      "7882: lr: 0.0050, train loss: 2.4676, val loss: 2.4739\n",
      "7883: lr: 0.0050, train loss: 2.4732, val loss: 2.5041\n",
      "7884: lr: 0.0050, train loss: 2.4673, val loss: 2.5175\n",
      "7885: lr: 0.0050, train loss: 2.4332, val loss: 2.4698\n",
      "7886: lr: 0.0050, train loss: 2.4369, val loss: 2.4722\n",
      "7887: lr: 0.0050, train loss: 2.4886, val loss: 2.4783\n",
      "7888: lr: 0.0050, train loss: 2.4135, val loss: 2.5197\n",
      "7889: lr: 0.0050, train loss: 2.4586, val loss: 2.4496\n",
      "7890: lr: 0.0050, train loss: 2.4290, val loss: 2.4940\n",
      "7891: lr: 0.0050, train loss: 2.4967, val loss: 2.5067\n",
      "7892: lr: 0.0050, train loss: 2.4783, val loss: 2.4474\n",
      "7893: lr: 0.0050, train loss: 2.4716, val loss: 2.5285\n",
      "7894: lr: 0.0050, train loss: 2.4423, val loss: 2.5261\n",
      "7895: lr: 0.0050, train loss: 2.5053, val loss: 2.5101\n",
      "7896: lr: 0.0050, train loss: 2.4256, val loss: 2.5331\n",
      "7897: lr: 0.0050, train loss: 2.4654, val loss: 2.4937\n",
      "7898: lr: 0.0050, train loss: 2.4887, val loss: 2.4572\n",
      "7899: lr: 0.0050, train loss: 2.4712, val loss: 2.4823\n",
      "7900: lr: 0.0050, train loss: 2.4319, val loss: 2.4735\n",
      "7901: lr: 0.0050, train loss: 2.4815, val loss: 2.5067\n",
      "7902: lr: 0.0050, train loss: 2.4515, val loss: 2.4958\n",
      "7903: lr: 0.0050, train loss: 2.4655, val loss: 2.5293\n",
      "7904: lr: 0.0050, train loss: 2.4358, val loss: 2.4827\n",
      "7905: lr: 0.0050, train loss: 2.4380, val loss: 2.4722\n",
      "7906: lr: 0.0050, train loss: 2.4773, val loss: 2.4619\n",
      "7907: lr: 0.0050, train loss: 2.5032, val loss: 2.5144\n",
      "7908: lr: 0.0050, train loss: 2.4586, val loss: 2.4741\n",
      "7909: lr: 0.0050, train loss: 2.4919, val loss: 2.5077\n",
      "7910: lr: 0.0050, train loss: 2.4809, val loss: 2.5033\n",
      "7911: lr: 0.0050, train loss: 2.4289, val loss: 2.5133\n",
      "7912: lr: 0.0050, train loss: 2.4685, val loss: 2.5374\n",
      "7913: lr: 0.0050, train loss: 2.4425, val loss: 2.4788\n",
      "7914: lr: 0.0050, train loss: 2.4482, val loss: 2.4866\n",
      "7915: lr: 0.0050, train loss: 2.4351, val loss: 2.4086\n",
      "7916: lr: 0.0050, train loss: 2.4644, val loss: 2.4445\n",
      "7917: lr: 0.0050, train loss: 2.3949, val loss: 2.4975\n",
      "7918: lr: 0.0050, train loss: 2.4346, val loss: 2.5198\n",
      "7919: lr: 0.0050, train loss: 2.4580, val loss: 2.5264\n",
      "7920: lr: 0.0050, train loss: 2.4770, val loss: 2.4823\n",
      "7921: lr: 0.0050, train loss: 2.4393, val loss: 2.4657\n",
      "7922: lr: 0.0050, train loss: 2.4743, val loss: 2.4758\n",
      "7923: lr: 0.0050, train loss: 2.4702, val loss: 2.4553\n",
      "7924: lr: 0.0050, train loss: 2.4282, val loss: 2.4991\n",
      "7925: lr: 0.0050, train loss: 2.4781, val loss: 2.5451\n",
      "7926: lr: 0.0050, train loss: 2.4755, val loss: 2.4914\n",
      "7927: lr: 0.0050, train loss: 2.4763, val loss: 2.5186\n",
      "7928: lr: 0.0050, train loss: 2.4416, val loss: 2.4775\n",
      "7929: lr: 0.0050, train loss: 2.4210, val loss: 2.4402\n",
      "7930: lr: 0.0050, train loss: 2.4451, val loss: 2.4368\n",
      "7931: lr: 0.0050, train loss: 2.3963, val loss: 2.4404\n",
      "7932: lr: 0.0050, train loss: 2.4379, val loss: 2.4612\n",
      "7933: lr: 0.0050, train loss: 2.4218, val loss: 2.5095\n",
      "7934: lr: 0.0050, train loss: 2.4728, val loss: 2.4607\n",
      "7935: lr: 0.0050, train loss: 2.4971, val loss: 2.4730\n",
      "7936: lr: 0.0050, train loss: 2.4409, val loss: 2.5448\n",
      "7937: lr: 0.0050, train loss: 2.4461, val loss: 2.4771\n",
      "7938: lr: 0.0050, train loss: 2.4248, val loss: 2.4893\n",
      "7939: lr: 0.0050, train loss: 2.4404, val loss: 2.5224\n",
      "7940: lr: 0.0050, train loss: 2.4373, val loss: 2.4627\n",
      "7941: lr: 0.0050, train loss: 2.4706, val loss: 2.4632\n",
      "7942: lr: 0.0050, train loss: 2.4569, val loss: 2.4412\n",
      "7943: lr: 0.0050, train loss: 2.4800, val loss: 2.5117\n",
      "7944: lr: 0.0050, train loss: 2.4734, val loss: 2.4814\n",
      "7945: lr: 0.0050, train loss: 2.4257, val loss: 2.5058\n",
      "7946: lr: 0.0050, train loss: 2.4541, val loss: 2.4230\n",
      "7947: lr: 0.0050, train loss: 2.4780, val loss: 2.5090\n",
      "7948: lr: 0.0050, train loss: 2.4686, val loss: 2.4850\n",
      "7949: lr: 0.0050, train loss: 2.4357, val loss: 2.4798\n",
      "7950: lr: 0.0050, train loss: 2.4827, val loss: 2.4510\n",
      "7951: lr: 0.0050, train loss: 2.4471, val loss: 2.5261\n",
      "7952: lr: 0.0050, train loss: 2.4150, val loss: 2.5732\n",
      "7953: lr: 0.0050, train loss: 2.4414, val loss: 2.5414\n",
      "7954: lr: 0.0050, train loss: 2.4469, val loss: 2.5007\n",
      "7955: lr: 0.0050, train loss: 2.4421, val loss: 2.4482\n",
      "7956: lr: 0.0050, train loss: 2.4009, val loss: 2.4486\n",
      "7957: lr: 0.0050, train loss: 2.4420, val loss: 2.4849\n",
      "7958: lr: 0.0050, train loss: 2.4813, val loss: 2.5095\n",
      "7959: lr: 0.0050, train loss: 2.4270, val loss: 2.5411\n",
      "7960: lr: 0.0050, train loss: 2.4519, val loss: 2.4458\n",
      "7961: lr: 0.0050, train loss: 2.4207, val loss: 2.4690\n",
      "7962: lr: 0.0050, train loss: 2.4490, val loss: 2.4889\n",
      "7963: lr: 0.0050, train loss: 2.4567, val loss: 2.4612\n",
      "7964: lr: 0.0050, train loss: 2.4414, val loss: 2.5074\n",
      "7965: lr: 0.0050, train loss: 2.4500, val loss: 2.4709\n",
      "7966: lr: 0.0050, train loss: 2.4950, val loss: 2.4703\n",
      "7967: lr: 0.0050, train loss: 2.5007, val loss: 2.5033\n",
      "7968: lr: 0.0050, train loss: 2.4408, val loss: 2.4999\n",
      "7969: lr: 0.0050, train loss: 2.4291, val loss: 2.4799\n",
      "7970: lr: 0.0050, train loss: 2.4986, val loss: 2.4620\n",
      "7971: lr: 0.0050, train loss: 2.4928, val loss: 2.4763\n",
      "7972: lr: 0.0050, train loss: 2.4376, val loss: 2.5111\n",
      "7973: lr: 0.0050, train loss: 2.4576, val loss: 2.4891\n",
      "7974: lr: 0.0050, train loss: 2.4200, val loss: 2.4620\n",
      "7975: lr: 0.0050, train loss: 2.4718, val loss: 2.4337\n",
      "7976: lr: 0.0050, train loss: 2.4471, val loss: 2.4732\n",
      "7977: lr: 0.0050, train loss: 2.4920, val loss: 2.5276\n",
      "7978: lr: 0.0050, train loss: 2.4808, val loss: 2.4602\n",
      "7979: lr: 0.0050, train loss: 2.4821, val loss: 2.5023\n",
      "7980: lr: 0.0050, train loss: 2.4532, val loss: 2.4957\n",
      "7981: lr: 0.0050, train loss: 2.4338, val loss: 2.4567\n",
      "7982: lr: 0.0050, train loss: 2.4395, val loss: 2.5083\n",
      "7983: lr: 0.0050, train loss: 2.4194, val loss: 2.5295\n",
      "7984: lr: 0.0050, train loss: 2.4666, val loss: 2.4495\n",
      "7985: lr: 0.0050, train loss: 2.4772, val loss: 2.5003\n",
      "7986: lr: 0.0050, train loss: 2.4393, val loss: 2.4688\n",
      "7987: lr: 0.0050, train loss: 2.4655, val loss: 2.4707\n",
      "7988: lr: 0.0050, train loss: 2.5149, val loss: 2.4565\n",
      "7989: lr: 0.0050, train loss: 2.4643, val loss: 2.5172\n",
      "7990: lr: 0.0050, train loss: 2.4696, val loss: 2.5183\n",
      "7991: lr: 0.0050, train loss: 2.4777, val loss: 2.4878\n",
      "7992: lr: 0.0050, train loss: 2.4376, val loss: 2.4772\n",
      "7993: lr: 0.0050, train loss: 2.4485, val loss: 2.5395\n",
      "7994: lr: 0.0050, train loss: 2.4559, val loss: 2.4497\n",
      "7995: lr: 0.0050, train loss: 2.5289, val loss: 2.4809\n",
      "7996: lr: 0.0050, train loss: 2.4488, val loss: 2.4840\n",
      "7997: lr: 0.0050, train loss: 2.4588, val loss: 2.4723\n",
      "7998: lr: 0.0050, train loss: 2.4157, val loss: 2.4786\n",
      "7999: lr: 0.0050, train loss: 2.4578, val loss: 2.4744\n",
      "8000: lr: 0.0025, train loss: 2.4867, val loss: 2.5514\n",
      "8001: lr: 0.0025, train loss: 2.4529, val loss: 2.5282\n",
      "8002: lr: 0.0025, train loss: 2.4427, val loss: 2.4525\n",
      "8003: lr: 0.0025, train loss: 2.4580, val loss: 2.4975\n",
      "8004: lr: 0.0025, train loss: 2.4759, val loss: 2.5058\n",
      "8005: lr: 0.0025, train loss: 2.4507, val loss: 2.4505\n",
      "8006: lr: 0.0025, train loss: 2.3975, val loss: 2.5533\n",
      "8007: lr: 0.0025, train loss: 2.5041, val loss: 2.4632\n",
      "8008: lr: 0.0025, train loss: 2.4885, val loss: 2.4397\n",
      "8009: lr: 0.0025, train loss: 2.4501, val loss: 2.4471\n",
      "8010: lr: 0.0025, train loss: 2.4269, val loss: 2.4992\n",
      "8011: lr: 0.0025, train loss: 2.4314, val loss: 2.5133\n",
      "8012: lr: 0.0025, train loss: 2.4574, val loss: 2.4892\n",
      "8013: lr: 0.0025, train loss: 2.4306, val loss: 2.4618\n",
      "8014: lr: 0.0025, train loss: 2.4590, val loss: 2.4879\n",
      "8015: lr: 0.0025, train loss: 2.4066, val loss: 2.4466\n",
      "8016: lr: 0.0025, train loss: 2.4518, val loss: 2.4651\n",
      "8017: lr: 0.0025, train loss: 2.4538, val loss: 2.4948\n",
      "8018: lr: 0.0025, train loss: 2.4703, val loss: 2.4854\n",
      "8019: lr: 0.0025, train loss: 2.4771, val loss: 2.4300\n",
      "8020: lr: 0.0025, train loss: 2.4433, val loss: 2.4414\n",
      "8021: lr: 0.0025, train loss: 2.4694, val loss: 2.4522\n",
      "8022: lr: 0.0025, train loss: 2.4481, val loss: 2.4735\n",
      "8023: lr: 0.0025, train loss: 2.4394, val loss: 2.4842\n",
      "8024: lr: 0.0025, train loss: 2.4492, val loss: 2.5288\n",
      "8025: lr: 0.0025, train loss: 2.4320, val loss: 2.5532\n",
      "8026: lr: 0.0025, train loss: 2.4610, val loss: 2.5424\n",
      "8027: lr: 0.0025, train loss: 2.4693, val loss: 2.4546\n",
      "8028: lr: 0.0025, train loss: 2.4368, val loss: 2.4902\n",
      "8029: lr: 0.0025, train loss: 2.4527, val loss: 2.4597\n",
      "8030: lr: 0.0025, train loss: 2.4791, val loss: 2.4985\n",
      "8031: lr: 0.0025, train loss: 2.4280, val loss: 2.4885\n",
      "8032: lr: 0.0025, train loss: 2.4589, val loss: 2.4650\n",
      "8033: lr: 0.0025, train loss: 2.4762, val loss: 2.5345\n",
      "8034: lr: 0.0025, train loss: 2.4651, val loss: 2.4428\n",
      "8035: lr: 0.0025, train loss: 2.4517, val loss: 2.5005\n",
      "8036: lr: 0.0025, train loss: 2.4442, val loss: 2.5063\n",
      "8037: lr: 0.0025, train loss: 2.4587, val loss: 2.4862\n",
      "8038: lr: 0.0025, train loss: 2.4482, val loss: 2.4729\n",
      "8039: lr: 0.0025, train loss: 2.4306, val loss: 2.5410\n",
      "8040: lr: 0.0025, train loss: 2.4617, val loss: 2.4901\n",
      "8041: lr: 0.0025, train loss: 2.4133, val loss: 2.4549\n",
      "8042: lr: 0.0025, train loss: 2.4737, val loss: 2.5216\n",
      "8043: lr: 0.0025, train loss: 2.4276, val loss: 2.4846\n",
      "8044: lr: 0.0025, train loss: 2.4170, val loss: 2.4869\n",
      "8045: lr: 0.0025, train loss: 2.4953, val loss: 2.4872\n",
      "8046: lr: 0.0025, train loss: 2.4665, val loss: 2.4600\n",
      "8047: lr: 0.0025, train loss: 2.5065, val loss: 2.4848\n",
      "8048: lr: 0.0025, train loss: 2.4330, val loss: 2.4835\n",
      "8049: lr: 0.0025, train loss: 2.4445, val loss: 2.5010\n",
      "8050: lr: 0.0025, train loss: 2.4325, val loss: 2.5310\n",
      "8051: lr: 0.0025, train loss: 2.4318, val loss: 2.5040\n",
      "8052: lr: 0.0025, train loss: 2.4702, val loss: 2.4637\n",
      "8053: lr: 0.0025, train loss: 2.4057, val loss: 2.4910\n",
      "8054: lr: 0.0025, train loss: 2.4335, val loss: 2.4978\n",
      "8055: lr: 0.0025, train loss: 2.4427, val loss: 2.4529\n",
      "8056: lr: 0.0025, train loss: 2.4174, val loss: 2.5334\n",
      "8057: lr: 0.0025, train loss: 2.4001, val loss: 2.4743\n",
      "8058: lr: 0.0025, train loss: 2.4378, val loss: 2.4725\n",
      "8059: lr: 0.0025, train loss: 2.4522, val loss: 2.4816\n",
      "8060: lr: 0.0025, train loss: 2.4814, val loss: 2.4853\n",
      "8061: lr: 0.0025, train loss: 2.4315, val loss: 2.5235\n",
      "8062: lr: 0.0025, train loss: 2.4667, val loss: 2.4448\n",
      "8063: lr: 0.0025, train loss: 2.4421, val loss: 2.4656\n",
      "8064: lr: 0.0025, train loss: 2.4477, val loss: 2.5304\n",
      "8065: lr: 0.0025, train loss: 2.4292, val loss: 2.4912\n",
      "8066: lr: 0.0025, train loss: 2.4945, val loss: 2.5251\n",
      "8067: lr: 0.0025, train loss: 2.4228, val loss: 2.4411\n",
      "8068: lr: 0.0025, train loss: 2.4323, val loss: 2.4609\n",
      "8069: lr: 0.0025, train loss: 2.4494, val loss: 2.4434\n",
      "8070: lr: 0.0025, train loss: 2.4259, val loss: 2.4779\n",
      "8071: lr: 0.0025, train loss: 2.4752, val loss: 2.4483\n",
      "8072: lr: 0.0025, train loss: 2.4613, val loss: 2.5297\n",
      "8073: lr: 0.0025, train loss: 2.4830, val loss: 2.4861\n",
      "8074: lr: 0.0025, train loss: 2.4323, val loss: 2.4936\n",
      "8075: lr: 0.0025, train loss: 2.4551, val loss: 2.5103\n",
      "8076: lr: 0.0025, train loss: 2.4679, val loss: 2.5250\n",
      "8077: lr: 0.0025, train loss: 2.4795, val loss: 2.4581\n",
      "8078: lr: 0.0025, train loss: 2.4745, val loss: 2.4061\n",
      "8079: lr: 0.0025, train loss: 2.4796, val loss: 2.4553\n",
      "8080: lr: 0.0025, train loss: 2.4527, val loss: 2.4645\n",
      "8081: lr: 0.0025, train loss: 2.4566, val loss: 2.4651\n",
      "8082: lr: 0.0025, train loss: 2.4605, val loss: 2.4694\n",
      "8083: lr: 0.0025, train loss: 2.4929, val loss: 2.4641\n",
      "8084: lr: 0.0025, train loss: 2.4682, val loss: 2.4601\n",
      "8085: lr: 0.0025, train loss: 2.4644, val loss: 2.5195\n",
      "8086: lr: 0.0025, train loss: 2.4183, val loss: 2.5134\n",
      "8087: lr: 0.0025, train loss: 2.4748, val loss: 2.5403\n",
      "8088: lr: 0.0025, train loss: 2.4377, val loss: 2.5222\n",
      "8089: lr: 0.0025, train loss: 2.5684, val loss: 2.5067\n",
      "8090: lr: 0.0025, train loss: 2.4461, val loss: 2.4446\n",
      "8091: lr: 0.0025, train loss: 2.4396, val loss: 2.4998\n",
      "8092: lr: 0.0025, train loss: 2.4753, val loss: 2.5147\n",
      "8093: lr: 0.0025, train loss: 2.4245, val loss: 2.5177\n",
      "8094: lr: 0.0025, train loss: 2.4157, val loss: 2.4831\n",
      "8095: lr: 0.0025, train loss: 2.4858, val loss: 2.4561\n",
      "8096: lr: 0.0025, train loss: 2.4876, val loss: 2.4943\n",
      "8097: lr: 0.0025, train loss: 2.4386, val loss: 2.4342\n",
      "8098: lr: 0.0025, train loss: 2.4783, val loss: 2.4623\n",
      "8099: lr: 0.0025, train loss: 2.4204, val loss: 2.5353\n",
      "8100: lr: 0.0025, train loss: 2.4898, val loss: 2.4651\n",
      "8101: lr: 0.0025, train loss: 2.4201, val loss: 2.4734\n",
      "8102: lr: 0.0025, train loss: 2.4521, val loss: 2.4360\n",
      "8103: lr: 0.0025, train loss: 2.4222, val loss: 2.4229\n",
      "8104: lr: 0.0025, train loss: 2.4418, val loss: 2.4925\n",
      "8105: lr: 0.0025, train loss: 2.4359, val loss: 2.5120\n",
      "8106: lr: 0.0025, train loss: 2.4441, val loss: 2.5201\n",
      "8107: lr: 0.0025, train loss: 2.4268, val loss: 2.4753\n",
      "8108: lr: 0.0025, train loss: 2.4357, val loss: 2.5181\n",
      "8109: lr: 0.0025, train loss: 2.4080, val loss: 2.4753\n",
      "8110: lr: 0.0025, train loss: 2.4308, val loss: 2.4878\n",
      "8111: lr: 0.0025, train loss: 2.4705, val loss: 2.4864\n",
      "8112: lr: 0.0025, train loss: 2.4212, val loss: 2.5324\n",
      "8113: lr: 0.0025, train loss: 2.4694, val loss: 2.4487\n",
      "8114: lr: 0.0025, train loss: 2.4294, val loss: 2.4677\n",
      "8115: lr: 0.0025, train loss: 2.4489, val loss: 2.5061\n",
      "8116: lr: 0.0025, train loss: 2.4561, val loss: 2.4973\n",
      "8117: lr: 0.0025, train loss: 2.4382, val loss: 2.4932\n",
      "8118: lr: 0.0025, train loss: 2.4643, val loss: 2.5274\n",
      "8119: lr: 0.0025, train loss: 2.4912, val loss: 2.5414\n",
      "8120: lr: 0.0025, train loss: 2.4423, val loss: 2.4831\n",
      "8121: lr: 0.0025, train loss: 2.4529, val loss: 2.4678\n",
      "8122: lr: 0.0025, train loss: 2.4531, val loss: 2.5044\n",
      "8123: lr: 0.0025, train loss: 2.4843, val loss: 2.5256\n",
      "8124: lr: 0.0025, train loss: 2.4668, val loss: 2.4824\n",
      "8125: lr: 0.0025, train loss: 2.4706, val loss: 2.4872\n",
      "8126: lr: 0.0025, train loss: 2.5048, val loss: 2.4818\n",
      "8127: lr: 0.0025, train loss: 2.4508, val loss: 2.4418\n",
      "8128: lr: 0.0025, train loss: 2.4722, val loss: 2.4243\n",
      "8129: lr: 0.0025, train loss: 2.4679, val loss: 2.4617\n",
      "8130: lr: 0.0025, train loss: 2.4900, val loss: 2.4945\n",
      "8131: lr: 0.0025, train loss: 2.4610, val loss: 2.5708\n",
      "8132: lr: 0.0025, train loss: 2.4931, val loss: 2.4953\n",
      "8133: lr: 0.0025, train loss: 2.4291, val loss: 2.5068\n",
      "8134: lr: 0.0025, train loss: 2.4172, val loss: 2.4889\n",
      "8135: lr: 0.0025, train loss: 2.4587, val loss: 2.4978\n",
      "8136: lr: 0.0025, train loss: 2.4368, val loss: 2.4681\n",
      "8137: lr: 0.0025, train loss: 2.4522, val loss: 2.4386\n",
      "8138: lr: 0.0025, train loss: 2.4593, val loss: 2.4578\n",
      "8139: lr: 0.0025, train loss: 2.4277, val loss: 2.4998\n",
      "8140: lr: 0.0025, train loss: 2.4212, val loss: 2.4811\n",
      "8141: lr: 0.0025, train loss: 2.4454, val loss: 2.4896\n",
      "8142: lr: 0.0025, train loss: 2.4862, val loss: 2.5078\n",
      "8143: lr: 0.0025, train loss: 2.4515, val loss: 2.4427\n",
      "8144: lr: 0.0025, train loss: 2.4542, val loss: 2.4983\n",
      "8145: lr: 0.0025, train loss: 2.4053, val loss: 2.5059\n",
      "8146: lr: 0.0025, train loss: 2.4355, val loss: 2.5363\n",
      "8147: lr: 0.0025, train loss: 2.4838, val loss: 2.4672\n",
      "8148: lr: 0.0025, train loss: 2.4820, val loss: 2.5356\n",
      "8149: lr: 0.0025, train loss: 2.4219, val loss: 2.5230\n",
      "8150: lr: 0.0025, train loss: 2.4487, val loss: 2.4850\n",
      "8151: lr: 0.0025, train loss: 2.4884, val loss: 2.4858\n",
      "8152: lr: 0.0025, train loss: 2.4431, val loss: 2.4940\n",
      "8153: lr: 0.0025, train loss: 2.4204, val loss: 2.4722\n",
      "8154: lr: 0.0025, train loss: 2.4662, val loss: 2.5124\n",
      "8155: lr: 0.0025, train loss: 2.4694, val loss: 2.4857\n",
      "8156: lr: 0.0025, train loss: 2.4232, val loss: 2.4759\n",
      "8157: lr: 0.0025, train loss: 2.4482, val loss: 2.4711\n",
      "8158: lr: 0.0025, train loss: 2.5210, val loss: 2.5170\n",
      "8159: lr: 0.0025, train loss: 2.4770, val loss: 2.4516\n",
      "8160: lr: 0.0025, train loss: 2.4319, val loss: 2.5065\n",
      "8161: lr: 0.0025, train loss: 2.4619, val loss: 2.4188\n",
      "8162: lr: 0.0025, train loss: 2.4548, val loss: 2.5698\n",
      "8163: lr: 0.0025, train loss: 2.4463, val loss: 2.4927\n",
      "8164: lr: 0.0025, train loss: 2.4686, val loss: 2.4601\n",
      "8165: lr: 0.0025, train loss: 2.4414, val loss: 2.5127\n",
      "8166: lr: 0.0025, train loss: 2.4598, val loss: 2.4665\n",
      "8167: lr: 0.0025, train loss: 2.4660, val loss: 2.4879\n",
      "8168: lr: 0.0025, train loss: 2.4659, val loss: 2.4772\n",
      "8169: lr: 0.0025, train loss: 2.4548, val loss: 2.4916\n",
      "8170: lr: 0.0025, train loss: 2.4817, val loss: 2.5005\n",
      "8171: lr: 0.0025, train loss: 2.4868, val loss: 2.5136\n",
      "8172: lr: 0.0025, train loss: 2.4497, val loss: 2.4659\n",
      "8173: lr: 0.0025, train loss: 2.4650, val loss: 2.4550\n",
      "8174: lr: 0.0025, train loss: 2.4727, val loss: 2.4614\n",
      "8175: lr: 0.0025, train loss: 2.3942, val loss: 2.4897\n",
      "8176: lr: 0.0025, train loss: 2.4896, val loss: 2.4840\n",
      "8177: lr: 0.0025, train loss: 2.4430, val loss: 2.4801\n",
      "8178: lr: 0.0025, train loss: 2.4589, val loss: 2.5329\n",
      "8179: lr: 0.0025, train loss: 2.4781, val loss: 2.4744\n",
      "8180: lr: 0.0025, train loss: 2.4560, val loss: 2.4981\n",
      "8181: lr: 0.0025, train loss: 2.4547, val loss: 2.4604\n",
      "8182: lr: 0.0025, train loss: 2.4660, val loss: 2.5148\n",
      "8183: lr: 0.0025, train loss: 2.4740, val loss: 2.4670\n",
      "8184: lr: 0.0025, train loss: 2.4451, val loss: 2.4971\n",
      "8185: lr: 0.0025, train loss: 2.4252, val loss: 2.4692\n",
      "8186: lr: 0.0025, train loss: 2.4351, val loss: 2.4442\n",
      "8187: lr: 0.0025, train loss: 2.4413, val loss: 2.4674\n",
      "8188: lr: 0.0025, train loss: 2.4887, val loss: 2.4931\n",
      "8189: lr: 0.0025, train loss: 2.4860, val loss: 2.4948\n",
      "8190: lr: 0.0025, train loss: 2.4746, val loss: 2.5165\n",
      "8191: lr: 0.0025, train loss: 2.4719, val loss: 2.4600\n",
      "8192: lr: 0.0025, train loss: 2.4113, val loss: 2.4731\n",
      "8193: lr: 0.0025, train loss: 2.4514, val loss: 2.4989\n",
      "8194: lr: 0.0025, train loss: 2.4966, val loss: 2.4903\n",
      "8195: lr: 0.0025, train loss: 2.5045, val loss: 2.5098\n",
      "8196: lr: 0.0025, train loss: 2.4565, val loss: 2.5305\n",
      "8197: lr: 0.0025, train loss: 2.4339, val loss: 2.5249\n",
      "8198: lr: 0.0025, train loss: 2.4355, val loss: 2.4243\n",
      "8199: lr: 0.0025, train loss: 2.4152, val loss: 2.4679\n",
      "8200: lr: 0.0025, train loss: 2.5168, val loss: 2.4783\n",
      "8201: lr: 0.0025, train loss: 2.4652, val loss: 2.5057\n",
      "8202: lr: 0.0025, train loss: 2.4500, val loss: 2.4622\n",
      "8203: lr: 0.0025, train loss: 2.4668, val loss: 2.5409\n",
      "8204: lr: 0.0025, train loss: 2.4433, val loss: 2.4789\n",
      "8205: lr: 0.0025, train loss: 2.4214, val loss: 2.4802\n",
      "8206: lr: 0.0025, train loss: 2.4927, val loss: 2.4455\n",
      "8207: lr: 0.0025, train loss: 2.4839, val loss: 2.4564\n",
      "8208: lr: 0.0025, train loss: 2.4615, val loss: 2.4719\n",
      "8209: lr: 0.0025, train loss: 2.4675, val loss: 2.4802\n",
      "8210: lr: 0.0025, train loss: 2.4625, val loss: 2.4339\n",
      "8211: lr: 0.0025, train loss: 2.4664, val loss: 2.4504\n",
      "8212: lr: 0.0025, train loss: 2.3685, val loss: 2.4699\n",
      "8213: lr: 0.0025, train loss: 2.4351, val loss: 2.4939\n",
      "8214: lr: 0.0025, train loss: 2.4200, val loss: 2.4865\n",
      "8215: lr: 0.0025, train loss: 2.4550, val loss: 2.5439\n",
      "8216: lr: 0.0025, train loss: 2.4432, val loss: 2.5288\n",
      "8217: lr: 0.0025, train loss: 2.4841, val loss: 2.5282\n",
      "8218: lr: 0.0025, train loss: 2.4279, val loss: 2.5052\n",
      "8219: lr: 0.0025, train loss: 2.4031, val loss: 2.4856\n",
      "8220: lr: 0.0025, train loss: 2.4529, val loss: 2.4727\n",
      "8221: lr: 0.0025, train loss: 2.5149, val loss: 2.5095\n",
      "8222: lr: 0.0025, train loss: 2.4803, val loss: 2.4602\n",
      "8223: lr: 0.0025, train loss: 2.4329, val loss: 2.4840\n",
      "8224: lr: 0.0025, train loss: 2.4636, val loss: 2.4935\n",
      "8225: lr: 0.0025, train loss: 2.4226, val loss: 2.5091\n",
      "8226: lr: 0.0025, train loss: 2.4650, val loss: 2.5140\n",
      "8227: lr: 0.0025, train loss: 2.4348, val loss: 2.4398\n",
      "8228: lr: 0.0025, train loss: 2.4413, val loss: 2.4877\n",
      "8229: lr: 0.0025, train loss: 2.4708, val loss: 2.4772\n",
      "8230: lr: 0.0025, train loss: 2.4574, val loss: 2.4868\n",
      "8231: lr: 0.0025, train loss: 2.4661, val loss: 2.4558\n",
      "8232: lr: 0.0025, train loss: 2.4523, val loss: 2.4785\n",
      "8233: lr: 0.0025, train loss: 2.4416, val loss: 2.4668\n",
      "8234: lr: 0.0025, train loss: 2.4406, val loss: 2.5305\n",
      "8235: lr: 0.0025, train loss: 2.4768, val loss: 2.4979\n",
      "8236: lr: 0.0025, train loss: 2.4548, val loss: 2.4684\n",
      "8237: lr: 0.0025, train loss: 2.5203, val loss: 2.4772\n",
      "8238: lr: 0.0025, train loss: 2.4492, val loss: 2.4987\n",
      "8239: lr: 0.0025, train loss: 2.4694, val loss: 2.5749\n",
      "8240: lr: 0.0025, train loss: 2.4413, val loss: 2.5132\n",
      "8241: lr: 0.0025, train loss: 2.4661, val loss: 2.4652\n",
      "8242: lr: 0.0025, train loss: 2.5106, val loss: 2.4774\n",
      "8243: lr: 0.0025, train loss: 2.4557, val loss: 2.4811\n",
      "8244: lr: 0.0025, train loss: 2.4509, val loss: 2.4654\n",
      "8245: lr: 0.0025, train loss: 2.4238, val loss: 2.4927\n",
      "8246: lr: 0.0025, train loss: 2.4452, val loss: 2.4123\n",
      "8247: lr: 0.0025, train loss: 2.5016, val loss: 2.4512\n",
      "8248: lr: 0.0025, train loss: 2.4926, val loss: 2.5173\n",
      "8249: lr: 0.0025, train loss: 2.4396, val loss: 2.4944\n",
      "8250: lr: 0.0025, train loss: 2.4864, val loss: 2.4636\n",
      "8251: lr: 0.0025, train loss: 2.4139, val loss: 2.5019\n",
      "8252: lr: 0.0025, train loss: 2.4691, val loss: 2.5101\n",
      "8253: lr: 0.0025, train loss: 2.4818, val loss: 2.4608\n",
      "8254: lr: 0.0025, train loss: 2.4426, val loss: 2.4834\n",
      "8255: lr: 0.0025, train loss: 2.4508, val loss: 2.4581\n",
      "8256: lr: 0.0025, train loss: 2.4065, val loss: 2.4284\n",
      "8257: lr: 0.0025, train loss: 2.4448, val loss: 2.4666\n",
      "8258: lr: 0.0025, train loss: 2.4522, val loss: 2.4947\n",
      "8259: lr: 0.0025, train loss: 2.4205, val loss: 2.4472\n",
      "8260: lr: 0.0025, train loss: 2.4026, val loss: 2.4822\n",
      "8261: lr: 0.0025, train loss: 2.4804, val loss: 2.4962\n",
      "8262: lr: 0.0025, train loss: 2.4257, val loss: 2.4551\n",
      "8263: lr: 0.0025, train loss: 2.4903, val loss: 2.5291\n",
      "8264: lr: 0.0025, train loss: 2.3944, val loss: 2.5367\n",
      "8265: lr: 0.0025, train loss: 2.4273, val loss: 2.5289\n",
      "8266: lr: 0.0025, train loss: 2.4974, val loss: 2.4764\n",
      "8267: lr: 0.0025, train loss: 2.4703, val loss: 2.4519\n",
      "8268: lr: 0.0025, train loss: 2.4232, val loss: 2.4949\n",
      "8269: lr: 0.0025, train loss: 2.4782, val loss: 2.4846\n",
      "8270: lr: 0.0025, train loss: 2.4612, val loss: 2.4613\n",
      "8271: lr: 0.0025, train loss: 2.4844, val loss: 2.4665\n",
      "8272: lr: 0.0025, train loss: 2.4073, val loss: 2.4779\n",
      "8273: lr: 0.0025, train loss: 2.4591, val loss: 2.5601\n",
      "8274: lr: 0.0025, train loss: 2.4779, val loss: 2.5427\n",
      "8275: lr: 0.0025, train loss: 2.4604, val loss: 2.4605\n",
      "8276: lr: 0.0025, train loss: 2.4951, val loss: 2.5278\n",
      "8277: lr: 0.0025, train loss: 2.4125, val loss: 2.4760\n",
      "8278: lr: 0.0025, train loss: 2.4693, val loss: 2.4913\n",
      "8279: lr: 0.0025, train loss: 2.4689, val loss: 2.4210\n",
      "8280: lr: 0.0025, train loss: 2.4628, val loss: 2.5089\n",
      "8281: lr: 0.0025, train loss: 2.4459, val loss: 2.4583\n",
      "8282: lr: 0.0025, train loss: 2.4551, val loss: 2.4800\n",
      "8283: lr: 0.0025, train loss: 2.4779, val loss: 2.4996\n",
      "8284: lr: 0.0025, train loss: 2.4305, val loss: 2.4663\n",
      "8285: lr: 0.0025, train loss: 2.4509, val loss: 2.4585\n",
      "8286: lr: 0.0025, train loss: 2.4495, val loss: 2.4961\n",
      "8287: lr: 0.0025, train loss: 2.4221, val loss: 2.4604\n",
      "8288: lr: 0.0025, train loss: 2.3869, val loss: 2.5081\n",
      "8289: lr: 0.0025, train loss: 2.4300, val loss: 2.4587\n",
      "8290: lr: 0.0025, train loss: 2.4447, val loss: 2.4976\n",
      "8291: lr: 0.0025, train loss: 2.4539, val loss: 2.5349\n",
      "8292: lr: 0.0025, train loss: 2.4190, val loss: 2.4855\n",
      "8293: lr: 0.0025, train loss: 2.5282, val loss: 2.5118\n",
      "8294: lr: 0.0025, train loss: 2.4781, val loss: 2.4868\n",
      "8295: lr: 0.0025, train loss: 2.5157, val loss: 2.4819\n",
      "8296: lr: 0.0025, train loss: 2.4179, val loss: 2.5420\n",
      "8297: lr: 0.0025, train loss: 2.4787, val loss: 2.4516\n",
      "8298: lr: 0.0025, train loss: 2.4078, val loss: 2.4647\n",
      "8299: lr: 0.0025, train loss: 2.4460, val loss: 2.5255\n",
      "8300: lr: 0.0025, train loss: 2.4277, val loss: 2.5038\n",
      "8301: lr: 0.0025, train loss: 2.4765, val loss: 2.5395\n",
      "8302: lr: 0.0025, train loss: 2.4804, val loss: 2.5616\n",
      "8303: lr: 0.0025, train loss: 2.4591, val loss: 2.4606\n",
      "8304: lr: 0.0025, train loss: 2.4620, val loss: 2.4315\n",
      "8305: lr: 0.0025, train loss: 2.4744, val loss: 2.4568\n",
      "8306: lr: 0.0025, train loss: 2.4487, val loss: 2.5497\n",
      "8307: lr: 0.0025, train loss: 2.4477, val loss: 2.4611\n",
      "8308: lr: 0.0025, train loss: 2.4535, val loss: 2.4976\n",
      "8309: lr: 0.0025, train loss: 2.4690, val loss: 2.5179\n",
      "8310: lr: 0.0025, train loss: 2.4263, val loss: 2.4956\n",
      "8311: lr: 0.0025, train loss: 2.4719, val loss: 2.4339\n",
      "8312: lr: 0.0025, train loss: 2.3889, val loss: 2.4718\n",
      "8313: lr: 0.0025, train loss: 2.4877, val loss: 2.4515\n",
      "8314: lr: 0.0025, train loss: 2.4375, val loss: 2.4870\n",
      "8315: lr: 0.0025, train loss: 2.5053, val loss: 2.5179\n",
      "8316: lr: 0.0025, train loss: 2.4806, val loss: 2.5204\n",
      "8317: lr: 0.0025, train loss: 2.4201, val loss: 2.5382\n",
      "8318: lr: 0.0025, train loss: 2.4217, val loss: 2.4779\n",
      "8319: lr: 0.0025, train loss: 2.4690, val loss: 2.5188\n",
      "8320: lr: 0.0025, train loss: 2.4345, val loss: 2.5096\n",
      "8321: lr: 0.0025, train loss: 2.4190, val loss: 2.4922\n",
      "8322: lr: 0.0025, train loss: 2.4820, val loss: 2.5005\n",
      "8323: lr: 0.0025, train loss: 2.4748, val loss: 2.4518\n",
      "8324: lr: 0.0025, train loss: 2.4364, val loss: 2.5306\n",
      "8325: lr: 0.0025, train loss: 2.4480, val loss: 2.4784\n",
      "8326: lr: 0.0025, train loss: 2.4674, val loss: 2.4853\n",
      "8327: lr: 0.0025, train loss: 2.4625, val loss: 2.4838\n",
      "8328: lr: 0.0025, train loss: 2.4467, val loss: 2.4864\n",
      "8329: lr: 0.0025, train loss: 2.4150, val loss: 2.4995\n",
      "8330: lr: 0.0025, train loss: 2.4314, val loss: 2.5403\n",
      "8331: lr: 0.0025, train loss: 2.4434, val loss: 2.5002\n",
      "8332: lr: 0.0025, train loss: 2.4222, val loss: 2.4838\n",
      "8333: lr: 0.0025, train loss: 2.4373, val loss: 2.4770\n",
      "8334: lr: 0.0025, train loss: 2.4481, val loss: 2.4575\n",
      "8335: lr: 0.0025, train loss: 2.4699, val loss: 2.4852\n",
      "8336: lr: 0.0025, train loss: 2.4570, val loss: 2.4658\n",
      "8337: lr: 0.0025, train loss: 2.4411, val loss: 2.5048\n",
      "8338: lr: 0.0025, train loss: 2.4396, val loss: 2.4668\n",
      "8339: lr: 0.0025, train loss: 2.4322, val loss: 2.4959\n",
      "8340: lr: 0.0025, train loss: 2.4542, val loss: 2.4363\n",
      "8341: lr: 0.0025, train loss: 2.4663, val loss: 2.4574\n",
      "8342: lr: 0.0025, train loss: 2.4702, val loss: 2.5004\n",
      "8343: lr: 0.0025, train loss: 2.4977, val loss: 2.4929\n",
      "8344: lr: 0.0025, train loss: 2.4393, val loss: 2.5210\n",
      "8345: lr: 0.0025, train loss: 2.5033, val loss: 2.5102\n",
      "8346: lr: 0.0025, train loss: 2.4444, val loss: 2.5213\n",
      "8347: lr: 0.0025, train loss: 2.4497, val loss: 2.4572\n",
      "8348: lr: 0.0025, train loss: 2.4618, val loss: 2.5315\n",
      "8349: lr: 0.0025, train loss: 2.4560, val loss: 2.4719\n",
      "8350: lr: 0.0025, train loss: 2.4750, val loss: 2.4654\n",
      "8351: lr: 0.0025, train loss: 2.4564, val loss: 2.4425\n",
      "8352: lr: 0.0025, train loss: 2.5007, val loss: 2.4661\n",
      "8353: lr: 0.0025, train loss: 2.4585, val loss: 2.5193\n",
      "8354: lr: 0.0025, train loss: 2.4291, val loss: 2.5145\n",
      "8355: lr: 0.0025, train loss: 2.4548, val loss: 2.4600\n",
      "8356: lr: 0.0025, train loss: 2.4844, val loss: 2.4485\n",
      "8357: lr: 0.0025, train loss: 2.4591, val loss: 2.5178\n",
      "8358: lr: 0.0025, train loss: 2.4491, val loss: 2.4880\n",
      "8359: lr: 0.0025, train loss: 2.4794, val loss: 2.4811\n",
      "8360: lr: 0.0025, train loss: 2.4236, val loss: 2.4488\n",
      "8361: lr: 0.0025, train loss: 2.4577, val loss: 2.4860\n",
      "8362: lr: 0.0025, train loss: 2.4494, val loss: 2.5003\n",
      "8363: lr: 0.0025, train loss: 2.4293, val loss: 2.4952\n",
      "8364: lr: 0.0025, train loss: 2.4441, val loss: 2.5115\n",
      "8365: lr: 0.0025, train loss: 2.4708, val loss: 2.4617\n",
      "8366: lr: 0.0025, train loss: 2.4108, val loss: 2.4710\n",
      "8367: lr: 0.0025, train loss: 2.4739, val loss: 2.4288\n",
      "8368: lr: 0.0025, train loss: 2.4342, val loss: 2.5172\n",
      "8369: lr: 0.0025, train loss: 2.4939, val loss: 2.5141\n",
      "8370: lr: 0.0025, train loss: 2.4448, val loss: 2.4257\n",
      "8371: lr: 0.0025, train loss: 2.4281, val loss: 2.5086\n",
      "8372: lr: 0.0025, train loss: 2.4802, val loss: 2.4868\n",
      "8373: lr: 0.0025, train loss: 2.4450, val loss: 2.4611\n",
      "8374: lr: 0.0025, train loss: 2.4779, val loss: 2.4755\n",
      "8375: lr: 0.0025, train loss: 2.4477, val loss: 2.4662\n",
      "8376: lr: 0.0025, train loss: 2.4783, val loss: 2.4833\n",
      "8377: lr: 0.0025, train loss: 2.4681, val loss: 2.4686\n",
      "8378: lr: 0.0025, train loss: 2.4660, val loss: 2.4126\n",
      "8379: lr: 0.0025, train loss: 2.4579, val loss: 2.4708\n",
      "8380: lr: 0.0025, train loss: 2.4595, val loss: 2.4412\n",
      "8381: lr: 0.0025, train loss: 2.4613, val loss: 2.5215\n",
      "8382: lr: 0.0025, train loss: 2.4483, val loss: 2.4313\n",
      "8383: lr: 0.0025, train loss: 2.4740, val loss: 2.5056\n",
      "8384: lr: 0.0025, train loss: 2.4648, val loss: 2.4796\n",
      "8385: lr: 0.0025, train loss: 2.4342, val loss: 2.4778\n",
      "8386: lr: 0.0025, train loss: 2.4759, val loss: 2.4421\n",
      "8387: lr: 0.0025, train loss: 2.4539, val loss: 2.5364\n",
      "8388: lr: 0.0025, train loss: 2.4493, val loss: 2.4848\n",
      "8389: lr: 0.0025, train loss: 2.4909, val loss: 2.5093\n",
      "8390: lr: 0.0025, train loss: 2.4415, val loss: 2.4556\n",
      "8391: lr: 0.0025, train loss: 2.4545, val loss: 2.4878\n",
      "8392: lr: 0.0025, train loss: 2.4803, val loss: 2.4777\n",
      "8393: lr: 0.0025, train loss: 2.4362, val loss: 2.4931\n",
      "8394: lr: 0.0025, train loss: 2.4411, val loss: 2.4552\n",
      "8395: lr: 0.0025, train loss: 2.4509, val loss: 2.4422\n",
      "8396: lr: 0.0025, train loss: 2.4584, val loss: 2.4975\n",
      "8397: lr: 0.0025, train loss: 2.4574, val loss: 2.4856\n",
      "8398: lr: 0.0025, train loss: 2.4163, val loss: 2.4718\n",
      "8399: lr: 0.0025, train loss: 2.4614, val loss: 2.4931\n",
      "8400: lr: 0.0025, train loss: 2.4404, val loss: 2.5533\n",
      "8401: lr: 0.0025, train loss: 2.4343, val loss: 2.4845\n",
      "8402: lr: 0.0025, train loss: 2.4327, val loss: 2.4801\n",
      "8403: lr: 0.0025, train loss: 2.4786, val loss: 2.4600\n",
      "8404: lr: 0.0025, train loss: 2.4435, val loss: 2.4804\n",
      "8405: lr: 0.0025, train loss: 2.4295, val loss: 2.4937\n",
      "8406: lr: 0.0025, train loss: 2.4676, val loss: 2.4742\n",
      "8407: lr: 0.0025, train loss: 2.4280, val loss: 2.5213\n",
      "8408: lr: 0.0025, train loss: 2.4104, val loss: 2.4912\n",
      "8409: lr: 0.0025, train loss: 2.4697, val loss: 2.5251\n",
      "8410: lr: 0.0025, train loss: 2.4520, val loss: 2.4758\n",
      "8411: lr: 0.0025, train loss: 2.4654, val loss: 2.4486\n",
      "8412: lr: 0.0025, train loss: 2.4343, val loss: 2.4622\n",
      "8413: lr: 0.0025, train loss: 2.4463, val loss: 2.4807\n",
      "8414: lr: 0.0025, train loss: 2.4225, val loss: 2.5218\n",
      "8415: lr: 0.0025, train loss: 2.4359, val loss: 2.5448\n",
      "8416: lr: 0.0025, train loss: 2.4532, val loss: 2.5197\n",
      "8417: lr: 0.0025, train loss: 2.4554, val loss: 2.4863\n",
      "8418: lr: 0.0025, train loss: 2.4306, val loss: 2.4911\n",
      "8419: lr: 0.0025, train loss: 2.4262, val loss: 2.5521\n",
      "8420: lr: 0.0025, train loss: 2.4872, val loss: 2.4595\n",
      "8421: lr: 0.0025, train loss: 2.4435, val loss: 2.4768\n",
      "8422: lr: 0.0025, train loss: 2.4569, val loss: 2.5012\n",
      "8423: lr: 0.0025, train loss: 2.4367, val loss: 2.4801\n",
      "8424: lr: 0.0025, train loss: 2.4713, val loss: 2.5215\n",
      "8425: lr: 0.0025, train loss: 2.4419, val loss: 2.4989\n",
      "8426: lr: 0.0025, train loss: 2.4716, val loss: 2.4741\n",
      "8427: lr: 0.0025, train loss: 2.4506, val loss: 2.4736\n",
      "8428: lr: 0.0025, train loss: 2.4296, val loss: 2.4810\n",
      "8429: lr: 0.0025, train loss: 2.4933, val loss: 2.5411\n",
      "8430: lr: 0.0025, train loss: 2.5021, val loss: 2.4678\n",
      "8431: lr: 0.0025, train loss: 2.4412, val loss: 2.4851\n",
      "8432: lr: 0.0025, train loss: 2.4772, val loss: 2.5233\n",
      "8433: lr: 0.0025, train loss: 2.4543, val loss: 2.4857\n",
      "8434: lr: 0.0025, train loss: 2.4531, val loss: 2.5179\n",
      "8435: lr: 0.0025, train loss: 2.4465, val loss: 2.5021\n",
      "8436: lr: 0.0025, train loss: 2.4576, val loss: 2.5133\n",
      "8437: lr: 0.0025, train loss: 2.4214, val loss: 2.5003\n",
      "8438: lr: 0.0025, train loss: 2.4455, val loss: 2.4495\n",
      "8439: lr: 0.0025, train loss: 2.4457, val loss: 2.4756\n",
      "8440: lr: 0.0025, train loss: 2.4566, val loss: 2.4830\n",
      "8441: lr: 0.0025, train loss: 2.4612, val loss: 2.4857\n",
      "8442: lr: 0.0025, train loss: 2.4104, val loss: 2.4718\n",
      "8443: lr: 0.0025, train loss: 2.4695, val loss: 2.4688\n",
      "8444: lr: 0.0025, train loss: 2.4334, val loss: 2.4621\n",
      "8445: lr: 0.0025, train loss: 2.4490, val loss: 2.4928\n",
      "8446: lr: 0.0025, train loss: 2.4427, val loss: 2.5419\n",
      "8447: lr: 0.0025, train loss: 2.4787, val loss: 2.5273\n",
      "8448: lr: 0.0025, train loss: 2.4005, val loss: 2.4794\n",
      "8449: lr: 0.0025, train loss: 2.4817, val loss: 2.4634\n",
      "8450: lr: 0.0025, train loss: 2.4355, val loss: 2.4807\n",
      "8451: lr: 0.0025, train loss: 2.4671, val loss: 2.4942\n",
      "8452: lr: 0.0025, train loss: 2.4556, val loss: 2.4906\n",
      "8453: lr: 0.0025, train loss: 2.4954, val loss: 2.4655\n",
      "8454: lr: 0.0025, train loss: 2.4277, val loss: 2.5013\n",
      "8455: lr: 0.0025, train loss: 2.4916, val loss: 2.4846\n",
      "8456: lr: 0.0025, train loss: 2.4728, val loss: 2.4612\n",
      "8457: lr: 0.0025, train loss: 2.4609, val loss: 2.4675\n",
      "8458: lr: 0.0025, train loss: 2.4500, val loss: 2.4846\n",
      "8459: lr: 0.0025, train loss: 2.5106, val loss: 2.4419\n",
      "8460: lr: 0.0025, train loss: 2.4740, val loss: 2.4869\n",
      "8461: lr: 0.0025, train loss: 2.4650, val loss: 2.4584\n",
      "8462: lr: 0.0025, train loss: 2.4722, val loss: 2.4502\n",
      "8463: lr: 0.0025, train loss: 2.4496, val loss: 2.4904\n",
      "8464: lr: 0.0025, train loss: 2.4781, val loss: 2.4739\n",
      "8465: lr: 0.0025, train loss: 2.4776, val loss: 2.4953\n",
      "8466: lr: 0.0025, train loss: 2.4437, val loss: 2.4924\n",
      "8467: lr: 0.0025, train loss: 2.4339, val loss: 2.4467\n",
      "8468: lr: 0.0025, train loss: 2.4483, val loss: 2.4152\n",
      "8469: lr: 0.0025, train loss: 2.4583, val loss: 2.4827\n",
      "8470: lr: 0.0025, train loss: 2.4404, val loss: 2.4928\n",
      "8471: lr: 0.0025, train loss: 2.5071, val loss: 2.4902\n",
      "8472: lr: 0.0025, train loss: 2.4135, val loss: 2.4426\n",
      "8473: lr: 0.0025, train loss: 2.4245, val loss: 2.5051\n",
      "8474: lr: 0.0025, train loss: 2.4830, val loss: 2.5223\n",
      "8475: lr: 0.0025, train loss: 2.4412, val loss: 2.4932\n",
      "8476: lr: 0.0025, train loss: 2.4661, val loss: 2.4604\n",
      "8477: lr: 0.0025, train loss: 2.4665, val loss: 2.4823\n",
      "8478: lr: 0.0025, train loss: 2.3967, val loss: 2.4898\n",
      "8479: lr: 0.0025, train loss: 2.4520, val loss: 2.4569\n",
      "8480: lr: 0.0025, train loss: 2.4753, val loss: 2.4711\n",
      "8481: lr: 0.0025, train loss: 2.4217, val loss: 2.4783\n",
      "8482: lr: 0.0025, train loss: 2.4693, val loss: 2.4932\n",
      "8483: lr: 0.0025, train loss: 2.4269, val loss: 2.5172\n",
      "8484: lr: 0.0025, train loss: 2.4170, val loss: 2.5402\n",
      "8485: lr: 0.0025, train loss: 2.4416, val loss: 2.5040\n",
      "8486: lr: 0.0025, train loss: 2.4963, val loss: 2.4712\n",
      "8487: lr: 0.0025, train loss: 2.4680, val loss: 2.5220\n",
      "8488: lr: 0.0025, train loss: 2.4739, val loss: 2.4724\n",
      "8489: lr: 0.0025, train loss: 2.4295, val loss: 2.4968\n",
      "8490: lr: 0.0025, train loss: 2.4702, val loss: 2.5413\n",
      "8491: lr: 0.0025, train loss: 2.4470, val loss: 2.5528\n",
      "8492: lr: 0.0025, train loss: 2.5177, val loss: 2.5289\n",
      "8493: lr: 0.0025, train loss: 2.4074, val loss: 2.4597\n",
      "8494: lr: 0.0025, train loss: 2.4246, val loss: 2.5023\n",
      "8495: lr: 0.0025, train loss: 2.4705, val loss: 2.4959\n",
      "8496: lr: 0.0025, train loss: 2.4599, val loss: 2.5012\n",
      "8497: lr: 0.0025, train loss: 2.4509, val loss: 2.5259\n",
      "8498: lr: 0.0025, train loss: 2.5115, val loss: 2.4908\n",
      "8499: lr: 0.0025, train loss: 2.4752, val loss: 2.4554\n",
      "8500: lr: 0.0025, train loss: 2.4956, val loss: 2.5231\n",
      "8501: lr: 0.0025, train loss: 2.4259, val loss: 2.4652\n",
      "8502: lr: 0.0025, train loss: 2.4758, val loss: 2.4585\n",
      "8503: lr: 0.0025, train loss: 2.4717, val loss: 2.5051\n",
      "8504: lr: 0.0025, train loss: 2.4848, val loss: 2.4714\n",
      "8505: lr: 0.0025, train loss: 2.4655, val loss: 2.4628\n",
      "8506: lr: 0.0025, train loss: 2.4410, val loss: 2.5122\n",
      "8507: lr: 0.0025, train loss: 2.4807, val loss: 2.4589\n",
      "8508: lr: 0.0025, train loss: 2.4395, val loss: 2.4379\n",
      "8509: lr: 0.0025, train loss: 2.4485, val loss: 2.4905\n",
      "8510: lr: 0.0025, train loss: 2.3766, val loss: 2.4562\n",
      "8511: lr: 0.0025, train loss: 2.4917, val loss: 2.4907\n",
      "8512: lr: 0.0025, train loss: 2.4337, val loss: 2.5467\n",
      "8513: lr: 0.0025, train loss: 2.4350, val loss: 2.4934\n",
      "8514: lr: 0.0025, train loss: 2.4698, val loss: 2.4725\n",
      "8515: lr: 0.0025, train loss: 2.4590, val loss: 2.5003\n",
      "8516: lr: 0.0025, train loss: 2.4551, val loss: 2.4691\n",
      "8517: lr: 0.0025, train loss: 2.4725, val loss: 2.4964\n",
      "8518: lr: 0.0025, train loss: 2.4549, val loss: 2.5137\n",
      "8519: lr: 0.0025, train loss: 2.3948, val loss: 2.4847\n",
      "8520: lr: 0.0025, train loss: 2.4347, val loss: 2.4843\n",
      "8521: lr: 0.0025, train loss: 2.4438, val loss: 2.4408\n",
      "8522: lr: 0.0025, train loss: 2.4641, val loss: 2.4878\n",
      "8523: lr: 0.0025, train loss: 2.4567, val loss: 2.5056\n",
      "8524: lr: 0.0025, train loss: 2.4539, val loss: 2.4855\n",
      "8525: lr: 0.0025, train loss: 2.4943, val loss: 2.5083\n",
      "8526: lr: 0.0025, train loss: 2.4249, val loss: 2.4967\n",
      "8527: lr: 0.0025, train loss: 2.4216, val loss: 2.5149\n",
      "8528: lr: 0.0025, train loss: 2.4424, val loss: 2.4794\n",
      "8529: lr: 0.0025, train loss: 2.4454, val loss: 2.5116\n",
      "8530: lr: 0.0025, train loss: 2.4600, val loss: 2.4732\n",
      "8531: lr: 0.0025, train loss: 2.4337, val loss: 2.4575\n",
      "8532: lr: 0.0025, train loss: 2.4444, val loss: 2.5419\n",
      "8533: lr: 0.0025, train loss: 2.4366, val loss: 2.4623\n",
      "8534: lr: 0.0025, train loss: 2.4854, val loss: 2.4752\n",
      "8535: lr: 0.0025, train loss: 2.4468, val loss: 2.4901\n",
      "8536: lr: 0.0025, train loss: 2.4816, val loss: 2.5147\n",
      "8537: lr: 0.0025, train loss: 2.4766, val loss: 2.5271\n",
      "8538: lr: 0.0025, train loss: 2.4797, val loss: 2.4975\n",
      "8539: lr: 0.0025, train loss: 2.4610, val loss: 2.4640\n",
      "8540: lr: 0.0025, train loss: 2.4723, val loss: 2.4553\n",
      "8541: lr: 0.0025, train loss: 2.4550, val loss: 2.4945\n",
      "8542: lr: 0.0025, train loss: 2.4298, val loss: 2.4934\n",
      "8543: lr: 0.0025, train loss: 2.4404, val loss: 2.5201\n",
      "8544: lr: 0.0025, train loss: 2.4539, val loss: 2.5107\n",
      "8545: lr: 0.0025, train loss: 2.4344, val loss: 2.4713\n",
      "8546: lr: 0.0025, train loss: 2.4731, val loss: 2.4909\n",
      "8547: lr: 0.0025, train loss: 2.4868, val loss: 2.5226\n",
      "8548: lr: 0.0025, train loss: 2.4254, val loss: 2.5247\n",
      "8549: lr: 0.0025, train loss: 2.4273, val loss: 2.4664\n",
      "8550: lr: 0.0025, train loss: 2.4710, val loss: 2.4887\n",
      "8551: lr: 0.0025, train loss: 2.4144, val loss: 2.4232\n",
      "8552: lr: 0.0025, train loss: 2.4628, val loss: 2.4328\n",
      "8553: lr: 0.0025, train loss: 2.4347, val loss: 2.4844\n",
      "8554: lr: 0.0025, train loss: 2.4373, val loss: 2.4704\n",
      "8555: lr: 0.0025, train loss: 2.4585, val loss: 2.4510\n",
      "8556: lr: 0.0025, train loss: 2.4505, val loss: 2.5103\n",
      "8557: lr: 0.0025, train loss: 2.4835, val loss: 2.5082\n",
      "8558: lr: 0.0025, train loss: 2.4074, val loss: 2.4716\n",
      "8559: lr: 0.0025, train loss: 2.4478, val loss: 2.5355\n",
      "8560: lr: 0.0025, train loss: 2.4550, val loss: 2.4837\n",
      "8561: lr: 0.0025, train loss: 2.4500, val loss: 2.5365\n",
      "8562: lr: 0.0025, train loss: 2.4908, val loss: 2.4397\n",
      "8563: lr: 0.0025, train loss: 2.4738, val loss: 2.5090\n",
      "8564: lr: 0.0025, train loss: 2.4541, val loss: 2.5118\n",
      "8565: lr: 0.0025, train loss: 2.4399, val loss: 2.4775\n",
      "8566: lr: 0.0025, train loss: 2.4829, val loss: 2.5238\n",
      "8567: lr: 0.0025, train loss: 2.4879, val loss: 2.4955\n",
      "8568: lr: 0.0025, train loss: 2.4557, val loss: 2.5075\n",
      "8569: lr: 0.0025, train loss: 2.4410, val loss: 2.4868\n",
      "8570: lr: 0.0025, train loss: 2.4584, val loss: 2.5105\n",
      "8571: lr: 0.0025, train loss: 2.4501, val loss: 2.4716\n",
      "8572: lr: 0.0025, train loss: 2.4480, val loss: 2.5322\n",
      "8573: lr: 0.0025, train loss: 2.4478, val loss: 2.5227\n",
      "8574: lr: 0.0025, train loss: 2.4723, val loss: 2.4732\n",
      "8575: lr: 0.0025, train loss: 2.4203, val loss: 2.4885\n",
      "8576: lr: 0.0025, train loss: 2.4456, val loss: 2.5201\n",
      "8577: lr: 0.0025, train loss: 2.4937, val loss: 2.4921\n",
      "8578: lr: 0.0025, train loss: 2.4558, val loss: 2.5180\n",
      "8579: lr: 0.0025, train loss: 2.4673, val loss: 2.4648\n",
      "8580: lr: 0.0025, train loss: 2.4491, val loss: 2.5293\n",
      "8581: lr: 0.0025, train loss: 2.4760, val loss: 2.4614\n",
      "8582: lr: 0.0025, train loss: 2.4551, val loss: 2.4530\n",
      "8583: lr: 0.0025, train loss: 2.4460, val loss: 2.5205\n",
      "8584: lr: 0.0025, train loss: 2.4495, val loss: 2.4770\n",
      "8585: lr: 0.0025, train loss: 2.4477, val loss: 2.4516\n",
      "8586: lr: 0.0025, train loss: 2.4173, val loss: 2.4624\n",
      "8587: lr: 0.0025, train loss: 2.4127, val loss: 2.5404\n",
      "8588: lr: 0.0025, train loss: 2.4594, val loss: 2.4765\n",
      "8589: lr: 0.0025, train loss: 2.4940, val loss: 2.4589\n",
      "8590: lr: 0.0025, train loss: 2.4689, val loss: 2.4947\n",
      "8591: lr: 0.0025, train loss: 2.4875, val loss: 2.5145\n",
      "8592: lr: 0.0025, train loss: 2.4544, val loss: 2.4953\n",
      "8593: lr: 0.0025, train loss: 2.5224, val loss: 2.4892\n",
      "8594: lr: 0.0025, train loss: 2.4386, val loss: 2.5047\n",
      "8595: lr: 0.0025, train loss: 2.4087, val loss: 2.4423\n",
      "8596: lr: 0.0025, train loss: 2.4274, val loss: 2.4791\n",
      "8597: lr: 0.0025, train loss: 2.4568, val loss: 2.4868\n",
      "8598: lr: 0.0025, train loss: 2.4491, val loss: 2.4773\n",
      "8599: lr: 0.0025, train loss: 2.4397, val loss: 2.4438\n",
      "8600: lr: 0.0025, train loss: 2.4339, val loss: 2.4909\n",
      "8601: lr: 0.0025, train loss: 2.4411, val loss: 2.5007\n",
      "8602: lr: 0.0025, train loss: 2.4440, val loss: 2.4523\n",
      "8603: lr: 0.0025, train loss: 2.4710, val loss: 2.4922\n",
      "8604: lr: 0.0025, train loss: 2.4825, val loss: 2.4690\n",
      "8605: lr: 0.0025, train loss: 2.4229, val loss: 2.5297\n",
      "8606: lr: 0.0025, train loss: 2.4262, val loss: 2.4880\n",
      "8607: lr: 0.0025, train loss: 2.4402, val loss: 2.5080\n",
      "8608: lr: 0.0025, train loss: 2.4737, val loss: 2.5317\n",
      "8609: lr: 0.0025, train loss: 2.4945, val loss: 2.4583\n",
      "8610: lr: 0.0025, train loss: 2.4689, val loss: 2.4611\n",
      "8611: lr: 0.0025, train loss: 2.4545, val loss: 2.5428\n",
      "8612: lr: 0.0025, train loss: 2.3746, val loss: 2.4642\n",
      "8613: lr: 0.0025, train loss: 2.4555, val loss: 2.4661\n",
      "8614: lr: 0.0025, train loss: 2.4337, val loss: 2.4707\n",
      "8615: lr: 0.0025, train loss: 2.4112, val loss: 2.4790\n",
      "8616: lr: 0.0025, train loss: 2.4542, val loss: 2.4642\n",
      "8617: lr: 0.0025, train loss: 2.4600, val loss: 2.4484\n",
      "8618: lr: 0.0025, train loss: 2.4316, val loss: 2.5037\n",
      "8619: lr: 0.0025, train loss: 2.4741, val loss: 2.4547\n",
      "8620: lr: 0.0025, train loss: 2.4408, val loss: 2.4456\n",
      "8621: lr: 0.0025, train loss: 2.4364, val loss: 2.5226\n",
      "8622: lr: 0.0025, train loss: 2.4170, val loss: 2.5013\n",
      "8623: lr: 0.0025, train loss: 2.4847, val loss: 2.4481\n",
      "8624: lr: 0.0025, train loss: 2.4502, val loss: 2.4976\n",
      "8625: lr: 0.0025, train loss: 2.4460, val loss: 2.4934\n",
      "8626: lr: 0.0025, train loss: 2.4488, val loss: 2.5547\n",
      "8627: lr: 0.0025, train loss: 2.4864, val loss: 2.4925\n",
      "8628: lr: 0.0025, train loss: 2.4391, val loss: 2.4760\n",
      "8629: lr: 0.0025, train loss: 2.4826, val loss: 2.4417\n",
      "8630: lr: 0.0025, train loss: 2.4379, val loss: 2.4786\n",
      "8631: lr: 0.0025, train loss: 2.4585, val loss: 2.5232\n",
      "8632: lr: 0.0025, train loss: 2.4484, val loss: 2.4560\n",
      "8633: lr: 0.0025, train loss: 2.4512, val loss: 2.4523\n",
      "8634: lr: 0.0025, train loss: 2.4908, val loss: 2.4551\n",
      "8635: lr: 0.0025, train loss: 2.4502, val loss: 2.4922\n",
      "8636: lr: 0.0025, train loss: 2.4749, val loss: 2.4881\n",
      "8637: lr: 0.0025, train loss: 2.4448, val loss: 2.4345\n",
      "8638: lr: 0.0025, train loss: 2.4985, val loss: 2.4830\n",
      "8639: lr: 0.0025, train loss: 2.4629, val loss: 2.4860\n",
      "8640: lr: 0.0025, train loss: 2.4529, val loss: 2.4966\n",
      "8641: lr: 0.0025, train loss: 2.4367, val loss: 2.5179\n",
      "8642: lr: 0.0025, train loss: 2.4556, val loss: 2.4822\n",
      "8643: lr: 0.0025, train loss: 2.4752, val loss: 2.4990\n",
      "8644: lr: 0.0025, train loss: 2.4572, val loss: 2.4640\n",
      "8645: lr: 0.0025, train loss: 2.4292, val loss: 2.4731\n",
      "8646: lr: 0.0025, train loss: 2.4518, val loss: 2.5151\n",
      "8647: lr: 0.0025, train loss: 2.4692, val loss: 2.4911\n",
      "8648: lr: 0.0025, train loss: 2.4461, val loss: 2.5115\n",
      "8649: lr: 0.0025, train loss: 2.4000, val loss: 2.5307\n",
      "8650: lr: 0.0025, train loss: 2.4028, val loss: 2.4986\n",
      "8651: lr: 0.0025, train loss: 2.4854, val loss: 2.4261\n",
      "8652: lr: 0.0025, train loss: 2.4606, val loss: 2.4621\n",
      "8653: lr: 0.0025, train loss: 2.4738, val loss: 2.4700\n",
      "8654: lr: 0.0025, train loss: 2.4438, val loss: 2.4629\n",
      "8655: lr: 0.0025, train loss: 2.4405, val loss: 2.5266\n",
      "8656: lr: 0.0025, train loss: 2.4242, val loss: 2.5300\n",
      "8657: lr: 0.0025, train loss: 2.4404, val loss: 2.5024\n",
      "8658: lr: 0.0025, train loss: 2.5078, val loss: 2.4695\n",
      "8659: lr: 0.0025, train loss: 2.4434, val loss: 2.5158\n",
      "8660: lr: 0.0025, train loss: 2.5051, val loss: 2.5216\n",
      "8661: lr: 0.0025, train loss: 2.4724, val loss: 2.5024\n",
      "8662: lr: 0.0025, train loss: 2.4670, val loss: 2.4621\n",
      "8663: lr: 0.0025, train loss: 2.4353, val loss: 2.5265\n",
      "8664: lr: 0.0025, train loss: 2.4073, val loss: 2.4579\n",
      "8665: lr: 0.0025, train loss: 2.4364, val loss: 2.4582\n",
      "8666: lr: 0.0025, train loss: 2.4579, val loss: 2.4739\n",
      "8667: lr: 0.0025, train loss: 2.4256, val loss: 2.5069\n",
      "8668: lr: 0.0025, train loss: 2.4231, val loss: 2.4652\n",
      "8669: lr: 0.0025, train loss: 2.5121, val loss: 2.4742\n",
      "8670: lr: 0.0025, train loss: 2.4374, val loss: 2.4995\n",
      "8671: lr: 0.0025, train loss: 2.4530, val loss: 2.4710\n",
      "8672: lr: 0.0025, train loss: 2.4708, val loss: 2.5170\n",
      "8673: lr: 0.0025, train loss: 2.4472, val loss: 2.5246\n",
      "8674: lr: 0.0025, train loss: 2.4506, val loss: 2.4894\n",
      "8675: lr: 0.0025, train loss: 2.4388, val loss: 2.4378\n",
      "8676: lr: 0.0025, train loss: 2.5003, val loss: 2.4599\n",
      "8677: lr: 0.0025, train loss: 2.4554, val loss: 2.4881\n",
      "8678: lr: 0.0025, train loss: 2.4323, val loss: 2.4886\n",
      "8679: lr: 0.0025, train loss: 2.4607, val loss: 2.4472\n",
      "8680: lr: 0.0025, train loss: 2.4631, val loss: 2.4933\n",
      "8681: lr: 0.0025, train loss: 2.3734, val loss: 2.5081\n",
      "8682: lr: 0.0025, train loss: 2.4522, val loss: 2.4801\n",
      "8683: lr: 0.0025, train loss: 2.5164, val loss: 2.4922\n",
      "8684: lr: 0.0025, train loss: 2.4732, val loss: 2.4764\n",
      "8685: lr: 0.0025, train loss: 2.4468, val loss: 2.4386\n",
      "8686: lr: 0.0025, train loss: 2.4704, val loss: 2.5128\n",
      "8687: lr: 0.0025, train loss: 2.4365, val loss: 2.5189\n",
      "8688: lr: 0.0025, train loss: 2.4530, val loss: 2.4967\n",
      "8689: lr: 0.0025, train loss: 2.4610, val loss: 2.4841\n",
      "8690: lr: 0.0025, train loss: 2.3959, val loss: 2.4809\n",
      "8691: lr: 0.0025, train loss: 2.4844, val loss: 2.4697\n",
      "8692: lr: 0.0025, train loss: 2.4318, val loss: 2.4305\n",
      "8693: lr: 0.0025, train loss: 2.4955, val loss: 2.5244\n",
      "8694: lr: 0.0025, train loss: 2.4582, val loss: 2.4768\n",
      "8695: lr: 0.0025, train loss: 2.4719, val loss: 2.5133\n",
      "8696: lr: 0.0025, train loss: 2.4264, val loss: 2.4691\n",
      "8697: lr: 0.0025, train loss: 2.4137, val loss: 2.5267\n",
      "8698: lr: 0.0025, train loss: 2.4868, val loss: 2.4707\n",
      "8699: lr: 0.0025, train loss: 2.4664, val loss: 2.4878\n",
      "8700: lr: 0.0025, train loss: 2.4640, val loss: 2.4556\n",
      "8701: lr: 0.0025, train loss: 2.4877, val loss: 2.5055\n",
      "8702: lr: 0.0025, train loss: 2.4546, val loss: 2.4571\n",
      "8703: lr: 0.0025, train loss: 2.4722, val loss: 2.4910\n",
      "8704: lr: 0.0025, train loss: 2.4498, val loss: 2.4722\n",
      "8705: lr: 0.0025, train loss: 2.4877, val loss: 2.5061\n",
      "8706: lr: 0.0025, train loss: 2.5140, val loss: 2.4667\n",
      "8707: lr: 0.0025, train loss: 2.4144, val loss: 2.4486\n",
      "8708: lr: 0.0025, train loss: 2.4451, val loss: 2.4838\n",
      "8709: lr: 0.0025, train loss: 2.4676, val loss: 2.5569\n",
      "8710: lr: 0.0025, train loss: 2.4078, val loss: 2.4901\n",
      "8711: lr: 0.0025, train loss: 2.4508, val loss: 2.5195\n",
      "8712: lr: 0.0025, train loss: 2.5065, val loss: 2.4754\n",
      "8713: lr: 0.0025, train loss: 2.4512, val loss: 2.5943\n",
      "8714: lr: 0.0025, train loss: 2.4540, val loss: 2.4902\n",
      "8715: lr: 0.0025, train loss: 2.4508, val loss: 2.4783\n",
      "8716: lr: 0.0025, train loss: 2.4288, val loss: 2.5024\n",
      "8717: lr: 0.0025, train loss: 2.4614, val loss: 2.5019\n",
      "8718: lr: 0.0025, train loss: 2.4830, val loss: 2.5626\n",
      "8719: lr: 0.0025, train loss: 2.4204, val loss: 2.4773\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[109], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 15\u001b[0m t_loss, v_loss \u001b[38;5;241m=\u001b[39m \u001b[43mget_split_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m: lr: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m, train loss: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m, val loss: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (e, LR, t_loss, v_loss))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[108], line 7\u001b[0m, in \u001b[0;36mget_split_losses\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m      6\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m get_batch(split)\n\u001b[1;32m----> 7\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      9\u001b[0m     losses[split] \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[107], line 12\u001b[0m, in \u001b[0;36mLM.forward\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens):\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[106], line 11\u001b[0m, in \u001b[0;36mFCL.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 11\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"%d trainable parameters\" % (sum(p.nelement() for p in net.parameters())))\n",
    "optimizer = optim.AdamW(net.parameters(), lr=LR)\n",
    "\n",
    "for e in range(1, EPOCHS + 1):\n",
    "    if e % (EPOCHS / 25) == 0:\n",
    "        LR /= 2\n",
    "        for group in optimizer.param_groups:\n",
    "            group[\"lr\"] = LR\n",
    "    x, y, = get_batch(\"train\")\n",
    "    logits = net(x)\n",
    "    loss = F.cross_entropy(logits.view(-1, logits.shape[-1] ), y.view(-1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    t_loss, v_loss = get_split_losses()\n",
    "    print(\"%d: lr: %.4f, train loss: %.4f, val loss: %.4f\" % (e, LR, t_loss, v_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ouoa  oa ieu a aeoo  iuloioe ooau:eoul ooie oeeeo o  oreo o:ou oi e  oeo ll   e    oo  ueaoi  ooeeoeulioaaoeo:eooaoeei oul::aaooua aoiu: auooauooe eeoa   eoiouu liiel eeooul u el aual:: li aotou eoaloeooo:eo:oaooiai u oeaoeo aieaaolaoiooioaeei  a oaei  oeu  iuob:o:oooeoeea::u  ao  a oeoauaeoie:ioae areii aa: ooi o aori oooaaa oe  eoaeaoiuooo:oeaioeiia a uo;ui a ilaeee iiel  eeeeeui liu u oou l e aauu  oa  uaeoeeu:ouaauui:: ouiou: auoe u aaeaioaoie a oieeoiu e a ioeaoi o ou iaeeeea:: uio:lo:iaaooioul oieaeeou ioireee reo a iueeaoioo: ooa  ea"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[124], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[116], line 18\u001b[0m, in \u001b[0;36mLM.generate\u001b[1;34m(self, context, new_tokens)\u001b[0m\n\u001b[0;32m     16\u001b[0m out \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m new_tokens:\n\u001b[1;32m---> 18\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:, :]\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mnext\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(probs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, VOCAB_SIZE), num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[116], line 12\u001b[0m, in \u001b[0;36mLM.forward\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens):\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[106], line 11\u001b[0m, in \u001b[0;36mFCL.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 11\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net.generate(new_tokens=100000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
