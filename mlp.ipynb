{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 27\n",
    "SOURCE = \"names.txt\"\n",
    "SEP = \".\"\n",
    "text = open(SOURCE, encoding=\"utf-8\").read()\n",
    "s_to_i = {chr(97 + i): i + 1 for i in range(VOCAB_SIZE - 1)}\n",
    "s_to_i[SEP] = 0\n",
    "i_to_s = {v: k for k, v in s_to_i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic tokenizer, encodes text into tokens(int) and\n",
    "# decodes tokens(int) back to text\n",
    "\n",
    "def encode(text):\n",
    "    return [s_to_i[char] for char in text]\n",
    "\n",
    "def decode(tokens):\n",
    "    if isinstance(tokens, int):\n",
    "        return i_to_s[tokens]\n",
    "    return \"\".join([i_to_s[token] for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs, Ys = [], []\n",
    "H_LAYERS, H_DIM = 2, VOCAB_SIZE * 2\n",
    "\n",
    "with open(SOURCE, encoding=\"utf-8\") as f:\n",
    "    for name in f.read().splitlines():\n",
    "        name = [SEP] + list(name) + [SEP]\n",
    "        for c1, c2 in zip(name, name[1:]):\n",
    "            Xs.append(s_to_i[c1])\n",
    "            Ys.append(s_to_i[c2])\n",
    "\n",
    "Xs, Ys = torch.tensor(Xs), torch.tensor(Ys)\n",
    "# basic embedding, converts a token into a one hot vector\n",
    "# of size VOCAB_SIZE(i.e 27)\n",
    "x_emb = F.one_hot(Xs, num_classes=VOCAB_SIZE).float()\n",
    "g = torch.Generator().manual_seed(1337)\n",
    "W0 = torch.randn((VOCAB_SIZE, H_DIM), generator=g, requires_grad=True)\n",
    "Ws = [torch.randn((H_DIM, H_DIM), generator=g) for _ in range(H_LAYERS)]\n",
    "W1 = torch.randn((H_DIM, VOCAB_SIZE), generator=g, requires_grad=True)\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS, LR, ALPHA = 200, 2, 0.01\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    for w in [W0] + Ws + [W1]:\n",
    "        x = x @ w\n",
    "    logits = x @ W1\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    reg = torch.tensor([(w ** 2).mean() for w in [W1] + Ws + W1])\n",
    "    loss = -probs[torch.arange(Xs.nelement()), Ys].log().mean()\n",
    "    loss.backward()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
